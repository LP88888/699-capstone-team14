data:
  input_path: "./data/raw/wilmerarltstrmberg_data.csv"   # Full dataset for production
  # input_path: "./data/raw/sample_data.csv"                 # Smaller sample for testing
  ner_col: "NER"
  chunksize: 200000
  test: false

cleanup:
  enabled: true             # If true, delete files listed in cleanup.paths before run
  paths:
    - "./data/normalized/recipes_data_clean.parquet"
    - "./data/normalized/cosine_dedupe_map.jsonl"
    - "./data/normalized/recipes_data_clean_spell_dedup.parquet"
    - "./data/encoded/datasets_unified.parquet"
    - "./data/encoded/ingredient_id_to_token.json"
    - "./data/encoded/ingredient_token_to_id.json"

output:
  baseline_parquet: "./data/normalized/recipes_data_clean.parquet"              # spaCy-normalized: NER_clean
  dedup_parquet: "./data/normalized/recipes_data_clean_spell_dedup.parquet"     # after dedupe map applied
  cosine_map_path: "./data/normalized/cosine_dedupe_map.jsonl"                  # SBERT/W2V map output
  list_col_for_vocab: "NER_clean"

  # Encoder outputs
  unified_parquet: "./data/encoded/datasets_unified.parquet"
  ingredient_id_to_token: "./data/encoded/ingredient_id_to_token.json"
  ingredient_token_to_id: "./data/encoded/ingredient_token_to_id.json"

stages:
  write_parquet: true       # Stage 1: spaCy normalize → baseline_parquet
  sbert_dedupe: true        # Stage 2: use SBERT dedupe (preferred at scale)
  w2v_dedupe: false         # Stage 2 alternative: set to true ONLY if sbert_dedupe=false
  apply_cosine_map: true    # Stage 3: apply dedupe map → dedup_parquet
  encode_ids: true          # Stage 4: encode to IDs → unified_parquet

sbert:
  model: "all-MiniLM-L6-v2"
  threshold: 0.88           # merge if cosine >= threshold (tune 0.86–0.90)
  topk: 25                  # neighbors per phrase to consider
  min_len: 2                # skip ultra-short tokens
  require_token_overlap: true
  block_generic_as_canon: true
  min_freq_for_vocab: 2     # filter tail before dedupe (good for 2M rows)
  spacy_model: "en_core_web_sm"   # spaCy model used in Stage 1

w2v:
  # Only used if stages.w2v_dedupe=true and stages.sbert_dedupe=false
  vector_size: 100
  window: 5
  min_count: 1
  workers: 4
  sg: 1
  epochs: 8
  threshold: 0.85
  topk: 25
  min_freq_for_vocab: 2

encoder:
  min_freq: 1
  dataset_id: 1
  ingredients_col: "NER_clean"

logging:
  level: INFO            # DEBUG | INFO | WARNING | ERROR
  console: true
  file: pipeline/logs/ingrnorm.log
  rotate:
    max_bytes: 10_485_760   # 10 MB
    backup_count: 5
  fmt: "[%(asctime)s] %(levelname)s %(name)s: %(message)s"
  datefmt: "%Y-%m-%d %H:%M:%S"
