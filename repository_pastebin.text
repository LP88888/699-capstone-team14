

================================================================================
FILE: create_pastebin.py
================================================================================
#!/usr/bin/env python
"""Create a paste bin file containing all .py and .yaml files."""
from pathlib import Path
import sys

def create_pastebin():
    root = Path(".")
    files = []
    
    # Find all .py, .yaml, and .yml files
    for ext in ["py", "yaml", "yml"]:
        files.extend(root.rglob(f"*.{ext}"))
    
    # Filter out unwanted directories
    exclude_dirs = {"__pycache__", ".git", "venv", "site-packages", ".pytest_cache", ".mypy_cache"}
    files = [
        f for f in sorted(set(files))
        if f.exists() and not any(excluded in str(f) for excluded in exclude_dirs)
    ]
    
    content = []
    for f in files:
        try:
            file_content = f.read_text(encoding="utf-8", errors="ignore")
            content.append(f"\n\n{'='*80}\nFILE: {f}\n{'='*80}\n")
            content.append(file_content)
        except Exception as e:
            print(f"Error reading {f}: {e}", file=sys.stderr)
    
    output_path = Path("repository_pastebin.text")
    output_path.write_text("".join(content), encoding="utf-8")
    print(f"Created pastebin with {len(files)} files: {output_path.resolve()}")
    return output_path

if __name__ == "__main__":
    create_pastebin()



================================================================================
FILE: networks\interative_map.py
================================================================================
#!/usr/bin/env python
"""
Build an interactive cuisine network HTML where:
- Nodes = cuisines
- Edges = shared top ingredients between cuisines

Reads:
    reports/phase2/top_features_per_cuisine.csv

Writes:
    reports/viz/cuisine_network_pyvis.html
"""

from pathlib import Path
import numpy as np
import pandas as pd

from pyvis.network import Network

# -------------------------
# Paths
# -------------------------
BASE_DIR = Path(__file__).resolve().parents[1]  # adjust if needed
REPORTS_DIR = BASE_DIR / "reports" / "phase2"
VIZ_DIR = BASE_DIR / "reports" / "viz"
VIZ_DIR.mkdir(parents=True, exist_ok=True)

TOP_FEAT_PATH = REPORTS_DIR / "top_features_per_cuisine.csv"
OUT_HTML = VIZ_DIR / "cuisine_network_pyvis.html"

RANDOM_STATE = 42

# -------------------------
# Parameters
# -------------------------
N_CUISINES = 25           # how many cuisines to include (top by frequency in top_features file)
TOP_TERMS_PER_CUISINE = 40
MIN_SHARED_TERMS = 5      # create an edge only if at least this many shared top ingredients


def main():
    print(f"[INFO] Reading top features from: {TOP_FEAT_PATH}")
    if not TOP_FEAT_PATH.exists():
        raise FileNotFoundError(f"File not found: {TOP_FEAT_PATH}")

    df = pd.read_csv(TOP_FEAT_PATH)

    required_cols = {"cuisine", "feature", "rank"}
    if not required_cols.issubset(df.columns):
        raise ValueError(
            f"Expected columns {required_cols} in {TOP_FEAT_PATH}, "
            f"got {set(df.columns)}"
        )

    # Use frequencies in this file as a proxy for "importance"/coverage
    cuisine_counts = df["cuisine"].value_counts()

    # Select top cuisines by count
    selected_cuisines = cuisine_counts.head(N_CUISINES).index.tolist()
    print(f"[INFO] Using {len(selected_cuisines)} cuisines: {selected_cuisines}")

    # Filter to selected cuisines + top terms
    df_sel = df[
        (df["cuisine"].isin(selected_cuisines)) &
        (df["rank"] <= TOP_TERMS_PER_CUISINE)
    ].copy()

    if df_sel.empty:
        raise RuntimeError("No data after filtering by cuisine + rank; check parameters.")

    # Build mapping: cuisine -> set of top ingredients
    cuisine_to_terms = (
        df_sel.groupby("cuisine")["feature"]
        .apply(lambda s: set(str(x) for x in s))
        .to_dict()
    )

    # -------------------------
    # Build PyVis network
    # -------------------------
    net = Network(
        height="800px",
        width="100%",
        bgcolor="#ffffff",
        font_color="#222222",
        notebook=False,
        directed=False,
    )

    # A bit nicer physics
    net.barnes_hut(gravity=-20000, central_gravity=0.3, spring_length=150, spring_strength=0.02)

    # Add cuisine nodes
    for cuisine in selected_cuisines:
        terms = cuisine_to_terms.get(cuisine, set())
        num_terms = len(terms)
        freq = int(cuisine_counts.get(cuisine, 0))

        # Node size scaled by count in top_features file
        value = max(1, freq)

        title_html = (
            f"<b>{cuisine}</b><br>"
            f"features in file: {freq}<br>"
            f"top terms (sample): {', '.join(sorted(list(terms))[:15])}"
        )

        net.add_node(
            cuisine,
            label=cuisine,
            title=title_html,
            value=value,
        )

    # Add edges between cuisines with enough shared ingredients
    cuisines = list(cuisine_to_terms.keys())

    rng = np.random.RandomState(RANDOM_STATE)

    for i in range(len(cuisines)):
        for j in range(i + 1, len(cuisines)):
            ci, cj = cuisines[i], cuisines[j]
            terms_i = cuisine_to_terms.get(ci, set())
            terms_j = cuisine_to_terms.get(cj, set())
            if not terms_i or not terms_j:
                continue

            shared = terms_i & terms_j
            if len(shared) >= MIN_SHARED_TERMS:
                shared_list = sorted(list(shared))
                # Limit the hover text length
                shared_preview = ", ".join(shared_list[:20])
                if len(shared_list) > 20:
                    shared_preview += ", ..."

                title = (
                    f"<b>{ci}</b> – <b>{cj}</b><br>"
                    f"shared top ingredients: {len(shared)}<br>"
                    f"{shared_preview}"
                )

                # edge value scales line thickness
                net.add_edge(
                    ci,
                    cj,
                    value=len(shared),
                    title=title,
                )

    # If the graph ended up empty, warn
    if len(net.nodes) == 0 or len(net.edges) == 0:
        print("[WARN] Network ended up with no nodes or edges; check thresholds.")
    else:
        print(f"[INFO] Network has {len(net.nodes)} nodes and {len(net.edges)} edges.")

    # Write HTML (no auto-open; you can open in browser)
    print(f"[INFO] Writing HTML to: {OUT_HTML}")
    net.write_html(str(OUT_HTML))


if __name__ == "__main__":
    main()


================================================================================
FILE: networks\modeling.py
================================================================================
# flavor_saviors_modeling.py
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, f1_score
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings("ignore")

# 1. Load dataset
df = pd.read_parquet("data/encoded/combined_raw_datasets_with_cuisine_encoded.parquet")

# 2. Basic cleanup (enough to model)
# Keep only rows that have both cuisine and ingredients
df = df.dropna(subset=["cuisine", "ingredients"])

# Ensure strings
df["ingredients"] = df["ingredients"].astype(str)
df["cuisine"] = df["cuisine"].astype(str).str.strip().str.lower()

print(f"Loaded {len(df)} recipes across {df['cuisine'].nunique()} cuisines.")
print(df.head(3))

# 3. Vectorize ingredients (bag-of-words or TF-IDF)
tfidf = TfidfVectorizer(
    token_pattern=r"(?u)\b\w+\b",
    stop_words="english",
    max_features=5000
)
X = tfidf.fit_transform(df["ingredients"])



# Inspect class sizes
vc = df["cuisine"].value_counts()
print("Class counts (top 20):")
print(vc.head(20))

# Option A: collapse rare cuisines into 'other'
MIN_PER_CLASS = 5  # for test_size=0.2 this ensures at least 1 test sample
df["cuisine"] = df["cuisine"].where(df["cuisine"].map(vc) >= MIN_PER_CLASS, "other")

# Recompute counts after collapsing
vc2 = df["cuisine"].value_counts()
print("\nAfter collapsing rare classes:")
print(vc2)

# If 'other' is still too small (edge case), drop it
if vc2.get("other", 0) < 2:
    df = df[df["cuisine"] != "other"]

# Rebuild X, y if needed (if you already built them above)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

tfidf = TfidfVectorizer(token_pattern=r"(?u)\b\w+\b", stop_words="english", max_features=5000)
X = tfidf.fit_transform(df["ingredients"].astype(str))


le = LabelEncoder()
y = le.fit_transform(df["cuisine"])

# Now the stratified split should work
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 5. Logistic Regression baseline
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)
y_pred_lr = log_reg.predict(X_test)
print("\n=== Logistic Regression ===")
print(classification_report(y_test, y_pred_lr, target_names=le.classes_))
print("Macro F1:", f1_score(y_test, y_pred_lr, average="macro"))

# 6. Random Forest baseline
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("\n=== Random Forest ===")
print(classification_report(y_test, y_pred_rf, target_names=le.classes_))
print("Macro F1:", f1_score(y_test, y_pred_rf, average="macro"))

# 7. Clustering exploration (unsupervised)
print("\n=== KMeans Clustering on Ingredient Vectors ===")
n_clusters = len(np.unique(y))
kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(X)

sil_score = silhouette_score(X, cluster_labels)
print(f"Silhouette Score: {sil_score:.4f}")

# Map clusters back to cuisines (rough exploratory mapping)
df["cluster"] = cluster_labels
cluster_summary = (
    df.groupby("cluster")["cuisine"]
      .apply(lambda x: x.value_counts().head(3))
)
print("\nTop cuisines per cluster:\n", cluster_summary)

# 8. Save model results
results = {
    "log_reg_macro_f1": f1_score(y_test, y_pred_lr, average="macro"),
    "rf_macro_f1": f1_score(y_test, y_pred_rf, average="macro"),
    "silhouette_score": sil_score
}
pd.DataFrame([results]).to_csv("model_results.csv", index=False)

print("\nResults saved to model_results.csv")


================================================================================
FILE: networks\modeling_extended.py
================================================================================
# modeling_ext.py
import re, os, json
from pathlib import Path
import numpy as np
import pandas as pd
from itertools import combinations
from collections import Counter, defaultdict

from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder, normalize
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, f1_score, silhouette_score, adjusted_rand_score
from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering
from scipy import sparse
    
import networkx as nx

OUT = Path("reports/phase2")
OUT.mkdir(parents=True, exist_ok=True)

DATA = "data/encoded/combined_raw_datasets_with_cuisine_encoded.parquet"
RANDOM_STATE = 42
TEST_SIZE = 0.2

# ----------------------
# Light normalization
# ----------------------
UNIT_RX = r"\b(oz|ounce|ounces|cup|cups|tsp|teaspoon|teaspoons|tbsp|tablespoon|tablespoons|g|kg|ml|l|lb|lbs|pound|pounds)\b"
def norm_ingredients(s: str) -> str:
    s = str(s).lower()
    s = re.sub(r"\d+([\/\.\d]*)?", " ", s)
    s = re.sub(UNIT_RX, " ", s)
    s = re.sub(r"[^a-z\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def preprocess_cuisine(c: str) -> str:
    c = str(c).lower().strip()
    c = re.sub(r"\s*,\s*", ",", c)
    c = re.sub(r"\brecipes?\b", "", c).strip()
    if "," in c:  # single-label baseline: keep first
        c = c.split(",")[0]
    return c

def main():
    df = pd.read_parquet(DATA)
    df = df.dropna(subset=["ingredients", "cuisine"]).copy()
    df["ingredients_raw"] = df["ingredients"].astype(str)
    df["ingredients_clean"] = df["ingredients_raw"].map(norm_ingredients)
    df["cuisine"] = df["cuisine"].map(preprocess_cuisine)

    # Keep only cuisines with minimal support to stabilize reports
    vc = df["cuisine"].value_counts()
    keep = vc[vc >= 100].index  # adjust as needed
    if len(keep) < 6:
        keep = vc.head(15).index
    df = df[df["cuisine"].isin(keep)].copy()
    df.reset_index(drop=True, inplace=True)

    # ----------------------
    # TF-IDF + baseline LR
    # ----------------------
    tfidf = TfidfVectorizer(analyzer="word", ngram_range=(1,2), min_df=5, max_df=0.7, stop_words="english", max_features=40000)
    X_tfidf = tfidf.fit_transform(df["ingredients_clean"])
    # ran into somezeros could be just from lousy data starting out
    nonzero_mask = (X_tfidf.getnnz(axis=1) > 0)

    # Keep only rows with at least one nonzero feature
    df = df.loc[nonzero_mask].reset_index(drop=True)
    X_tfidf = X_tfidf[nonzero_mask]


    le = LabelEncoder()
    y = le.fit_transform(df["cuisine"])

    X_train, X_test, y_train, y_test = train_test_split(
        X_tfidf, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
    )

    lr = LogisticRegression(
        max_iter=2000, solver="saga", n_jobs=-1,
        penalty="l2", class_weight="balanced", multi_class="multinomial",
        random_state=RANDOM_STATE
    )
    lr.fit(X_train, y_train)
    y_pred = lr.predict(X_test)

    rep = classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True, zero_division=0)
    pd.DataFrame(rep).T.to_csv(OUT/"classification_report_tfidf_only.csv")

    # ----------------------
    # Interpretability: top features per cuisine
    # ----------------------
    feat_names = np.array(tfidf.get_feature_names_out())
    top_rows = []
    coefs = lr.coef_  # [n_classes, n_features]
    for i, lab in enumerate(le.classes_):
        top_idx = np.argsort(coefs[i])[-25:][::-1]
        for rank, j in enumerate(top_idx, 1):
            top_rows.append({"cuisine": lab, "rank": rank, "feature": feat_names[j], "coef": float(coefs[i, j])})
    pd.DataFrame(top_rows).to_csv(OUT/"top_features_per_cuisine.csv", index=False)

    # ----------------------
    # Build Ingredient Co-occurrence Network
    # ----------------------
    # Tokenize per recipe (use cleaned)
    token_lists = [t.split() for t in df["ingredients_clean"].tolist()]
    # Count ingredient occurrences & co-occurrences
    ing_counts = Counter()
    co_counts = Counter()
    for toks in token_lists:
        uniq = sorted(set(toks))
        ing_counts.update(uniq)
        for a, b in combinations(uniq, 2):
            co_counts[(a, b)] += 1

    ing_df = pd.DataFrame(ing_counts.items(), columns=["ingredient", "count"]).sort_values("count", ascending=False)
    ing_df.to_csv(OUT/"ingredient_counts.csv", index=False)

    # keep edges above a small threshold to avoid hairball
    EDGE_MIN = max(3, int(0.0005 * len(df)))  # dynamic-ish
    edges = [(a, b, w) for (a, b), w in co_counts.items() if w >= EDGE_MIN]

    G = nx.Graph()
    G.add_nodes_from(ing_df["ingredient"].tolist())
    for a, b, w in edges:
        G.add_edge(a, b, weight=int(w))

    # Centralities & communities
    deg = dict(G.degree())
    btw = nx.betweenness_centrality(G, k=min(500, len(G)), seed=RANDOM_STATE)
    try:
        comms = list(nx.algorithms.community.louvain_communities(G, seed=RANDOM_STATE, weight="weight"))
    except Exception:
        # Fallback: greedy modularity if louvain not available
        comms = list(nx.algorithms.community.greedy_modularity_communities(G, weight="weight"))

    com_id = {}
    for idx, cset in enumerate(comms):
        for n in cset:
            com_id[n] = idx

    net_rows = []
    for n in G.nodes():
        net_rows.append({
            "ingredient": n,
            "degree": int(deg.get(n, 0)),
            "betweenness": float(btw.get(n, 0.0)),
            "community": int(com_id.get(n, -1)),
            "occurrences": int(ing_counts.get(n, 0))
        })
    net_df = pd.DataFrame(net_rows).sort_values(["community", "degree"], ascending=[True, False])
    net_df.to_csv(OUT/"network_nodes_centrality.csv", index=False)

    # Optional interactive HTML (pyvis) — safe to skip if not installed
    try:
        from pyvis.network import Network
        net = Network(height="720px", width="100%", bgcolor="#ffffff", notebook=False, directed=False)
        net.barnes_hut()
        # Add a subset of high-degree nodes to keep HTML manageable
        keep_nodes = set(net_df.sort_values("degree", ascending=False).head(600)["ingredient"])
        for a, b, w in edges:
            if a in keep_nodes and b in keep_nodes:
                net.add_node(a, title=a)
                net.add_node(b, title=b)
                net.add_edge(a, b, value=w)
        net.show(str(OUT/"ingredient_network.html"))
    except Exception as e:
        with open(OUT/"_pyvis_warning.txt", "w") as f:
            f.write(f"pyvis not available or render failed: {e}\n")

    # ----------------------
    # Graph-derived features per recipe
    #   Sum/mean/max centrality of recipe tokens, one-hot communities
    # ----------------------
    deg_s = net_df.set_index("ingredient")["degree"]
    btw_s = net_df.set_index("ingredient")["betweenness"]
    com_s = net_df.set_index("ingredient")["community"]

    graph_feat = []
    all_coms = sorted(net_df["community"].unique().tolist())
    com_index = {c:i for i, c in enumerate(all_coms)}

    for toks in token_lists:
        toks = [t for t in set(toks) if t in deg_s.index]
        if toks:
            deg_vals = deg_s.loc[toks].values
            btw_vals = btw_s.loc[toks].values
            com_vals = com_s.loc[toks].values
        else:
            deg_vals = np.array([0.0]); btw_vals = np.array([0.0]); com_vals = np.array([-1])

        row = {
            "deg_sum": float(deg_vals.sum()), "deg_mean": float(deg_vals.mean()), "deg_max": float(deg_vals.max()),
            "btw_sum": float(btw_vals.sum()), "btw_mean": float(btw_vals.mean()), "btw_max": float(btw_vals.max())
        }
        # simple community bag
        com_vec = np.zeros(len(all_coms), dtype=float)
        for c in com_vals:
            if c in com_index:
                com_vec[com_index[c]] += 1.0
        # store as ndarray; will hstack later
        graph_feat.append((row, com_vec))

    graph_tab = pd.DataFrame([r for r,_ in graph_feat])
    com_mat = np.vstack([v for _,v in graph_feat])  # [n_samples, n_coms]
    graph_tab.to_csv(OUT/"graph_features_table.csv", index=False)

    # Combine TF-IDF with graph features
    
    X_graph = sparse.hstack([X_tfidf, sparse.csr_matrix(graph_tab.values), sparse.csr_matrix(com_mat)], format="csr")

    Xg_train, Xg_test, yg_train, yg_test = train_test_split(
        X_graph, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
    )
    lr_g = LogisticRegression(
        max_iter=2000, solver="saga", n_jobs=-1,
        penalty="l2", class_weight="balanced", multi_class="multinomial",
        random_state=RANDOM_STATE
    )
    lr_g.fit(Xg_train, yg_train)
    ypred_g = lr_g.predict(Xg_test)
    rep_g = classification_report(yg_test, ypred_g, target_names=le.classes_, output_dict=True, zero_division=0)
    pd.DataFrame(rep_g).T.to_csv(OUT/"classification_report_tfidf_plus_graph.csv")

    # ----------------------
    # Improved clustering views
    # ----------------------
    # Cosine-like k-means (normalize vectors)
    Xn = normalize(X_tfidf, norm="l2", copy=True)
    k_vals = [8, 12, 16, 20, 25, 30]
    rows = []
    for k in k_vals:
        km = MiniBatchKMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10, batch_size=4096)
        lab = km.fit_predict(Xn)
        sil = silhouette_score(Xn, lab)
        ari = adjusted_rand_score(y, lab)  # uses your labels as a rough sanity check
        rows.append({"k": k, "silhouette": float(sil), "ari_vs_labels": float(ari)})
    pd.DataFrame(rows).to_csv(OUT/"clustering_quality_kmeans.csv", index=False)

    # Agglomerative clustering (cosine via precomputed 1-cos sim on reduced space)
    # To keep it light, project to 300 dims with TruncatedSVD
    # SVD reduction (as you had)
    
    
    svd = TruncatedSVD(n_components=300, random_state=RANDOM_STATE)
    X_svd = svd.fit_transform(X_tfidf)
    Xs = normalize(X_svd, norm="l2")

    # Drop any residual zero rows (very rare but safe)
    row_norms = np.linalg.norm(Xs, axis=1)
    nz_mask = row_norms > 0
    Xs_nz = Xs[nz_mask]
    y_nz  = y[nz_mask]


    # ----------------------
    # Agglomerative clustering (subset to avoid O(n^2) memory blowup)
    # ----------------------
    from sklearn.cluster import AgglomerativeClustering
    from sklearn.metrics import silhouette_score, adjusted_rand_score

    # Current dataset size
    n_samples = Xs_nz.shape[0]

    # Max samples we allow for hierarchical clustering
    max_for_agg = 15000  # adjust based on RAM; 10k–15k is typical

    # Subsample if necessary
    if n_samples > max_for_agg:
        rng = np.random.RandomState(RANDOM_STATE)
        idx_sample = rng.choice(n_samples, size=max_for_agg, replace=False)
        X_agg = Xs_nz[idx_sample]
        y_agg = y_nz[idx_sample]
        print(f"[Agglomerative] Using subset of {max_for_agg} / {n_samples} samples")
    else:
        idx_sample = np.arange(n_samples)
        X_agg = Xs_nz
        y_agg = y_nz
        print(f"[Agglomerative] Using full dataset ({n_samples} samples)")

    # Run the agglomerative model (cosine metric is supported in modern sklearn)
    agg = AgglomerativeClustering(
        n_clusters=20,
        metric="cosine",
        linkage="average"
    )

    agg_lab = agg.fit_predict(X_agg)

    # Metrics computed only on the subset
    sil_agg = silhouette_score(X_agg, agg_lab, metric="cosine")
    ari_agg = adjusted_rand_score(y_agg, agg_lab)

    # Save metrics + cluster membership for subset only
    pd.DataFrame([{
        "k": 20,
        "n_samples_used": len(X_agg),
        "silhouette": float(sil_agg),
        "ari_vs_labels": float(ari_agg)
    }]).to_csv(OUT / "clustering_quality_agglomerative.csv", index=False)

    # Save subset assignments (for visualization)
    pd.DataFrame({
        "index_in_full": idx_sample,
        "cluster": agg_lab,
        "true_label": y_agg
    }).to_csv(OUT / "agg_clusters_subset.csv", index=False)

    print(f"Agglomerative clustering done on subset of size {len(X_agg)}")
    print(f"Silhouette: {sil_agg:.4f}, ARI: {ari_agg:.4f}")

    # ----------------------
    # Pairing recommender via PMI/Lift on co-occurrence
    # ----------------------
    N_recipes = len(df)
    # probabilities
    p_ing = {ing: cnt / N_recipes for ing, cnt in ing_counts.items() if cnt >= 3}
    pair_rows = []
    for (a,b), w in co_counts.items():
        if w < EDGE_MIN:
            continue
        pa = p_ing.get(a); pb = p_ing.get(b)
        if not pa or not pb:
            continue
        pab = w / N_recipes
        lift = pab / (pa * pb)
        pmi = np.log2(lift) if lift > 0 else 0.0
        pair_rows.append({"a": a, "b": b, "co_count": int(w), "lift": float(lift), "pmi": float(pmi)})

    pair_df = pd.DataFrame(pair_rows).sort_values(["pmi", "co_count"], ascending=[False, False])
    pair_df.to_csv(OUT/"pairings_pmi.csv", index=False)

    # Example: top suggested pairings per cuisine (based on ingredients present in that cuisine)
    # Build quick cuisine→ingredient frequency map
    cui_ing = defaultdict(Counter)
    for toks, lab in zip(token_lists, df["cuisine"].tolist()):
        for t in set(toks):
            cui_ing[lab][t] += 1
    # For each cuisine: top ingredients and their best partners by PMI
    sugg_rows = []
    pair_idx = {(row.a, row.b): (row.lift, row.pmi) for row in pair_df.itertuples(index=False)}
    pair_idx.update({(b,a):(l,p) for (a,b),(l,p) in pair_idx.items()})
    for lab, cnts in cui_ing.items():
        top_ings = [w for w,_ in cnts.most_common(50)]
        for ing in top_ings:
            best = []
            for other in top_ings:
                if other == ing: continue
                if (ing, other) in pair_idx:
                    lift, pmi = pair_idx[(ing, other)]
                    best.append((other, lift, pmi))
            best.sort(key=lambda x: (x[2], x[1]), reverse=True)
            for partner, lift, pmi in best[:5]:
                sugg_rows.append({"cuisine": lab, "ingredient": ing, "partner": partner, "lift": float(lift), "pmi": float(pmi)})
    pd.DataFrame(sugg_rows).to_csv(OUT/"pairings_by_cuisine.csv", index=False)

    # ----------------------
    # Summary
    # ----------------------
    def safe_macro_f1(report_path):
        path = OUT / report_path
        if not path.exists():
            return None
        df = pd.read_csv(path)
        # Handle either format (label as column or index)
        if "label" in df.columns:
            row = df.loc[df["label"] == "macro avg"]
            if not row.empty:
                return float(row["f1-score"].iloc[0])
        elif "Unnamed: 0" in df.columns:
            df = df.rename(columns={"Unnamed: 0": "label"})
            row = df.loc[df["label"] == "macro avg"]
            if not row.empty:
                return float(row["f1-score"].iloc[0])
        else:
            # fallback: try using index if CSV saved with index only
            df = pd.read_csv(path, index_col=0)
            if "macro avg" in df.index:
                return float(df.loc["macro avg", "f1-score"])
        return None

    
    summary = {
        "tfidf_only_macro_f1": safe_macro_f1("classification_report_tfidf_only.csv"),
        "tfidf_plus_graph_macro_f1": safe_macro_f1("classification_report_tfidf_plus_graph.csv"),
        "kmeans_grid": "see clustering_quality_kmeans.csv",
        "agglomerative_silhouette_subset": float(sil_agg),
        "agglomerative_ari_subset": float(ari_agg),
        "artifacts": [
            "top_features_per_cuisine.csv",
            "ingredient_counts.csv",
            "network_nodes_centrality.csv",
            "ingredient_network.html (if generated)",
            "graph_features_table.csv",
            "pairings_pmi.csv",
            "pairings_by_cuisine.csv",
            "agg_clusters_subset.csv"
        ]
    }


    json.dump(summary, open(OUT / "summary.json", "w"), indent=2)
    print("Phase-2 artifacts written to:", OUT.resolve())

if __name__ == "__main__":
    main()


================================================================================
FILE: networks\modeling_extended_faster.py
================================================================================
#!/usr/bin/env python
"""
Lightweight modeling_extended.py for large recipe+cuisine dataset.

What it does:
- Loads a combined recipes CSV with columns: cuisine, ingredients or ingredients_clean
- Builds a TF-IDF representation of ingredients
- Trains a logistic regression classifier (TF-IDF only)
- Evaluates with classification_report + macro F1
- Runs a single MiniBatchKMeans clustering on TF-IDF
- Saves:
    - classification_report_tfidf_only.csv
    - clustering_quality_kmeans.csv
    - top_features_per_cuisine.csv
    - ingredient_counts.csv
    - summary.json
"""

import json
from pathlib import Path
from collections import Counter

import numpy as np
import pandas as pd

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, normalize
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report,
    f1_score,
    silhouette_score,
    adjusted_rand_score,
)
from sklearn.cluster import MiniBatchKMeans
from sklearn.exceptions import ConvergenceWarning
import warnings


# =========================
# CONFIG
# =========================
RANDOM_STATE = 42

# Path to your merged dataset (update this!)
INPUT_CSV = Path("data/encoded/combined_raw_datasets_with_cuisine_encoded.parquet")

# Where to write modeling outputs
OUT = Path("../../reports/phase2")
OUT.mkdir(parents=True, exist_ok=True)

# Runtime controls
FAST_MODE = True

# Max rows to use (for both classification and clustering)
MAX_RECIPES = 40000

# TF-IDF config
TFIDF_MAX_FEATURES = 20000
MIN_SAMPLES_PER_CLASS = 10  # drop extremely rare cuisines

# Clustering config
KMEANS_K = 40
MAX_SAMPLES_FOR_CLUSTERING = 40000  # can be <= MAX_RECIPES


# =========================
# HELPERS
# =========================
def choose_text_column(df: pd.DataFrame) -> str:
    """
    Choose which column to use as ingredient text.
    Prefers 'ingredients_clean', falls back to 'ingredients'.
    """
    if "ingredients_clean" in df.columns:
        return "ingredients_clean"
    elif "ingredients" in df.columns:
        return "ingredients"
    else:
        raise ValueError(
            "Expected a column named 'ingredients_clean' or 'ingredients' in the input CSV."
        )


def compute_top_features_per_cuisine(
    X_tfidf, feature_names, cuisines, top_n=25
) -> pd.DataFrame:
    """
    For each cuisine, compute mean TF-IDF and take top_n features.
    Returns a long-format DataFrame.
    """
    cuisines = np.array(cuisines)
    rows = []
    unique_cuis = np.unique(cuisines)

    for c in unique_cuis:
        mask = cuisines == c
        if mask.sum() == 0:
            continue
        mean_vec = X_tfidf[mask].mean(axis=0)
        mean_vec = np.asarray(mean_vec).ravel()
        # top indices
        top_idx = np.argsort(mean_vec)[::-1][:top_n]
        for rank, j in enumerate(top_idx, start=1):
            rows.append(
                {
                    "cuisine": c,
                    "feature": feature_names[j],
                    "mean_tfidf": float(mean_vec[j]),
                    "rank": rank,
                }
            )

    return pd.DataFrame(rows)


def safe_macro_f1(report_path: Path) -> float | None:
    """
    Read a classification_report CSV and pull out macro avg F1 if present.
    Handles both 'label' as column or index.
    """
    if not report_path.exists():
        return None

    df = pd.read_csv(report_path)

    # common pattern: label column = 'label'
    if "label" in df.columns:
        row = df.loc[df["label"] == "macro avg"]
        if not row.empty and "f1-score" in row.columns:
            return float(row["f1-score"].iloc[0])

    # sometimes first column is unnamed index
    if "Unnamed: 0" in df.columns:
        df = df.rename(columns={"Unnamed: 0": "label"})
        row = df.loc[df["label"] == "macro avg"]
        if not row.empty and "f1-score" in row.columns:
            return float(row["f1-score"].iloc[0])

    # last fallback: try using index
    df_idx = pd.read_csv(report_path, index_col=0)
    if "macro avg" in df_idx.index and "f1-score" in df_idx.columns:
        return float(df_idx.loc["macro avg", "f1-score"])

    return None


# =========================
# MAIN
# =========================
def main():
    print(f"Loading data from {INPUT_CSV.resolve()}")
    df = pd.read_parquet(INPUT_CSV)

    # Basic checks
    if "cuisine" not in df.columns:
        raise ValueError("Input CSV must contain a 'cuisine' column.")

    text_col = choose_text_column(df)
    print(f"Using text column: {text_col}")

    # Drop rows with missing text or cuisine
    df = df.dropna(subset=[text_col, "cuisine"]).copy()

    # Optional downsample for speed
    if FAST_MODE and len(df) > MAX_RECIPES:
        df = df.sample(MAX_RECIPES, random_state=RANDOM_STATE).reset_index(drop=True)
        print(f"[FAST_MODE] Downsampled to {len(df)} recipes.")

    # Normalize cuisine labels
    df["cuisine"] = df["cuisine"].astype(str).str.strip().str.lower()

    # Drop extremely rare cuisines so stratified split doesn't blow up
    counts = df["cuisine"].value_counts()
    keep_cuis = counts[counts >= MIN_SAMPLES_PER_CLASS].index
    dropped = set(counts.index) - set(keep_cuis)
    if dropped:
        print(
            f"Dropping {len(dropped)} cuisines with < {MIN_SAMPLES_PER_CLASS} samples "
            f"(total dropped rows: {len(df) - len(df[df['cuisine'].isin(keep_cuis)])})"
        )
        df = df[df["cuisine"].isin(keep_cuis)].copy()

    print(f"Final dataset size for modeling: {len(df)} rows, {df['cuisine'].nunique()} cuisines")

    # =========================
    # TF-IDF
    # =========================
    print("Vectorizing ingredients with TF-IDF...")
    tfidf = TfidfVectorizer(
        analyzer="word",
        ngram_range=(1, 2),
        min_df=3,
        max_df=0.7,
        stop_words="english",
        max_features=TFIDF_MAX_FEATURES,
    )

    X_tfidf = tfidf.fit_transform(df[text_col].astype(str))
    feature_names = np.array(tfidf.get_feature_names_out())
    y = df["cuisine"].values

    # Only keep non-zero rows (should be all, but safe)
    nonzero = X_tfidf.getnnz(axis=1) > 0
    X_tfidf = X_tfidf[nonzero]
    y = y[nonzero]
    df = df.loc[nonzero].reset_index(drop=True)

    # Encode labels
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X_tfidf,
        y_encoded,
        test_size=0.2,
        random_state=RANDOM_STATE,
        stratify=y_encoded,
    )

    # =========================
    # Logistic Regression classifier
    # =========================
    print("Training Logistic Regression (TF-IDF only)...")
    warnings.filterwarnings("ignore", category=ConvergenceWarning)

    clf = LogisticRegression(
        max_iter=200,
        n_jobs=-1,
        solver="saga",
        multi_class="multinomial",
        verbose=0,
        random_state=RANDOM_STATE,
    )
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)
    macro_f1 = f1_score(y_test, y_pred, average="macro")
    acc = (y_pred == y_test).mean()

    print(f"LogReg TF-IDF only: accuracy={acc:.4f}, macro_F1={macro_f1:.4f}")

    # classification report -> CSV
    report_dict = classification_report(
        y_test,
        y_pred,
        target_names=le.classes_,
        output_dict=True,
        zero_division=0,
    )
    report_df = pd.DataFrame(report_dict).T
    report_df.insert(0, "label", report_df.index)
    report_df.to_csv(OUT / "classification_report_tfidf_only.csv", index=False)

    # =========================
    # KMeans clustering (MiniBatch) on TF-IDF
    # =========================
    print("Running MiniBatchKMeans clustering...")

    # optionally downsample for clustering if very large
    n_samples = X_tfidf.shape[0]
    if n_samples > MAX_SAMPLES_FOR_CLUSTERING:
        rng = np.random.RandomState(RANDOM_STATE)
        idx_cluster = rng.choice(n_samples, size=MAX_SAMPLES_FOR_CLUSTERING, replace=False)
        X_cluster = X_tfidf[idx_cluster]
        y_cluster = y_encoded[idx_cluster]
        print(
            f"[FAST_MODE] Clustering on subset of {MAX_SAMPLES_FOR_CLUSTERING} / {n_samples} recipes."
        )
    else:
        idx_cluster = np.arange(n_samples)
        X_cluster = X_tfidf
        y_cluster = y_encoded

    # normalize rows for cosine-like geometry
    X_cluster_norm = normalize(X_cluster, norm="l2")

    km = MiniBatchKMeans(
        n_clusters=KMEANS_K,
        random_state=RANDOM_STATE,
        batch_size=4096,
        n_init="auto",
    )
    km_labels = km.fit_predict(X_cluster_norm)

    sil_kmeans = silhouette_score(X_cluster_norm, km_labels, metric="cosine")
    ari_kmeans = adjusted_rand_score(y_cluster, km_labels)

    print(f"KMeans (k={KMEANS_K}) silhouette={sil_kmeans:.4f}, ARI vs cuisine={ari_kmeans:.4f}")

    pd.DataFrame(
    [
        {
            "k": KMEANS_K,
            "n_samples_used": X_cluster.shape[0],
            "silhouette": float(sil_kmeans),
            "ari_vs_labels": float(ari_kmeans),
        }
    ]
    ).to_csv(OUT / "clustering_quality_kmeans.csv", index=False)

    # =========================
    # Top features per cuisine
    # =========================
    print("Computing top TF-IDF features per cuisine...")
    top_feat_df = compute_top_features_per_cuisine(
        X_tfidf, feature_names, y, top_n=25
    )
    top_feat_df.to_csv(OUT / "top_features_per_cuisine.csv", index=False)

    # =========================
    # Ingredient counts (very rough, string-based)
    # =========================
    print("Computing ingredient frequency counts...")
    counts = Counter()
    for txt in df[text_col].astype(str):
        # naive split on comma, strip
        parts = [p.strip().lower() for p in txt.split(",") if p.strip()]
        counts.update(parts)

    ing_rows = [
        {"ingredient": ing, "count": cnt} for ing, cnt in counts.most_common()
    ]
    pd.DataFrame(ing_rows).to_csv(OUT / "ingredient_counts.csv", index=False)

    # =========================
    # Summary JSON
    # =========================
    print("Writing summary.json ...")
    summary = {
        "logreg_tfidf_accuracy": float(acc),
        "logreg_tfidf_macro_f1": safe_macro_f1(
            OUT / "classification_report_tfidf_only.csv"
        ),
        "kmeans_k": int(KMEANS_K),
        "kmeans_silhouette": float(sil_kmeans),
        "kmeans_ari_vs_labels": float(ari_kmeans),
        "n_recipes_used": int(len(df)),
        "n_cuisines": int(df["cuisine"].nunique()),
        "artifacts": [
            "classification_report_tfidf_only.csv",
            "clustering_quality_kmeans.csv",
            "top_features_per_cuisine.csv",
            "ingredient_counts.csv",
        ],
    }

    with open(OUT / "summary.json", "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2)

    print("Done. Phase-2 artifacts written to:", OUT.resolve())


if __name__ == "__main__":
    main()


================================================================================
FILE: networks\pipeline.py
================================================================================
import os
import re
import json
import numpy as np
import pandas as pd
from pathlib import Path
from collections import Counter, defaultdict

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder, normalize
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, silhouette_score
from sklearn.cluster import MiniBatchKMeans

# ----------------------------
# Config
# ----------------------------
DATA_PATH = "combined_recipes.csv"
OUT_DIR = Path("reports/baseline")
OUT_DIR.mkdir(parents=True, exist_ok=True)

TEST_SIZE = 0.2
RANDOM_STATE = 42

# for clustering sweep
K_GRID = [8, 12, 16, 20, 25, 30]

# minimum support to keep a cuisine as its own class in taxonomy suggestions
MIN_SUPPORT_FOR_OWN_CLASS = 100  # adjust later

BASIC_PARENT_MAP = {
    # quick heuristic to collapse obvious variants (edit freely later)
    r".*\b(usa|american)\b.*": "american",
    r".*\brit(alian|alian recipes)\b.*": "italian",
    r".*\bind(ian|ian recipes)\b.*": "indian",
    r".*\bmex(ic|ican)\b.*": "mexican",
    r".*\bchin(ese)?\b.*": "chinese",
    r".*\bgreek\b.*": "greek",
    r".*\bfrench\b.*": "french",
    r".*\bkorean\b.*": "korean",
    r".*\bthai\b.*": "thai",
    r".*\bjapan(ese)?\b.*": "japanese",
    r".*\bgerman\b.*": "german",
    r".*\barab(ic)?\b.*": "arabic",
    r".*\bpakistan(i)?\b.*": "pakistani",
    r".*\bcar(r)?ibbean\b.*": "caribbean",
    r".*\bbrit(ish|ain|unitedkingdom)\b.*": "british",
    r".*\bcontinental\b.*": "continental",
    r".*\bmediterranean\b.*": "mediterranean",
}

DROP_LABEL_PATTERNS = [
    r"kid[-\s]?friendly",
    r"recipes?$",
    r"^other$",
    r"^fusion$",
    r"^asian$"  # optional to drop "asian" if it's too broad
]

# ----------------------------
# Helpers
# ----------------------------
def normalize_ingredients(s: str) -> str:
    s = str(s).lower()
    s = re.sub(r"\d+([\/\.\d]*)?", " ", s)  # numbers/fractions
    s = re.sub(r"\b(oz|ounce|ounces|cup|cups|tsp|teaspoon|teaspoons|tbsp|tablespoon|tablespoons|g|kg|ml|l|pound|lb|lbs)\b", " ", s)
    s = re.sub(r"[^a-z\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def preprocess_cuisine(raw: str) -> str:
    c = str(raw).lower().strip()
    c = re.sub(r"\s*,\s*", ",", c)    # normalize commas
    c = re.sub(r"\s+", " ", c).strip()
    # single-label: keep first token if commas are present
    if "," in c:
        c = c.split(",")[0]

    # remove “recipes”
    c = re.sub(r"\brecipes?\b", "", c).strip()

    # drop patterns
    for pat in DROP_LABEL_PATTERNS:
        if re.search(pat, c):
            return "drop_me"

    return c

def suggest_parent_label(cuisine: str) -> str:
    for pat, parent in BASIC_PARENT_MAP.items():
        if re.match(pat, cuisine):
            return parent
    return cuisine  # fallback: keep as-is

def top_n_tokens(X, vectorizer, n=100):
    counts = np.asarray(X.sum(axis=0)).ravel()
    idx = np.argsort(counts)[::-1][:n]
    terms = vectorizer.get_feature_names_out()[idx]
    vals = counts[idx]
    return pd.DataFrame({"token": terms, "count": vals})

def class_report_table(y_true, y_pred, classes):
    rep = classification_report(y_true, y_pred, target_names=classes, output_dict=True, zero_division=0)
    df_rep = pd.DataFrame(rep).T.reset_index().rename(columns={"index": "label"})
    return df_rep

def cm_long(cm, classes):
    cm_df = pd.DataFrame(cm, index=classes, columns=classes)
    cm_long = (cm_df.stack()
               .reset_index(name="count")
               .rename(columns={"level_0": "true", "level_1": "pred"}))
    cm_long = cm_long[cm_long["count"] > 0].sort_values("count", ascending=False)
    return cm_long

def cluster_majority_labels(df_with_clusters, cluster_col="cluster", label_col="cuisine", top_k=1):
    out = []
    for cid, sub in df_with_clusters.groupby(cluster_col):
        counts = sub[label_col].value_counts()
        top = counts.head(top_k)
        for lab, cnt in top.items():
            out.append({"cluster": cid, "label": lab, "count": int(cnt), "support": int(len(sub))})
    return pd.DataFrame(out).sort_values(["cluster", "count"], ascending=[True, False])

# ----------------------------
# Main
# ----------------------------
def main():
    # 1) Load
    df = pd.read_csv(DATA_PATH)
    if not {"ingredients", "cuisine"}.issubset(df.columns):
        raise ValueError("Expected columns 'ingredients' and 'cuisine' in combined_recipes.csv")

    # 2) Light cleanup to proceed
    df = df.dropna(subset=["ingredients", "cuisine"]).copy()
    df["ingredients_clean"] = df["ingredients"].map(normalize_ingredients)

    # Keep a raw cuisine copy for audits
    df["cuisine_raw"] = df["cuisine"]
    df["cuisine"] = df["cuisine"].map(preprocess_cuisine)
    df = df[df["cuisine"] != "drop_me"].copy()

    # 3) Suggest parent/normalized cuisine for taxonomy proposals
    df["cuisine_suggested"] = df["cuisine"].map(suggest_parent_label)

    # 4) TF-IDF
    vec = TfidfVectorizer(
        analyzer="word",
        ngram_range=(1,2),
        min_df=5,
        max_df=0.7,
        stop_words="english",
        max_features=40000
    )
    X = vec.fit_transform(df["ingredients_clean"])

    # 5) Encode label (using suggested parent, but keep both)
    le = LabelEncoder()
    y = le.fit_transform(df["cuisine_suggested"])

    # 6) Optional: enforce minimum support for cleaner baseline report
    # Compute label counts aligned to df's index
    label_series = df["cuisine_suggested"]  # same length/index as df
    vc = label_series.value_counts()

    # Choose which labels to keep
    keep_labels = vc[vc >= MIN_SUPPORT_FOR_OWN_CLASS].index

    # Fallback: if nothing passes the threshold, keep top-N most common labels
    if len(keep_labels) == 0:
        TOP_N = 20
        keep_labels = vc.head(TOP_N).index

    # Build an aligned boolean mask from df, not from y
    keep_mask = df["cuisine_suggested"].isin(keep_labels)

    # Use the mask for both df and X (convert to numpy for the sparse matrix)
    df_kee = df.loc[keep_mask].copy()
    X_kee = X[keep_mask.to_numpy()]

    # Refit label encoder on the filtered frame
    le = LabelEncoder()
    y_kee = le.fit_transform(df_kee["cuisine_suggested"])


    # 7) Split
    X_train, X_test, y_train, y_test = train_test_split(
        X_kee, y_kee, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_kee
    )

    # 8) Baseline classifier (balanced, multinomial)
    clf = LogisticRegression(
        max_iter=2000,
        n_jobs=-1,
        solver="saga",
        penalty="l2",
        class_weight="balanced",
        multi_class="multinomial",
        random_state=RANDOM_STATE,
    )
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    classes = le.classes_
    # 9) Reports
    rep_df = class_report_table(y_test, y_pred, classes)
    rep_df.to_csv(OUT_DIR / "classification_report.csv", index=False)

    cm = confusion_matrix(y_test, y_pred)
    pd.DataFrame(cm, index=classes, columns=classes).to_csv(OUT_DIR / "confusion_matrix_wide.csv")
    cm_long(cm, classes).to_csv(OUT_DIR / "confusion_matrix_long.csv", index=False)

    top_tokens_df = top_n_tokens(X_kee, vec, n=200)
    top_tokens_df.to_csv(OUT_DIR / "top_tokens.csv", index=False)

    summary = {
        "macro_f1": float(rep_df.loc[rep_df["label"] == "macro avg", "f1-score"].values[0]),
        "accuracy": float(rep_df.loc[rep_df["label"] == "accuracy", "precision"].values[0]),
        "classes_kept": classes.tolist(),
        "min_support_for_class": MIN_SUPPORT_FOR_OWN_CLASS,
    }
    json.dump(summary, open(OUT_DIR / "summary.json", "w"), indent=2)

    # 10) K sweep (cosine-like with L2-normalized TF-IDF)
    Xn = normalize(X_kee, norm="l2", copy=False)
    k_sweep = []
    for k in K_GRID:
        km = MiniBatchKMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10, batch_size=4096)
        lab = km.fit_predict(Xn)
        sil = silhouette_score(Xn, lab)
        k_sweep.append({"k": k, "silhouette": float(sil)})
    pd.DataFrame(k_sweep).to_csv(OUT_DIR / "k_sweep.csv", index=False)

    # 11) Pick a k (best silhouette) and build cluster→label map by majority vote
    best_k = sorted(k_sweep, key=lambda d: d["silhouette"], reverse=True)[0]["k"]
    km_final = MiniBatchKMeans(n_clusters=best_k, random_state=RANDOM_STATE, n_init=10, batch_size=4096)
    clusters = km_final.fit_predict(Xn)

    df_kee = df_kee.copy()
    df_kee["cluster"] = clusters
    cluster_map = cluster_majority_labels(df_kee, "cluster", "cuisine_suggested", top_k=1)
    cluster_map.to_csv(OUT_DIR / "cluster_label_map.csv", index=False)

    # 12) Taxonomy suggestions (CSV to edit by hand)
    # Rule: any cuisine with support < MIN_SUPPORT_FOR_OWN_CLASS → suggest parent (via regex map) or "other"
    raw_counts = df["cuisine"].value_counts()
    sug_counts = df["cuisine_suggested"].value_counts()
    rows = []
    for lab, count in raw_counts.items():
        parent = suggest_parent_label(lab)
        if count >= MIN_SUPPORT_FOR_OWN_CLASS:
            suggested = parent  # keep as class
        else:
            suggested = parent if parent != lab else "other"
        rows.append({
            "raw_cuisine": lab,
            "count": int(count),
            "suggested_super_cuisine": suggested
        })
    taxo = pd.DataFrame(rows).sort_values(["suggested_super_cuisine", "count"], ascending=[True, False])
    taxo.to_csv(OUT_DIR / "taxonomy_suggestions.csv", index=False)

    # 13) Save a small README for teammates
    with open(OUT_DIR / "README.txt", "w", encoding="utf-8") as f:
        f.write(
            "Baseline artifacts generated.\n"
            "- classification_report.csv: per-class precision/recall/f1\n"
            "- confusion_matrix_wide.csv / confusion_matrix_long.csv\n"
            "- top_tokens.csv: most frequent tokens in TF-IDF\n"
            "- k_sweep.csv: silhouette by K\n"
            "- cluster_label_map.csv: majority label per cluster (editable)\n"
            "- taxonomy_suggestions.csv: suggested cuisine→super-cuisine mapping (editable)\n"
            "- summary.json: macro-F1, accuracy, kept classes, thresholds\n"
        )

    print(f"Done. Artifacts written to: {OUT_DIR.resolve()}")


if __name__ == "__main__":
    main()


================================================================================
FILE: networks\plots.py
================================================================================
#!/usr/bin/env python
"""
Master visualization script for cuisine–ingredient project.

Generates:
- Cuisine distribution bar chart
- Top ingredients per cuisine heatmap
- Model macro-F1 comparison plot
- Cuisine similarity network map (based on shared top ingredients)
"""

import os
from pathlib import Path
import json

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx

# -------------------------
# Paths / config
# -------------------------
RANDOM_STATE = 42

BASE_DIR = Path(__file__).resolve().parents[2]
DATA_DIR = BASE_DIR / "data"
REPORTS_DIR = "reports/phase2"
VIZ_DIR = "reports/viz"

INPUT_CSV = "data/encoded/combined_raw_datasets_with_cuisine_encoded.parquet"


os.makedirs(VIZ_DIR, exist_ok=True)

sns.set(style="whitegrid", context="talk")


# -------------------------
# Helpers
# -------------------------
def safe_read_csv(path: Path) -> pd.DataFrame | None:
    if not os.path.exists(path):
        print(f"[WARN] File not found: {path}")
        return None
    return pd.read_parquet(path)


def get_macro_f1_from_report(path: Path) -> float | None:
    """Try to pull macro F1 from a classification_report CSV."""
    if not os.path.exists(path):
        return None
    df = pd.read_csv(path)

    # Case 1: has 'label' column
    if "label" in df.columns:
        row = df.loc[df["label"] == "macro avg"]
        if not row.empty and "f1-score" in row.columns:
            return float(row["f1-score"].iloc[0])

    # Case 2: unnamed first column
    if "Unnamed: 0" in df.columns:
        df = df.rename(columns={"Unnamed: 0": "label"})
        row = df.loc[df["label"] == "macro avg"]
        if not row.empty and "f1-score" in row.columns:
            return float(row["f1-score"].iloc[0])

    # Case 3: index as label
    df_idx = pd.read_csv(path, index_col=0)
    if "macro avg" in df_idx.index and "f1-score" in df_idx.columns:
        return float(df_idx.loc["macro avg", "f1-score"])

    return None


# -------------------------
# Plot 1: Cuisine distribution
# -------------------------
def plot_cuisine_distribution(df: pd.DataFrame, out_path: Path, top_n: int = 20) -> None:
    print("[PLOT] Cuisine distribution")
    if "cuisine" not in df.columns:
        print("[WARN] No 'cuisine' column in dataframe; skipping cuisine distribution.")
        return

    counts = (
        df["cuisine"]
        .astype(str)
        .str.strip()
        .str.lower()
        .value_counts()
    )

    if counts.empty:
        print("[WARN] No cuisine counts; skipping cuisine distribution.")
        return

    top = counts.head(top_n)[::-1]  # reverse for nicer barh order

    plt.figure(figsize=(8, 0.4 * len(top) + 2))
    plt.barh(top.index, top.values)
    plt.xlabel("Number of recipes")
    plt.ylabel("Cuisine")
    plt.title(f"Top {len(top)} cuisines by recipe count")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"  -> saved to {out_path}")


# -------------------------
# Plot 2: Top ingredients heatmap
# -------------------------
def plot_top_ingredients_heatmap(
    top_features_path: Path,
    cuisine_counts: pd.Series,
    out_path: Path,
    n_cuisines: int = 10,
    top_per_cuisine: int = 10,
) -> None:
    print("[PLOT] Top ingredients per cuisine heatmap")
    df_top = safe_read_csv(top_features_path)
    if df_top is None:
        print("[WARN] No top_features_per_cuisine.csv; skipping heatmap.")
        return

    required_cols = {"cuisine", "feature", "mean_tfidf", "rank"}
    if not required_cols.issubset(df_top.columns):
        print(f"[WARN] top_features file missing columns {required_cols}; skipping heatmap.")
        return

    # Select top cuisines by recipe count
    chosen_cuisines = cuisine_counts.head(n_cuisines).index.tolist()
    df_top = df_top[df_top["cuisine"].isin(chosen_cuisines)]
    df_top = df_top[df_top["rank"] <= top_per_cuisine]

    if df_top.empty:
        print("[WARN] No data after filtering top cuisines/features; skipping heatmap.")
        return

    # Pivot: cuisines x ingredients
    pivot = df_top.pivot(index="cuisine", columns="feature", values="mean_tfidf").fillna(0.0)

    # Order cuisines by overall count
    pivot = pivot.loc[[c for c in chosen_cuisines if c in pivot.index]]

    plt.figure(figsize=(1.2 * pivot.shape[1], 0.5 * pivot.shape[0] + 3))
    sns.heatmap(
        pivot,
        cmap="viridis",
        cbar_kws={"label": "Mean TF-IDF"},
        linewidths=0.3,
        linecolor="gray",
    )
    plt.xlabel("Ingredient")
    plt.ylabel("Cuisine")
    plt.title("Top ingredients (by TF-IDF) for top cuisines")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"  -> saved to {out_path}")


# -------------------------
# Plot 3: Model macro-F1 comparison
# -------------------------
def plot_model_performance(out_path: Path) -> None:
    print("[PLOT] Model macro-F1 comparison")
    models = []
    macro_f1s = []

    # TF-IDF only
    tfidf_only_path = REPORTS_DIR + "/classification_report_tfidf_only.csv"
    m_f1 = get_macro_f1_from_report(tfidf_only_path)
    if m_f1 is not None:
        models.append("TF-IDF only")
        macro_f1s.append(m_f1)

    # TF-IDF + graph (if exists)
    tfidf_graph_path = REPORTS_DIR + "/classification_report_tfidf_plus_graph.csv"
    m_f1_graph = get_macro_f1_from_report(tfidf_graph_path)
    if m_f1_graph is not None:
        models.append("TF-IDF + Graph")
        macro_f1s.append(m_f1_graph)

    if not models:
        print("[WARN] No classification reports found; skipping model performance plot.")
        return

    plt.figure(figsize=(6, 4))
    sns.barplot(x=models, y=macro_f1s)
    plt.ylabel("Macro F1-score")
    plt.ylim(0, max(macro_f1s) * 1.1)
    plt.title("Model performance (macro F1)")
    for i, v in enumerate(macro_f1s):
        plt.text(i, v + 0.01, f"{v:.3f}", ha="center", va="bottom", fontsize=10)
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"  -> saved to {out_path}")


# -------------------------
# Plot 4: Cuisine similarity network map
# -------------------------
def plot_cuisine_network(
    top_features_path: Path,
    cuisine_counts: pd.Series,
    out_path: Path,
    n_cuisines: int = 20,
    top_terms_per_cuisine: int = 30,
    min_shared_terms: int = 5,
) -> None:
    print("[PLOT] Cuisine similarity network map")
    df_top = safe_read_csv(top_features_path)
    if df_top is None:
        print("[WARN] No top_features_per_cuisine.csv; skipping network.")
        return

    required_cols = {"cuisine", "feature", "rank"}
    if not required_cols.issubset(df_top.columns):
        print(f"[WARN] top_features file missing columns {required_cols}; skipping network.")
        return

    # Select top cuisines and their top terms
    selected_cuisines = cuisine_counts.head(n_cuisines).index.tolist()
    df_top = df_top[
        (df_top["cuisine"].isin(selected_cuisines))
        & (df_top["rank"] <= top_terms_per_cuisine)
    ]
    if df_top.empty:
        print("[WARN] No data for selected cuisines/terms; skipping network.")
        return

    # Group features by cuisine
    cuisine_to_terms = (
        df_top.groupby("cuisine")["feature"]
        .apply(lambda s: set(s.astype(str)))
        .to_dict()
    )
    cuisines = list(cuisine_to_terms.keys())

    # Build graph
    G = nx.Graph()
    for c in cuisines:
        G.add_node(c, recipes=int(cuisine_counts.get(c, 0)))

    for i in range(len(cuisines)):
        for j in range(i + 1, len(cuisines)):
            ci, cj = cuisines[i], cuisines[j]
            shared = cuisine_to_terms[ci] & cuisine_to_terms[cj]
            if len(shared) >= min_shared_terms:
                G.add_edge(ci, cj, weight=len(shared))

    if G.number_of_nodes() == 0 or G.number_of_edges() == 0:
        print("[WARN] Graph has no edges after thresholding; skipping network.")
        return

    # Layout
    pos = nx.spring_layout(G, seed=RANDOM_STATE, k=0.6, weight="weight")

    # Node sizes based on # recipes
    node_sizes = []
    for n in G.nodes():
        rec = G.nodes[n].get("recipes", 0)
        node_sizes.append(100 + 3 * np.sqrt(rec))

    # Edge widths based on number of shared ingredients
    edge_widths = [1 + 0.4 * G[u][v]["weight"] for u, v in G.edges()]

    plt.figure(figsize=(10, 8))
    nx.draw_networkx_edges(
        G,
        pos,
        width=edge_widths,
        edge_color="lightgray",
        alpha=0.8,
    )
    nodes = nx.draw_networkx_nodes(
        G,
        pos,
        node_size=node_sizes,
        node_color="tab:blue",
        alpha=0.85,
    )
    nx.draw_networkx_labels(G, pos, font_size=9)

    plt.title("Cuisine similarity network (shared top ingredients)")
    plt.axis("off")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"  -> saved to {out_path}")


# -------------------------
# Main
# -------------------------
def main():
    print(f"[INFO] Base dir: {BASE_DIR}")
    print(f"[INFO] Reading recipes from: {INPUT_CSV}")

    
    df = pd.read_parquet(INPUT_CSV)

    if "cuisine" not in df.columns:
        raise ValueError("Input CSV must contain a 'cuisine' column.")

    # Clean cuisine
    df["cuisine"] = (
        df["cuisine"]
        .astype(str)
        .str.strip()
        .str.lower()
    )
    df = df.dropna(subset=["cuisine"]).copy()

    cuisine_counts = df["cuisine"].value_counts()

    # 1) Cuisine distribution
    plot_cuisine_distribution(
        df, VIZ_DIR + "/cuisine_distribution.png", top_n=20
    )

    # 2) Top ingredients heatmap
    plot_top_ingredients_heatmap(
        REPORTS_DIR + "/ top_features_per_cuisine.csv",
        cuisine_counts,
        VIZ_DIR + "/ top_ingredients_heatmap.png",
        n_cuisines=10,
        top_per_cuisine=10,
    )

    # 3) Model macro-F1 comparison
    plot_model_performance(
        VIZ_DIR + "/model_macro_f1.png"
    )

    # 4) Cuisine similarity network
    plot_cuisine_network(
        REPORTS_DIR + "/ top_features_per_cuisine.csv",
        cuisine_counts,
        VIZ_DIR + "/ cuisine_network.png",
        n_cuisines=20,
        top_terms_per_cuisine=30,
        min_shared_terms=5,
    )

    print("[INFO] All visualizations generated in:", VIZ_DIR)


if __name__ == "__main__":
    main()


================================================================================
FILE: networks\visualization.py
================================================================================
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import json
from pathlib import Path
from sklearn.decomposition import PCA

# -----------------------------
# Setup
# -----------------------------
OUT = Path("reports/phase2")  # update if your modeling script saves elsewhere
DATA_PATH = Path("data/encoded/combined_raw_datasets_with_cuisine_encoded.parquet")

# -----------------------------

# Load Data
# -----------------------------
df = pd.read_parquet(DATA_PATH)
summary = json.load(open(OUT / "summary.json"))

print("Loaded data with", len(df), "rows")
print("Summary metrics:")
for k, v in summary.items():
    print(f"  {k}: {v}")

# -----------------------------
# 1. Macro F1 Comparison
# -----------------------------
plt.figure(figsize=(5,4))
scores = {
    "TF-IDF Only": summary.get("tfidf_only_macro_f1"),
    "TF-IDF + Graph": summary.get("tfidf_plus_graph_macro_f1"),
}
sns.barplot(x=list(scores.keys()), y=list(scores.values()), palette="viridis")
plt.title("Macro F1 Comparison of Models")
plt.ylabel("Macro F1 Score")
plt.tight_layout()
plt.savefig(OUT / "viz_macro_f1_comparison.png")
plt.show()

# -----------------------------
# 2. Top Cuisines by Frequency
# -----------------------------
top_cuisines = df["cuisine"].value_counts().head(15)
plt.figure(figsize=(8,5))
sns.barplot(x=top_cuisines.values, y=top_cuisines.index, palette="crest")
plt.title("Most Common Cuisines in Dataset")
plt.xlabel("Recipe Count")
plt.tight_layout()
plt.savefig(OUT / "viz_top_cuisines.png")
plt.show()

# -----------------------------
# 3. PCA Projection (optional)
# -----------------------------
if (OUT / "X_tfidf.csv").exists():
    X_tfidf = pd.read_csv(OUT / "X_tfidf.csv")
    pca = PCA(n_components=2, random_state=42)
    X2d = pca.fit_transform(X_tfidf.values)
    pca_df = pd.DataFrame(X2d, columns=["PC1", "PC2"])
    pca_df["cuisine"] = df["cuisine"]
    plt.figure(figsize=(10,7))
    sns.scatterplot(data=pca_df, x="PC1", y="PC2",
                    hue="cuisine", s=10, legend=False, palette="tab20")
    plt.title("PCA Projection of Recipes by Cuisine")
    plt.tight_layout()
    plt.savefig(OUT / "viz_pca_projection.png")
    plt.show()

# -----------------------------
# 4. Clustering Summary Heatmap (optional)
# -----------------------------
if (OUT / "clustering_quality_kmeans.csv").exists():
    cm = pd.read_csv(OUT / "clustering_quality_kmeans.csv")
    plt.figure(figsize=(8,6))
    sns.heatmap(cm.corr(), cmap="coolwarm", annot=False)
    plt.title("KMeans Clustering Quality Correlation")
    plt.tight_layout()
    plt.savefig(OUT / "viz_kmeans_correlation.png")
    plt.show()


================================================================================
FILE: preprocess_pipeline\common\__init__.py
================================================================================


================================================================================
FILE: preprocess_pipeline\common\logging_setup.py
================================================================================

from __future__ import annotations
import logging
import logging.handlers
from pathlib import Path
from typing import Optional, Dict, Any

def setup_logging(cfg: Optional[Dict[str, Any]] = None, *, force: bool = False) -> None:
    """Initialize root logging once. Safe to call multiple times if force=False."""
    if logging.getLogger().handlers and not force:
        return

    cfg = cfg or {}
    log_cfg = (cfg.get("logging") or {})

    level_name = str(log_cfg.get("level", "INFO")).upper()
    level = getattr(logging, level_name, logging.INFO)

    fmt = log_cfg.get("fmt", "[%(asctime)s] %(levelname)s %(name)s: %(message)s")
    datefmt = log_cfg.get("datefmt", "%Y-%m-%d %H:%M:%S")

    handlers = []

    # Console
    if log_cfg.get("console", True):
        ch = logging.StreamHandler()
        ch.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))
        handlers.append(ch)

    # Rotating file
    file_path = log_cfg.get("file")
    if file_path:
        fp = Path(file_path)
        fp.parent.mkdir(parents=True, exist_ok=True)
        rotate_cfg = log_cfg.get("rotate", {}) or {}
        max_bytes = int(rotate_cfg.get("max_bytes", 10_485_760))  # 10MB
        backup_count = int(rotate_cfg.get("backup_count", 5))
        fh = logging.handlers.RotatingFileHandler(
            filename=str(fp),
            maxBytes=max_bytes,
            backupCount=backup_count,
            encoding="utf-8"
        )
        fh.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))
        handlers.append(fh)

    logging.basicConfig(level=level, handlers=handlers)

    # Quiet noisy deps
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("botocore").setLevel(logging.WARNING)
    logging.getLogger("s3transfer").setLevel(logging.WARNING)

================================================================================
FILE: preprocess_pipeline\config\config.yaml
================================================================================
data:
  # Canonical raw data input (used by both ingrnorm + NER unless overridden)
  input_path: "./data/raw/wilmerarltstrmberg_data.csv" # main dataset
  # input_path: "./data/raw/sample_data.csv" # for quick local testing
  ner_col: "NER"           # list-like column if present
  chunksize: 200000

cleanup:
  enabled: true
  paths:
    - "./data/normalized/recipes_data_clean.parquet"
    - "./data/normalized/cosine_dedupe_map.jsonl"
    - "./data/normalized/recipes_data_clean_spell_dedup.parquet"
    - "./data/encoded/datasets_unified.parquet"
    - "./data/encoded/ingredient_id_to_token.json"
    - "./data/encoded/ingredient_token_to_id.json"

output:
  # ingrnorm artifacts
  baseline_parquet: "./data/normalized/recipes_data_clean.parquet"
  dedup_parquet: "./data/normalized/recipes_data_clean_spell_dedup.parquet"
  cosine_map_path: "./data/normalized/cosine_dedupe_map.jsonl"
  list_col_for_vocab: "NER_clean"

  unified_parquet: "./data/encoded/datasets_unified.parquet"
  ingredient_id_to_token: "./data/encoded/ingredient_id_to_token.json"
  ingredient_token_to_id: "./data/encoded/ingredient_token_to_id.json"

  # NER prediction outputs (single source of truth)
  ner_preds_base: "./data/training/predictions.parquet"   # we can derive *_wide / *_tall from this

stages:
  write_parquet: true
  sbert_dedupe: true
  w2v_dedupe: false
  apply_cosine_map: true
  encode_ids: true

sbert:
  model: "all-MiniLM-L6-v2"
  threshold: 0.88
  topk: 25
  min_len: 2
  require_token_overlap: true
  block_generic_as_canon: true
  min_freq_for_vocab: 2
  spacy_model: "en_core_web_sm"
  # Performance tuning for spaCy normalization
  spacy_batch_size: 1024  # Larger batches = better throughput (try 512-1024 for large datasets)
  spacy_n_process: 4     # 1=single-threaded (safe, works on Windows)
                         # >1=multiprocess (faster but may not work on Windows due to spawn method)
                         # If CPU usage is low, try increasing spacy_batch_size first

w2v:
  vector_size: 100
  window: 5
  min_count: 1
  workers: 4
  sg: 1
  epochs: 8
  threshold: 0.85
  topk: 25
  min_freq_for_vocab: 2

encoder:
  min_freq: 1
  dataset_id: 1
  ingredients_col: "NER_clean"

ner:
  enabled: true

  train_path: "./data/normalized/recipes_data_clean.parquet"
  data_is_parquet: true
  max_rows: null  # Set to a number (e.g., 10000) for debug mode

  # IMPORTANT: Use original NER column (messy) for training, not NER_clean (normalized)
  # The model should learn to handle messy input, then normalization happens via dedupe map
  text_col: null
  ner_list_col: "NER"  # Original messy column - model learns to recognize these
  lexicon_json: null

  # Debug mode options (for faster iteration during development)
  use_tok2vec_debug: false  # If true, use tok2vec instead of transformers (faster, CPU-friendly)
  max_train_docs: 5000      # Limit training docs for quick experiments (null = use all)

  random_seed: 42
  valid_fraction: 0.2
  shard_size: 2000
  n_epochs: 20
  lr: 5e-5
  dropout: 0.1
  transformer_model: "distilbert-base-uncased"
  window: 64
  stride: 48
  freeze_layers: 2
  use_amp: true
  early_stopping_patience: 3
  batch_size: 256
  data_loader_workers: 4  # Number of worker processes for DataLoader (0 = single-threaded)

  out_dir: "./models/ingredient_ner_trf"
  model_dir: "./models/ingredient_ner_trf/model-best"
  # NOTE: dedupe map & encoder maps are **not duplicated here**;
  # ingredient_ner.py will read them from `output.cosine_map_path` & `output.ingredient_*`

logging:
  level: INFO
  console: true
  file: logs/ingrnorm.log
  rotate:
    max_bytes: 10485760
    backup_count: 5
  fmt: "[%(asctime)s] %(levelname)s %(name)s: %(message)s"
  datefmt: "%Y-%m-%d %H:%M:%S"


================================================================================
FILE: preprocess_pipeline\config\cuisine_classifier.yaml
================================================================================
# Cuisine Classifier Training Configuration
# This config is used by train_cuisine_classifier.py for training the cuisine classification model

# Optional: fallback input path (if train_path is not set)
data:
  input_path: "./data/combined_raw_datasets_with_cuisine_encoded.parquet"  # Fallback if cuisine_classifier.train_path is not set
  text_col: "ingredients"  # Column containing text to classify (ingredients list)
  cuisine_col: "cuisine"  # Column containing cuisine labels

# Output paths for artifacts
output:
  preds_base: "./data/training/cuisine_predictions.parquet"   # Cuisine prediction outputs

cuisine_classifier:
  enabled: true

  train_path: "./data/combined_raw_datasets_with_cuisine_encoded.parquet"
  data_is_parquet: true
  max_rows: null # set to null for all rows - otherwise set to a number of rows to load
  
  # Column names
  text_col: "ingredients"  # Column with text to classify (e.g., ingredients list)
  cuisine_col: "cuisine"   # Column with cuisine labels
  
  # Training parameters
  random_seed: 42
  valid_fraction: 0.2
  shard_size: 2000
  n_epochs: 1
  lr: 5e-5
  dropout: 0.1
  transformer_model: "distilbert-base-uncased"
  window: 64
  stride: 48
  freeze_layers: 2
  use_amp: true
  early_stopping_patience: 3
  batch_size: 256
  eval_snapshot_max: 1500
  clear_cache_every: 200
  
  # Debug mode options (for faster iteration during development)
  use_tok2vec_debug: false  # If true, use tok2vec instead of transformers (faster, CPU-friendly)
  max_train_docs: null      # Limit training docs for quick experiments (null = use all)
  
  # Model output paths
  out_dir: "./models/cuisine_classifier_trf"
  model_dir: "./models/cuisine_classifier_trf/model-best"
  
  # Cleanup options
  cleanup_docbins: true  # Remove intermediate docbins after training

logging:
  level: INFO
  console: true
  file: preprocess_pipeline/logs/cuisine_classifier.log
  rotate:
    max_bytes: 10485760
    backup_count: 5
  fmt: "[%(asctime)s] %(levelname)s %(name)s: %(message)s"
  datefmt: "%Y-%m-%d %H:%M:%S"



================================================================================
FILE: preprocess_pipeline\config\cuisnorm.yaml
================================================================================
# Cuisine Normalization Configuration

# This config is used by run_cuisine_norm.py for normalizing and encoding cuisines

# All artifacts are separate from ingredient normalization to avoid conflicts



data:

  # Input dataset with cuisine column

  train_path: "./data/combined_raw_datasets_with_cuisine_encoded.parquet"

  input_path: "./data/combined_raw_datasets_with_cuisine_encoded.parquet"

  cuisine_col: "cuisine"  # Column name containing cuisine data

  chunksize: 200000



cleanup:

  enabled: true  # Cleanup intermediate parquet files, keep JSON artifacts

  paths:

    - "./data/cuisine_normalized/*.parquet"

    - "./data/cuisine_encoded/*.parquet"

    - "./data/cuisine_normalized/*.jsonl"

    - "./data/cuisine_encoded/*.json"

   

    # Note: JSON files are automatically preserved



output:

  # Cuisine normalization artifacts (separate from ingredient artifacts)

  baseline_parquet: "./data/cuisine_normalized/cuisine_baseline.parquet"

  dedup_parquet: "./data/cuisine_normalized/cuisine_deduped.parquet"

  cosine_map_path: "./data/cuisine_normalized/cuisine_dedupe_map.jsonl"

  list_col_for_vocab: "cuisine_clean"



  # Cuisine encoding artifacts (separate from ingredient encoding)

  unified_parquet: "./data/cuisine_encoded/cuisine_unified.parquet"

  cuisine_id_to_token: "./data/cuisine_encoded/cuisine_id_to_token.json"

  cuisine_token_to_id: "./data/cuisine_encoded/cuisine_token_to_id.json"



stages:

  write_parquet: true

  sbert_dedupe: true

  w2v_dedupe: false  # Use SBERT for cuisine deduplication

  apply_cosine_map: true

  encode_ids: true



sbert:

  model: "all-MiniLM-L6-v2"

  threshold: 0.88

  topk: 25

  min_len: 2

  require_token_overlap: true

  block_generic_as_canon: true

  min_freq_for_vocab: 1  # Lower threshold for cuisines (fewer unique values)

  spacy_model: "en_core_web_sm"

  # Performance tuning for spaCy normalization

  spacy_batch_size: 512

  spacy_n_process: 1  # Keep at 1 for Windows compatibility



w2v:

  vector_size: 100

  window: 5

  min_count: 1

  workers: 4

  sg: 1

  epochs: 8

  threshold: 0.85

  topk: 25

  min_freq_for_vocab: 1



encoder:

  min_freq: 1

  dataset_id: 1

  ingredients_col: "cuisine_clean"  # Reusing column name for consistency with pipeline



logging:

  level: INFO

  console: true

  file: pipeline/logs/cuisnorm.log

  rotate:

    max_bytes: 10485760

    backup_count: 5

  fmt: "[%(asctime)s] %(levelname)s %(name)s: %(message)s"

  datefmt: "%Y-%m-%d %H:%M:%S"



================================================================================
FILE: preprocess_pipeline\config\ingredient_ner.yaml
================================================================================
# Ingredient NER Training Configuration
# This config is used by run_ingredient_ner.py for training the NER model

# Optional: fallback input path (if train_path is not set)
data:
  input_path: "./data/raw/wilmerarltstrmberg_data.csv"  # main dataset
  # input_path: "./data/raw/sample_data.csv"  # Fallback if ner.train_path is not set
  ner_col: "NER_clean"  # Used for fallback column name

# Output paths for artifacts created by ingrnorm pipeline
# These are READ by the NER pipeline (dedupe map, encoder maps)
output:
  cosine_map_path: "./data/normalized/cosine_dedupe_map.jsonl"
  ingredient_id_to_token: "./data/encoded/ingredient_id_to_token.json"
  ingredient_token_to_id: "./data/encoded/ingredient_token_to_id.json"
  ner_preds_base: "./data/training/predictions.parquet"   # NER prediction outputs

ner:
  enabled: true

  train_path: "./data/normalized/recipes_data_clean.parquet"
  data_is_parquet: true
  max_rows: null # set to null for all rows - otherwise set to a number of rows to load
  # Use the column that exists in the parquet
  text_col: null
  ner_list_col: "NER_clean"   # <- CHANGE THIS FROM "NER" TO "NER_clean"
  lexicon_json: null

  random_seed: 42
  valid_fraction: 0.2
  shard_size: 2000
  n_epochs: 20
  lr: 5e-5
  dropout: 0.1
  transformer_model: "distilbert-base-uncased"
  window: 64
  stride: 48
  freeze_layers: 2
  use_amp: true
  early_stopping_patience: 3
  batch_size: 1024
  data_loader_workers: 4  # Number of worker processes for DataLoader (0 = single-threaded)

  out_dir: "./models/ingredient_ner_trf"
  model_dir: "./models/ingredient_ner_trf/model-best"


logging:
  level: INFO
  console: true
  file: preprocess_pipeline/logs/ingredient_ner.log
  rotate:
    max_bytes: 10485760
    backup_count: 5
  fmt: "[%(asctime)s] %(levelname)s %(name)s: %(message)s"
  datefmt: "%Y-%m-%d %H:%M:%S"



================================================================================
FILE: preprocess_pipeline\config\ingredient_ner_inference.yaml
================================================================================
# Ingredient NER Inference Configuration
# This config is used by apply_ingredient_ner.py for running inference on new datasets

# Model path (trained NER model)
model:
  model_dir: "./models/ingredient_ner_trf/model-best"  # Path to trained spaCy model

# Input data settings
data:
  # Default input path (can be overridden via --in-path CLI argument)
  input_path: null  # Set to a path if you want a default, or leave null to require --in-path
  data_is_parquet: true  # Auto-detected from file extension if not set

# Output settings
output:
  # Base path for output files (can be overridden via --out-base CLI argument)
  # Will write <base>_wide.parquet and <base>_tall.parquet
  out_base: "./data/inference_output"

# Paths to artifacts created by ingrnorm pipeline
# These are used for normalization and encoding
artifacts:
  # Dedupe map: maps variant phrases → canonical forms
  cosine_map_path: "./data/normalized/cosine_dedupe_map.jsonl"
  # Encoder maps: maps canonical tokens ↔ integer IDs
  ingredient_id_to_token: "./data/encoded/ingredient_id_to_token.json"
  ingredient_token_to_id: "./data/encoded/ingredient_token_to_id.json"

# Inference settings
inference:
  # Default text column name (can be overridden via --text-col CLI argument)
  text_col: null  # Must be specified via CLI or set here
  
  # Performance settings
  batch_size: 256  # Batch size for spaCy processing
  n_process: 1     # Number of processes (keep 1 for GPU/transformers, >1 may not work on Windows)
  use_gpu: true   # Attempt to use GPU (default: CPU for reliability)
  
  # Sampling defaults (can be overridden via CLI)
  sample_n: null      # Randomly sample N rows
  sample_frac: null   # Randomly sample fraction (0.0-1.0)
  head_n: null        # Take first N rows

logging:
  level: INFO
  console: true
  file: preprocess_pipeline/logs/ingredient_ner_inference.log
  rotate:
    max_bytes: 10485760
    backup_count: 5
  fmt: "[%(asctime)s] %(levelname)s %(name)s: %(message)s"
  datefmt: "%Y-%m-%d %H:%M:%S"



================================================================================
FILE: preprocess_pipeline\config\ingrnorm.yaml
================================================================================
data:
  # Canonical raw data input
  input_path: "./data/raw/wilmerarltstrmberg_data.csv" # main dataset
  # input_path: "./data/raw/sample_data.csv" # for quick local testing
  ner_col: "NER"           # list-like column if present
  chunksize: 200000

cleanup:
  enabled: true
  paths:
    - "./data/normalized/recipes_data_clean.parquet"
    - "./data/normalized/cosine_dedupe_map.jsonl"
    - "./data/normalized/recipes_data_clean_spell_dedup.parquet"
    - "./data/encoded/datasets_unified.parquet"
    - "./data/encoded/ingredient_id_to_token.json"
    - "./data/encoded/ingredient_token_to_id.json"

output:
  # ingrnorm artifacts
  baseline_parquet: "./data/normalized/recipes_data_clean.parquet"
  dedup_parquet: "./data/normalized/recipes_data_clean_spell_dedup.parquet"
  cosine_map_path: "./data/normalized/cosine_dedupe_map.jsonl"
  list_col_for_vocab: "NER_clean"

  unified_parquet: "./data/encoded/datasets_unified.parquet"
  ingredient_id_to_token: "./data/encoded/ingredient_id_to_token.json"
  ingredient_token_to_id: "./data/encoded/ingredient_token_to_id.json"

stages:
  write_parquet: true
  sbert_dedupe: true
  w2v_dedupe: false
  apply_cosine_map: true
  encode_ids: true

sbert:
  model: "all-MiniLM-L6-v2"
  threshold: 0.88
  topk: 25
  min_len: 2
  require_token_overlap: true
  block_generic_as_canon: true
  min_freq_for_vocab: 2
  spacy_model: "en_core_web_sm"
  # Performance tuning for spaCy normalization
  spacy_batch_size: 1024  # Larger batches = better throughput (try 512-1024 for large datasets)
  spacy_n_process: 4     # 1=single-threaded (safe, works on Windows)
                         # >1=multiprocess (faster but may not work on Windows due to spawn method)
                         # If CPU usage is low, try increasing spacy_batch_size first

w2v:
  vector_size: 100
  window: 5
  min_count: 1
  workers: 4
  sg: 1
  epochs: 8
  threshold: 0.85
  topk: 25
  min_freq_for_vocab: 2

encoder:
  min_freq: 1
  dataset_id: 1
  ingredients_col: "NER_clean"

logging:
  level: INFO
  console: true
  file: preprocess_pipeline/logs/ingrnorm.log
  rotate:
    max_bytes: 10485760
    backup_count: 5
  fmt: "[%(asctime)s] %(levelname)s %(name)s: %(message)s"
  datefmt: "%Y-%m-%d %H:%M:%S"



================================================================================
FILE: preprocess_pipeline\cuisine_classifier\__init__.py
================================================================================
"""
Cuisine Classifier package: training utilities for cuisine classification.
"""

# Public config interface: three global namespaces + helpers
from .config import (
    DATA,
    TRAIN,
    OUT,
    load_configs_from_yaml,
    load_configs_from_dict,
    print_configs,
)

__all__ = [
    "DATA",
    "TRAIN",
    "OUT",
    "load_configs_from_yaml",
    "load_configs_from_dict",
    "print_configs",
]



================================================================================
FILE: preprocess_pipeline\cuisine_classifier\config.py
================================================================================
# cuisine_classifier/config.py
from __future__ import annotations

from pathlib import Path
from types import SimpleNamespace
from typing import Any, Dict, Tuple

import yaml

# Public globals other modules import
DATA = SimpleNamespace()
TRAIN = SimpleNamespace()
OUT = SimpleNamespace()


def _to_path(v: Any) -> Path | None:
    if v is None:
        return None
    return Path(str(v))


def load_full_yaml(path: str | Path) -> Dict[str, Any]:
    """Load the full pipeline YAML."""
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"Config not found: {path}")
    with open(path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if not isinstance(cfg, dict):
        raise ValueError(f"YAML at {path} did not parse to a dict.")
    return cfg


def _build_data_ns(cfg: Dict[str, Any]) -> SimpleNamespace:
    data_cfg = cfg.get("data") or {}
    classifier_cfg = cfg.get("cuisine_classifier") or {}
    out_cfg = cfg.get("output") or {}

    # 1) training file path
    # Prefer cuisine_classifier.train_path, fallback to data.input_path, then default
    train_path_str = classifier_cfg.get("train_path")
    print(train_path_str)
    if not train_path_str:
        train_path_str = data_cfg.get("input_path")
    if not train_path_str:
        print(data_cfg)
        train_path_str = "./data/combined_raw_datasets_with_cuisine_encoded.parquet"
    train_path = Path(train_path_str)

    # 2) parquet vs csv
    if "data_is_parquet" in classifier_cfg:
        data_is_parquet = bool(classifier_cfg["data_is_parquet"])
    else:
        data_is_parquet = train_path.suffix.lower() == ".parquet"

    # 3) max rows
    max_rows_raw = classifier_cfg.get("max_rows")
    max_rows = int(max_rows_raw) if max_rows_raw is not None else None

    # 4) Column names
    text_col = classifier_cfg.get("text_col") or data_cfg.get("text_col") or "ingredients"
    cuisine_col = classifier_cfg.get("cuisine_col") or data_cfg.get("cuisine_col") or "cuisine"

    return SimpleNamespace(
        DATA_IS_PARQUET=data_is_parquet,
        TRAIN_PATH=train_path,
        MAX_ROWS=max_rows,
        TEXT_COL=text_col,
        CUISINE_COL=cuisine_col,
    )


def _build_train_ns(cfg: Dict[str, Any]) -> SimpleNamespace:
    """Build TRAIN namespace from cuisine_classifier config. Uses UPPERCASE keys to match existing code."""
    classifier_cfg = cfg.get("cuisine_classifier") or {}

    # Debug mode: if use_tok2vec_debug is True, we'll use tok2vec instead of transformers
    use_tok2vec_debug = bool(classifier_cfg.get("use_tok2vec_debug", False))
    
    # If debug mode, reduce epochs for faster iteration
    n_epochs = int(classifier_cfg.get("n_epochs", 20))
    if use_tok2vec_debug:
        n_epochs = min(n_epochs, 5)  # Cap at 5 epochs in debug mode

    return SimpleNamespace(
        RANDOM_SEED=int(classifier_cfg.get("random_seed", 42)),
        VALID_FRACTION=float(classifier_cfg.get("valid_fraction", 0.2)),
        SHARD_SIZE=int(classifier_cfg.get("shard_size", 2000)),
        BATCH_SIZE=int(classifier_cfg.get("batch_size", 256)),
        TRANSFORMER_MODEL=str(classifier_cfg.get("transformer_model", "distilbert-base-uncased")),
        WINDOW=int(classifier_cfg.get("window", 64)),
        STRIDE=int(classifier_cfg.get("stride", 48)),
        LR=float(classifier_cfg.get("lr", 5e-5)),
        DROPOUT=float(classifier_cfg.get("dropout", 0.1)),
        N_EPOCHS=n_epochs,
        FREEZE_LAYERS=int(classifier_cfg.get("freeze_layers", 2)),
        USE_AMP=bool(classifier_cfg.get("use_amp", True)),
        EARLY_STOPPING_PATIENCE=int(classifier_cfg.get("early_stopping_patience", 3)),
        EVAL_SNAPSHOT_MAX=int(classifier_cfg.get("eval_snapshot_max", 1500)),
        CLEAR_CACHE_EVERY=int(classifier_cfg.get("clear_cache_every", 200)),
        USE_TOK2VEC_DEBUG=use_tok2vec_debug,
        MAX_TRAIN_DOCS=int(classifier_cfg.get("max_train_docs")) if classifier_cfg.get("max_train_docs") is not None else None,
    )


def _build_out_ns(cfg: Dict[str, Any]) -> SimpleNamespace:
    classifier_cfg = cfg.get("cuisine_classifier") or {}
    out_cfg = cfg.get("output") or {}

    out_dir = _to_path(classifier_cfg.get("out_dir")) or Path("./models/cuisine_classifier_trf")
    model_dir = _to_path(classifier_cfg.get("model_dir")) or (out_dir / "model-best")

    boot_dir = out_dir / "bootstrapped"
    train_dir = boot_dir / "train"
    valid_dir = boot_dir / "valid"

    pred_out = _to_path(out_cfg.get("preds_base")) or Path("./data/training/cuisine_predictions.parquet")

    return SimpleNamespace(
        OUT_DIR=out_dir,
        MODEL_DIR=model_dir,
        BOOT_DIR=boot_dir,
        TRAIN_DIR=train_dir,
        VALID_DIR=valid_dir,
        PRED_OUT=pred_out,
    )


def _update_ns(target: SimpleNamespace, src: SimpleNamespace) -> None:
    """Mutate an existing SimpleNamespace to have src's attributes."""
    target.__dict__.clear()
    target.__dict__.update(src.__dict__)


# ---------------------- public API ---------------------- #

def load_configs_from_dict(cfg: Dict[str, Any]) -> Tuple[SimpleNamespace, SimpleNamespace, SimpleNamespace]:
    """
    Populate DATA / TRAIN / OUT from an in-memory YAML dict.

    IMPORTANT: we mutate the existing SimpleNamespace objects so
    any 'from cuisine_classifier.config import TRAIN' references see updates.
    """
    global DATA, TRAIN, OUT
    _update_ns(DATA, _build_data_ns(cfg))
    _update_ns(TRAIN, _build_train_ns(cfg))
    _update_ns(OUT, _build_out_ns(cfg))
    return DATA, TRAIN, OUT


def load_configs_from_yaml(path: str | Path) -> Tuple[SimpleNamespace, SimpleNamespace, SimpleNamespace]:
    """Convenience that reads YAML then delegates to load_configs_from_dict."""
    cfg = load_full_yaml(path)
    return load_configs_from_dict(cfg)


def print_configs() -> None:
    from pprint import pprint
    print("DATA:")
    pprint(vars(DATA))
    print("\nTRAIN:")
    pprint(vars(TRAIN))
    print("\nOUT:")
    pprint(vars(OUT))



================================================================================
FILE: preprocess_pipeline\cuisine_classifier\data_prep.py
================================================================================
import math
from pathlib import Path
from typing import List, Tuple

import pandas as pd
import spacy
from sklearn.model_selection import train_test_split
from spacy.tokens import Doc, DocBin
from tqdm import tqdm

from .config import DATA, TRAIN, OUT
from .utils import (
    load_data,
    parse_listlike,
    normalize_cuisine_label,
)


def docs_from_text_and_labels(df: pd.DataFrame, text_col: str, cuisine_col: str) -> List[Doc]:
    """Create spaCy Docs from text column with cuisine labels for text classification."""
    blank = spacy.blank("en")
    out: List[Doc] = []
    
    # First pass: Get unique cuisine labels
    unique_cuisines = set()
    for cuisine_val in df[cuisine_col]:
        cuisine = normalize_cuisine_label(cuisine_val)
        if cuisine:
            unique_cuisines.add(cuisine)
    
    all_labels = sorted(list(unique_cuisines))
    logger = __import__("logging").getLogger(__name__)
    logger.info(f"Found {len(all_labels)} unique cuisine labels")
    
    # Second pass: Create docs with all labels set (exclusive classification)
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Creating docs from text and labels"):
        # Parse text column (may be list-like or string)
        text_val = row[text_col]
        text_tokens = parse_listlike(text_val)
        
        if not text_tokens:
            # Empty text, skip
            continue
        
        # Join tokens into text
        text = ", ".join(text_tokens)
        
        # Get cuisine label
        cuisine = normalize_cuisine_label(row[cuisine_col])
        if not cuisine:
            # Skip rows without valid cuisine labels
            continue
        
        # Create doc
        doc = blank.make_doc(text)
        
        # Set textcat labels (exclusive classification: one True, rest False)
        doc.cats = {label: 1.0 if label == cuisine else 0.0 for label in all_labels}
        
        out.append(doc)
    
    return out


def build_docs_from_config() -> Tuple[List[Doc], List[Doc], str]:
    """High-level helper: build train & valid docs based on DATA + TRAIN config."""
    logger = __import__("logging").getLogger(__name__)
    
    # Debug info
    logger.info(f"[DEBUG] ===== Data Loading Configuration =====")
    logger.info(f"[DEBUG] TRAIN_PATH: {DATA.TRAIN_PATH}")
    logger.info(f"[DEBUG] Absolute path: {DATA.TRAIN_PATH.resolve()}")
    logger.info(f"[DEBUG] File exists: {DATA.TRAIN_PATH.exists()}")
    logger.info(f"[DEBUG] Data format: {'Parquet' if DATA.DATA_IS_PARQUET else 'CSV'}")
    logger.info(f"[DEBUG] TEXT_COL: {DATA.TEXT_COL}")
    logger.info(f"[DEBUG] CUISINE_COL: {DATA.CUISINE_COL}")
    logger.info(f"[DEBUG] ======================================")

    # Load data
    df = load_data(
        DATA.TRAIN_PATH, 
        DATA.DATA_IS_PARQUET, 
        DATA.TEXT_COL, 
        DATA.CUISINE_COL,
        max_rows=DATA.MAX_ROWS
    )
    
    logger.info(f"Loaded {len(df):,} rows from {DATA.TRAIN_PATH}")
    
    # Create docs
    docs_all = docs_from_text_and_labels(df, DATA.TEXT_COL, DATA.CUISINE_COL)
    
    # Apply max_train_docs limit if set (for debug mode)
    if hasattr(TRAIN, 'MAX_TRAIN_DOCS') and TRAIN.MAX_TRAIN_DOCS is not None:
        if len(docs_all) > TRAIN.MAX_TRAIN_DOCS:
            logger.info(f"[DEBUG] Limiting training docs to {TRAIN.MAX_TRAIN_DOCS} for debug mode")
            docs_all = docs_all[:TRAIN.MAX_TRAIN_DOCS]
    
    logger.info(f"Docs prepared: {len(docs_all):,}")
    
    # Get all unique cuisine labels for logging
    all_cuisines = set()
    for doc in docs_all:
        all_cuisines.update(doc.cats.keys())
    logger.info(f"Total unique cuisine labels: {len(all_cuisines)}")
    
    # Split train/valid
    train_docs, valid_docs = train_test_split(
        docs_all, test_size=TRAIN.VALID_FRACTION, random_state=TRAIN.RANDOM_SEED
    )
    logger.info(f"train={len(train_docs):,} | valid={len(valid_docs):,}")

    return train_docs, valid_docs, "text-classification"


def write_docbins(docs: List[Doc], out_dir: Path, shard_size: int) -> None:
    """Write docs into sharded DocBin files."""

    def clean_dir(path: Path):
        path.mkdir(parents=True, exist_ok=True)
        for p in path.glob("*.spacy"):
            logger = __import__("logging").getLogger(__name__)
            logger.info(f"[CLEAN] Removing old shard: {p}")
            p.unlink()
    
    clean_dir(out_dir)

    out_dir.mkdir(parents=True, exist_ok=True)
    n = len(docs)
    shards = max(1, math.ceil(n / max(1, shard_size)))
    for i in range(shards):
        db = DocBin(store_user_data=False)
        for d in docs[i * shard_size: (i + 1) * shard_size]:
            db.add(d)
        db.to_disk(out_dir / f"shard_{i:04d}.spacy")
    logger = __import__("logging").getLogger(__name__)
    logger.info(f"Wrote {n} docs to {out_dir} in {shards} shard(s).")


def prepare_docbins_from_config() -> None:
    """End-to-end: build docs and write train/valid DocBins according to config."""
    train_docs, valid_docs, _ = build_docs_from_config()
    write_docbins(train_docs, OUT.TRAIN_DIR, shard_size=TRAIN.SHARD_SIZE)
    write_docbins(valid_docs, OUT.VALID_DIR, shard_size=TRAIN.SHARD_SIZE)


__all__ = [
    "docs_from_text_and_labels",
    "build_docs_from_config",
    "write_docbins",
    "prepare_docbins_from_config",
]



================================================================================
FILE: preprocess_pipeline\cuisine_classifier\training.py
================================================================================
import logging
import random
import warnings
from pathlib import Path
from typing import Iterable, List, Optional, Tuple

import spacy
from spacy.language import Language
from spacy.tokens import DocBin
from spacy.training import Example

from .config import TRAIN, OUT
from .utils import configure_device, set_global_seed

logger = logging.getLogger(__name__)

try:
    import torch
except Exception:
    torch = None

# ---------- Transformer / tok2vec training using DocBins ----------

def build_nlp_transformer(all_labels: List[str]) -> spacy.language.Language:
    """Build a transformer + textcat classifier with optional layer freezing."""
    try:
        import spacy_transformers  # noqa: F401
    except Exception as e:
        raise RuntimeError("spacy-transformers is not available.") from e

    from .config import TRAIN as _TRAIN

    nlp = spacy.blank("en")
    trf_cfg = {
        "model": {
            "@architectures": "spacy-transformers.TransformerModel.v3",
            "name": _TRAIN.TRANSFORMER_MODEL,
            "tokenizer_config": {"use_fast": True},
            "transformer_config": {},
            "mixed_precision": bool(_TRAIN.USE_AMP),
            "grad_scaler_config": {"enabled": bool(_TRAIN.USE_AMP)},
            "get_spans": {
                "@span_getters": "spacy-transformers.strided_spans.v1",
                "window": int(_TRAIN.WINDOW),
                "stride": int(_TRAIN.STRIDE),
            },
        },
        "set_extra_annotations": {
            "@annotation_setters": "spacy-transformers.null_annotation_setter.v1"
        },
        "max_batch_items": 4096,
    }
    nlp.add_pipe("transformer", config=trf_cfg)
    
    # Add textcat component for multi-class classification (single label per example)
    # Note: exclusive_classes is set via model config, not pipe config in spaCy v3+
    textcat = nlp.add_pipe("textcat")
    for label in all_labels:
        textcat.add_label(label)

    # Optional layer freezing
    if _TRAIN.FREEZE_LAYERS > 0:
        try:
            trf = nlp.get_pipe("transformer").model
            hf = trf.transformer.model
            blocks = None
            if hasattr(hf, "transformer") and hasattr(hf.transformer, "layer"):  # distilbert
                blocks = hf.transformer.layer
            elif hasattr(hf, "encoder") and hasattr(hf.encoder, "layer"):        # bert/roberta
                blocks = hf.encoder.layer
            if blocks is not None:
                k = min(_TRAIN.FREEZE_LAYERS, len(blocks))
                for i in range(k):
                    for p in blocks[i].parameters():
                        p.requires_grad = False
                logger.info(f"[transformer] Froze {k} lower layer(s).")
        except Exception as e:
            warnings.warn(f"Could not freeze layers: {e}")
    return nlp


def build_nlp_tok2vec(all_labels: List[str]) -> spacy.language.Language:
    """CPU-friendly tok2vec + textcat fallback."""
    nlp = spacy.blank("en")
    nlp.add_pipe("tok2vec")
    # Note: exclusive_classes is set via model config, not pipe config in spaCy v3+
    textcat = nlp.add_pipe("textcat")
    for label in all_labels:
        textcat.add_label(label)
    logger.info("Using tok2vec fallback (no transformers).")
    return nlp


def choose_nlp(all_labels: List[str]) -> Tuple[Language, str]:
    """Choose transformer or tok2vec pipeline based on config and available deps."""
    from .config import TRAIN as _TRAIN

    # Check if debug mode forces tok2vec
    if hasattr(_TRAIN, 'USE_TOK2VEC_DEBUG') and _TRAIN.USE_TOK2VEC_DEBUG:
        logger.info("Debug mode: using tok2vec instead of transformers (faster, CPU-friendly)")
        return build_nlp_tok2vec(all_labels), "tok2vec"

    if torch is not None:
        has_trf = True
        try:
            import spacy_transformers  # noqa
        except Exception:
            has_trf = False
        if has_trf:
            try:
                return build_nlp_transformer(all_labels), "transformer"
            except Exception as e:
                warnings.warn(f"Falling back to tok2vec due to: {e}")
                return build_nlp_tok2vec(all_labels), "tok2vec"
    # No torch or transformers
    return build_nlp_tok2vec(all_labels), "tok2vec"


def iter_examples_from_docbins(
    nlp: Language,
    dir_path: Path,
    shuffle: bool = False,
    max_docs: Optional[int] = None,
) -> Iterable[Example]:
    shard_paths = sorted(p for p in dir_path.glob("*.spacy"))
    logger.debug(f"iter_examples_from_docbins: found {len(shard_paths)} shard(s) in {dir_path}")
    if shuffle:
        random.shuffle(shard_paths)

    count = 0
    for sp_i, sp_path in enumerate(shard_paths, start=1):
        db = DocBin().from_disk(sp_path)
        for d in db.get_docs(nlp.vocab):
            # For textcat, we use the cats dict directly
            yield Example.from_dict(nlp.make_doc(d.text), {"cats": d.cats})
            count += 1
            if max_docs is not None and count >= max_docs:
                logger.debug(f"max_docs={max_docs} reached, stopping iterator.")
                return


def sample_validation(nlp: Language, dir_path: Path, cap: int = 1500) -> List[Example]:
    out: List[Example] = []
    for eg in iter_examples_from_docbins(nlp, dir_path, shuffle=False):
        out.append(eg)
        if len(out) >= cap:
            break
    return out


def compounding_batch(epoch: int, total_epochs: int, start: int = 8, end: int = 16) -> int:
    if total_epochs <= 1:
        return end
    r = epoch / (total_epochs - 1)
    return max(1, int(round(start * ((end / start) ** r))))


def count_examples_in_docbins(nlp: Language, dir_path: Path) -> int:
    total = 0
    shard_paths = sorted(p for p in dir_path.glob("*.spacy"))
    logger.debug(f"Counting examples in {dir_path}, found {len(shard_paths)} shard(s)")
    for sp_i, sp_path in enumerate(shard_paths, start=1):
        db = DocBin().from_disk(sp_path)
        n_docs = sum(1 for _ in db.get_docs(nlp.vocab))
        total += n_docs
        logger.debug(f"  shard #{sp_i}: {sp_path.name} has {n_docs} docs")
    logger.debug(f"Total train examples in {dir_path}: {total}")
    return total


def get_all_labels_from_docbins(dir_path: Path) -> List[str]:
    """Extract all unique cuisine labels from docbins."""
    all_labels = set()
    shard_paths = sorted(p for p in dir_path.glob("*.spacy"))
    blank = spacy.blank("en")
    
    for sp_path in shard_paths:
        db = DocBin().from_disk(sp_path)
        for d in db.get_docs(blank.vocab):
            all_labels.update(d.cats.keys())
    
    return sorted(list(all_labels))


def train_classifier_from_docbins(
    train_dir: Path | None = None,
    valid_dir: Path | None = None,
    out_model_dir: Path | None = None,
) -> Language:
    from .config import TRAIN as _TRAIN, OUT as _OUT
    import time

    train_dir = train_dir or _OUT.TRAIN_DIR
    valid_dir = valid_dir or _OUT.VALID_DIR
    out_model_dir = out_model_dir or _OUT.MODEL_DIR

    logger.debug(f"train_dir={train_dir}")
    logger.debug(f"valid_dir={valid_dir}")
    logger.debug(f"out_model_dir={out_model_dir}")

    # Get all labels from training data
    logger.info("Extracting all cuisine labels from training data...")
    all_labels = get_all_labels_from_docbins(train_dir)
    logger.info(f"Found {len(all_labels)} unique cuisine labels: {all_labels[:10]}{'...' if len(all_labels) > 10 else ''}")

    # Configure device FIRST (before creating pipeline)
    configure_device()
    set_global_seed(_TRAIN.RANDOM_SEED)

    t0 = time.time()
    logger.debug("Choosing pipeline...")
    nlp, mode = choose_nlp(all_labels)
    
    # Log device info for transformer models
    if mode == "transformer" and torch is not None:
        try:
            trf = nlp.get_pipe("transformer")
            if hasattr(trf, "model"):
                model_ref = trf.model.get_ref("model")
                if hasattr(model_ref, "device"):
                    logger.info(f"[device] Transformer model device: {model_ref.device}")
                
                # Also check the underlying PyTorch model
                if hasattr(trf, "model") and hasattr(trf.model, "get_ref"):
                    try:
                        transformer_model = trf.model.get_ref("model")
                        if hasattr(transformer_model, "transformer"):
                            # Check the actual PyTorch model device
                            pytorch_model = transformer_model.transformer.model
                            if hasattr(pytorch_model, "device"):
                                logger.info(f"[device] PyTorch model device: {pytorch_model.device}")
                            # Check first parameter device
                            if hasattr(pytorch_model, "parameters"):
                                first_param = next(pytorch_model.parameters(), None)
                                if first_param is not None:
                                    logger.info(f"[device] Model parameters on device: {first_param.device}")
                    except Exception as e:
                        logger.debug(f"Could not inspect PyTorch model device: {e}")
        except Exception as e:
            logger.debug(f"Could not determine transformer device: {e}")
    
    logger.debug(f"choose_nlp() done in {time.time() - t0:.1f}s")
    logger.info(f"Pipeline mode: {mode}")
    
    # Log debug mode status
    if hasattr(_TRAIN, 'MAX_TRAIN_DOCS') and _TRAIN.MAX_TRAIN_DOCS is not None:
        logger.info(f"Debug mode: max_train_docs={_TRAIN.MAX_TRAIN_DOCS}")

    # Warm init
    logger.debug("Collecting warm-up examples...")
    t0 = time.time()
    warm: List[Example] = []
    for eg_i, eg in enumerate(
        iter_examples_from_docbins(nlp, train_dir, shuffle=True),
        start=1,
    ):
        warm.append(eg)
        if eg_i % 50 == 0:
            logger.debug(f"warm example #{eg_i}")
        if eg_i >= min(256, max(16, 100)):
            break
    logger.debug(f"Collected {len(warm)} warm examples in {time.time() - t0:.1f}s")

    logger.debug("Calling nlp.initialize(...)")
    t0 = time.time()
    optimizer = nlp.initialize(lambda: warm)
    logger.debug(f"nlp.initialize(...) finished in {time.time() - t0:.1f}s")

    if hasattr(optimizer, "learn_rate"):
        optimizer.learn_rate = float(_TRAIN.LR)
        logger.debug(f"Set optimizer learn_rate={optimizer.learn_rate}")

    logger.debug("Building validation snapshot...")
    t0 = time.time()
    valid_snapshot = sample_validation(nlp, valid_dir, cap=_TRAIN.EVAL_SNAPSHOT_MAX)
    logger.debug(f"Validation snapshot size={len(valid_snapshot)} "
          f"built in {time.time() - t0:.1f}s")

    best_f1 = -1.0
    bad_epochs = 0

    logger.debug("Sanity-checking training set size...")
    _ = count_examples_in_docbins(nlp, train_dir)

    for epoch in range(_TRAIN.N_EPOCHS):
        logger.info(f"===== Epoch {epoch + 1}/{_TRAIN.N_EPOCHS} =====")
        losses: dict = {}
        micro_bs = compounding_batch(epoch, _TRAIN.N_EPOCHS, start=8, end=16)
        logger.debug(f"micro-batch size (μbs) = {micro_bs}")
        buf: List[Example] = []
        updates = 0

        t_epoch = time.time()
        for eg_i, eg in enumerate(iter_examples_from_docbins(nlp, train_dir, shuffle=True), start=1):
            buf.append(eg)
            if len(buf) < micro_bs:
                continue
            nlp.update(buf, sgd=optimizer, drop=_TRAIN.DROPOUT, losses=losses)
            buf.clear()
            updates += 1

            if updates % 20 == 0:
                logger.debug(f"epoch={epoch+1} updates={updates} "
                      f"seen_examples≈{eg_i} loss={losses.get('textcat', 0):.2f}")

            if (torch is not None) and torch.cuda.is_available() \
                    and updates % _TRAIN.CLEAR_CACHE_EVERY == 0:
                logger.debug("emptying CUDA cache...")
                torch.cuda.empty_cache()

        if buf:
            logger.debug(f"Flushing last batch of size {len(buf)}")
            nlp.update(buf, sgd=optimizer, drop=_TRAIN.DROPOUT, losses=losses)
            buf.clear()

        logger.debug(f"Finished epoch {epoch+1} updates={updates} "
              f"in {time.time() - t_epoch:.1f}s")

        logger.debug("Running evaluation on validation snapshot...")
        with nlp.select_pipes(disable=[p for p in nlp.pipe_names if p != "textcat"]):
            scores = nlp.evaluate(valid_snapshot)

        # For textcat, we look at textcat_score
        f1 = float(scores.get("textcat_score", 0.0))

        logger.info(
            f"Epoch {epoch + 1:02d}/{_TRAIN.N_EPOCHS} | μbs={micro_bs:<3d} "
            f"| loss={losses.get('textcat', 0):.1f} | F1={f1:.3f}"
        )

        improved = f1 > best_f1 + 1e-6
        if improved:
            best_f1 = f1
            bad_epochs = 0
            out_model_dir.mkdir(parents=True, exist_ok=True)
            nlp.to_disk(out_model_dir)
            logger.info(f"  ↳ Saved model-best → {out_model_dir} (F1={f1:.3f})")
        else:
            bad_epochs += 1
            if bad_epochs >= _TRAIN.EARLY_STOPPING_PATIENCE:
                logger.info(f"Early stopping after {bad_epochs} non-improving epoch(s).")
                break

    logger.info(f"Best F1 observed: {best_f1 if best_f1 >= 0 else 0.0}")
    return nlp


__all__ = [
    "build_nlp_transformer",
    "build_nlp_tok2vec",
    "choose_nlp",
    "iter_examples_from_docbins",
    "sample_validation",
    "compounding_batch",
    "get_all_labels_from_docbins",
    "train_classifier_from_docbins",
]



================================================================================
FILE: preprocess_pipeline\cuisine_classifier\utils.py
================================================================================
import ast
import json
import logging
import random
import warnings
from pathlib import Path
from typing import Any, List, Optional, Tuple, Union

import numpy as np
import pandas as pd

try:
    import pyarrow.parquet as pq
    _HAS_PA = True
except ImportError:
    _HAS_PA = False

try:
    import torch
except ImportError:
    torch = None

# We use a global logger for this module
logger = logging.getLogger(__name__)

def set_global_seed(seed: int = 42) -> None:
    """Set seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    if torch is not None:
        try:
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(seed)
        except Exception:
            pass

def configure_device(use_gpu: bool = True) -> None:
    """
    Configure device for training/inference.
    
    This version respects the user's config but avoids spacy.require_gpu() 
    if it causes conflicts, relying on PyTorch directly.
    """
    if not use_gpu:
        logger.info("[device] GPU disabled by config. Using CPU.")
        return

    if torch is None:
        logger.warning("[device] Torch not installed. Using CPU.")
        return

    if torch.cuda.is_available():
        dev_name = torch.cuda.get_device_name(0)
        logger.info(f"[device] CUDA available. PyTorch will use GPU: {dev_name}")
        # NOTE: We intentionally do NOT call spacy.require_gpu() here 
        # to avoid conflicts with thinc/cupy if they aren't perfectly aligned.
    else:
        logger.info("[device] CUDA not available. Using CPU.")

def _read_csv_with_fallback(path: Path, dtype=str, nrows: Optional[int] = None) -> Tuple[pd.DataFrame, str]:
    """
    Internal helper: Tries to read CSV using UTF-8, then cp1252 (Windows), then latin-1.
    Returns (DataFrame, encoding_used).
    """
    encodings = ["utf-8", "cp1252", "latin-1"]
    
    for enc in encodings:
        try:
            # Use default C engine for speed
            df = pd.read_csv(path, dtype=dtype, encoding=enc, nrows=nrows)
            return df, enc
        except UnicodeDecodeError:
            continue # Try next encoding
        except Exception as e:
            logger.warning(f"Error reading with {enc}: {e}")
            raise

    # Last resort: Python engine with 'replace' (lossy but works)
    logger.warning("All standard encodings failed. Trying utf-8 with errors='replace'.")
    df = pd.read_csv(
        path, 
        dtype=dtype, 
        encoding="utf-8", 
        engine="python", 
        on_bad_lines="warn", 
        nrows=nrows
    )
    return df, "utf-8-replace"

def load_data(path: Union[str, Path], is_parquet: bool, text_col: str, cuisine_col: str, max_rows: Optional[int] = None) -> pd.DataFrame:
    """
    Load text and cuisine columns from CSV/Parquet and return a clean DataFrame.
    
    Handles Windows encoding (0xae / ®) issues automatically.
    """
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"Input file not found: {path}")

    df = pd.DataFrame()

    if is_parquet:
        if not _HAS_PA:
            raise RuntimeError("pyarrow is required to read Parquet files.")
        
        # Parquet reading logic
        if max_rows:
            pf = pq.ParquetFile(str(path))
            rows_read = 0
            frames = []
            for i in range(pf.num_row_groups):
                if rows_read >= max_rows: break
                chunk = pf.read_row_group(i, columns=[text_col, cuisine_col]).to_pandas()
                frames.append(chunk)
                rows_read += len(chunk)
            df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
            if len(df) > max_rows:
                df = df.head(max_rows)
        else:
            try:
                df = pd.read_parquet(path, columns=[text_col, cuisine_col])
            except Exception as e:
                logger.error(f"Error reading parquet file: {e}")
                df = pd.read_parquet(path)
                logger.error(f"Columns available: {list(df.columns)[:5]}...")
                print(df.head())
                raise e
    else:
        # CSV Reading with Fallback Logic
        df, used_encoding = _read_csv_with_fallback(path, dtype=str, nrows=max_rows)
        logger.info(f"Loaded CSV using encoding: {used_encoding}")

    # Column Validation
    if text_col not in df.columns:
        raise KeyError(f"Column '{text_col}' not found. Available: {list(df.columns)[:5]}...")
    if cuisine_col not in df.columns:
        raise KeyError(f"Column '{cuisine_col}' not found. Available: {list(df.columns)[:5]}...")
    
    # Return clean, non-empty dataframe
    return df[[text_col, cuisine_col]].dropna().reset_index(drop=True)

def parse_listlike(v: Any) -> List[str]:
    """Parse values that may be python-lists, JSON-lists, or comma-separated strings."""
    
    # 1. Handle list-likes (arrays, lists, tuples) FIRST.
    if isinstance(v, (list, tuple, np.ndarray)):
        # Convert elements to strings and filter empty ones
        return [str(x).strip() for x in v if str(x).strip()]
    
    # 2. Handle scalars (strings, floats, None).
    if pd.isna(v):
        return []
    
    s = str(v).strip()
    if not s:
        return []
        
    # 3. Try JSON/Literal eval (e.g. "['salt', 'pepper']")
    if s.startswith("[") and s.endswith("]"):
        for parser in (json.loads, ast.literal_eval):
            try:
                out = parser(s)
                # Ensure the result is actually a list/iterable
                if isinstance(out, (list, tuple, np.ndarray)):
                    return [str(x).strip() for x in out if str(x).strip()]
            except Exception:
                pass
                
    # 4. Fallback to comma separation
    return [x.strip() for x in s.split(",") if x.strip()]

def normalize_cuisine_label(cuisine: Any) -> str:
    """Normalize cuisine label to a clean string."""
    if pd.isna(cuisine):
        return ""
    
    s = str(cuisine).strip()
    if not s or s.lower() in ["nan", "none", "", "[]"]:
        return ""
    
    # Handle list-like strings
    if s.startswith("[") and s.endswith("]"):
        try:
            parsed = ast.literal_eval(s)
            if isinstance(parsed, list) and parsed:
                # Take first cuisine if multiple
                return str(parsed[0]).strip()
        except Exception:
            pass
    
    return s



================================================================================
FILE: preprocess_pipeline\ingredient_ner\__init__.py
================================================================================
"""
Ingredient NER package: training, normalization & encoding utilities.
"""

# Public config interface: three global namespaces + helpers
from .config import (
    DATA,
    TRAIN,
    OUT,
    load_configs_from_yaml,
    load_configs_from_dict,
    print_configs,
)

__all__ = [
    "DATA",
    "TRAIN",
    "OUT",
    "load_configs_from_yaml",
    "load_configs_from_dict",
    "print_configs",
]

================================================================================
FILE: preprocess_pipeline\ingredient_ner\config.py
================================================================================
# pipeline/ingredient_ner/config.py
from __future__ import annotations

from pathlib import Path
from types import SimpleNamespace
from typing import Any, Dict, Tuple

import yaml

# Public globals other modules import
DATA = SimpleNamespace()
TRAIN = SimpleNamespace()
OUT = SimpleNamespace()


def _to_path(v: Any) -> Path | None:
    if v is None:
        return None
    return Path(str(v))


def load_full_yaml(path: str | Path) -> Dict[str, Any]:
    """Load the full pipeline YAML."""
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"Config not found: {path}")
    with open(path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if not isinstance(cfg, dict):
        raise ValueError(f"YAML at {path} did not parse to a dict.")
    return cfg


def _build_data_ns(cfg: Dict[str, Any]) -> SimpleNamespace:
    data_cfg = cfg.get("data") or {}
    ner_cfg = cfg.get("ner") or {}
    out_cfg = cfg.get("output") or {}

    # 1) training file path
    # Prefer ner.train_path, fallback to data.input_path, then default
    train_path_str = ner_cfg.get("train_path")
    if not train_path_str:
        train_path_str = data_cfg.get("input_path")
    if not train_path_str:
        train_path_str = "./data/raw/wilmerarltstrmberg_data.csv"
    train_path = Path(train_path_str)

    # 2) parquet vs csv
    if "data_is_parquet" in ner_cfg:
        data_is_parquet = bool(ner_cfg["data_is_parquet"])
    else:
        data_is_parquet = train_path.suffix.lower() == ".parquet"

    # 3) max rows
    max_rows_raw = ner_cfg.get("max_rows")
    max_rows = int(max_rows_raw) if max_rows_raw is not None else None

    # 4) NER columns
    ner_list_col = ner_cfg.get("ner_list_col") or data_cfg.get("ner_col") or None
    text_col = ner_cfg.get("text_col") or None
    lexicon_json = _to_path(ner_cfg.get("lexicon_json")) if ner_cfg.get("lexicon_json") else None

    # 5) other artifacts
    dedupe_jsonl = _to_path(out_cfg.get("cosine_map_path"))
    ing_id2tok = _to_path(out_cfg.get("ingredient_id_to_token"))
    ing_tok2id = _to_path(out_cfg.get("ingredient_token_to_id"))

    return SimpleNamespace(
        DATA_IS_PARQUET=data_is_parquet,
        TRAIN_PATH=train_path,
        MAX_ROWS=max_rows,
        NER_LIST_COL=ner_list_col,
        TEXT_COL=text_col,
        LEXICON_JSON=lexicon_json,
        DEDUPE_JSONL=dedupe_jsonl,
        ING_ID2TOK_JSON=ing_id2tok,
        ING_TOK2ID_JSON=ing_tok2id,
    )


def _build_train_ns(cfg: Dict[str, Any]) -> SimpleNamespace:
    """Build TRAIN namespace from ner config. Uses UPPERCASE keys to match existing code."""
    ner_cfg = cfg.get("ner") or {}

    # Debug mode: if use_tok2vec_debug is True, we'll use tok2vec instead of transformers
    use_tok2vec_debug = bool(ner_cfg.get("use_tok2vec_debug", False))
    
    # If debug mode, reduce epochs for faster iteration
    n_epochs = int(ner_cfg.get("n_epochs", 20))
    if use_tok2vec_debug:
        n_epochs = min(n_epochs, 5)  # Cap at 5 epochs in debug mode

    return SimpleNamespace(
        RANDOM_SEED=int(ner_cfg.get("random_seed", 42)),
        VALID_FRACTION=float(ner_cfg.get("valid_fraction", 0.2)),
        SHARD_SIZE=int(ner_cfg.get("shard_size", 2000)),
        BATCH_SIZE=int(ner_cfg.get("batch_size", 256)),
        TRANSFORMER_MODEL=str(ner_cfg.get("transformer_model", "distilbert-base-uncased")),
        WINDOW=int(ner_cfg.get("window", 64)),
        STRIDE=int(ner_cfg.get("stride", 48)),
        LR=float(ner_cfg.get("lr", 5e-5)),
        DROPOUT=float(ner_cfg.get("dropout", 0.1)),
        N_EPOCHS=n_epochs,
        FREEZE_LAYERS=int(ner_cfg.get("freeze_layers", 2)),
        USE_AMP=bool(ner_cfg.get("use_amp", True)),
        EARLY_STOPPING_PATIENCE=int(ner_cfg.get("early_stopping_patience", 3)),
        EVAL_SNAPSHOT_MAX=int(ner_cfg.get("eval_snapshot_max", 1500)),
        CLEAR_CACHE_EVERY=int(ner_cfg.get("clear_cache_every", 200)),
        USE_TOK2VEC_DEBUG=use_tok2vec_debug,
        MAX_TRAIN_DOCS=int(ner_cfg.get("max_train_docs")) if ner_cfg.get("max_train_docs") is not None else None,
        DATA_LOADER_WORKERS=int(ner_cfg.get("data_loader_workers", 4)),
    )


def _build_out_ns(cfg: Dict[str, Any]) -> SimpleNamespace:
    ner_cfg = cfg.get("ner") or {}
    out_cfg = cfg.get("output") or {}

    out_dir = _to_path(ner_cfg.get("out_dir")) or Path("./models/ingredient_ner_trf")
    model_dir = _to_path(ner_cfg.get("model_dir")) or (out_dir / "model-best")

    boot_dir = out_dir / "bootstrapped"
    train_dir = boot_dir / "train"
    valid_dir = boot_dir / "valid"

    pred_out = _to_path(out_cfg.get("ner_preds_base")) or Path("./data/training/predictions.parquet")

    return SimpleNamespace(
        OUT_DIR=out_dir,
        MODEL_DIR=model_dir,
        BOOT_DIR=boot_dir,
        TRAIN_DIR=train_dir,
        VALID_DIR=valid_dir,
        PRED_OUT=pred_out,
    )


def _update_ns(target: SimpleNamespace, src: SimpleNamespace) -> None:
    """Mutate an existing SimpleNamespace to have src's attributes."""
    target.__dict__.clear()
    target.__dict__.update(src.__dict__)


# ---------------------- public API ---------------------- #

def load_configs_from_dict(cfg: Dict[str, Any]) -> Tuple[SimpleNamespace, SimpleNamespace, SimpleNamespace]:
    """
    Populate DATA / TRAIN / OUT from an in-memory YAML dict.

    IMPORTANT: we mutate the existing SimpleNamespace objects so
    any 'from ingredient_ner.config import TRAIN' references see updates.
    """
    global DATA, TRAIN, OUT
    _update_ns(DATA, _build_data_ns(cfg))
    _update_ns(TRAIN, _build_train_ns(cfg))
    _update_ns(OUT, _build_out_ns(cfg))
    return DATA, TRAIN, OUT


def load_configs_from_yaml(path: str | Path) -> Tuple[SimpleNamespace, SimpleNamespace, SimpleNamespace]:
    """Convenience that reads YAML then delegates to load_configs_from_dict."""
    cfg = load_full_yaml(path)
    return load_configs_from_dict(cfg)


def print_configs() -> None:
    from pprint import pprint
    print("DATA:")
    pprint(vars(DATA))
    print("\nTRAIN:")
    pprint(vars(TRAIN))
    print("\nOUT:")
    pprint(vars(OUT))


# ---------------------- Inference-specific config ---------------------- #

def _build_inference_data_ns(cfg: Dict[str, Any]) -> SimpleNamespace:
    """Build DATA namespace for inference from inference config YAML."""
    data_cfg = cfg.get("data") or {}
    artifacts_cfg = cfg.get("artifacts") or {}
    
    input_path_str = data_cfg.get("input_path")
    input_path = _to_path(input_path_str) if input_path_str else None
    
    # Determine if parquet or CSV
    if "data_is_parquet" in data_cfg:
        data_is_parquet = bool(data_cfg["data_is_parquet"])
    elif input_path:
        data_is_parquet = input_path.suffix.lower() == ".parquet"
    else:
        data_is_parquet = True  # Default to parquet
    
    return SimpleNamespace(
        DATA_IS_PARQUET=data_is_parquet,
        TRAIN_PATH=input_path,  # Reusing TRAIN_PATH name for consistency
        MAX_ROWS=None,
        NER_LIST_COL=None,
        TEXT_COL=None,
        LEXICON_JSON=None,
        DEDUPE_JSONL=_to_path(artifacts_cfg.get("cosine_map_path")),
        ING_ID2TOK_JSON=_to_path(artifacts_cfg.get("ingredient_id_to_token")),
        ING_TOK2ID_JSON=_to_path(artifacts_cfg.get("ingredient_token_to_id")),
    )


def _build_inference_out_ns(cfg: Dict[str, Any]) -> SimpleNamespace:
    """Build OUT namespace for inference from inference config YAML."""
    model_cfg = cfg.get("model") or {}
    out_cfg = cfg.get("output") or {}
    
    model_dir = _to_path(model_cfg.get("model_dir")) or Path("./models/ingredient_ner_trf/model-best")
    out_base = _to_path(out_cfg.get("out_base")) or Path("./data/inference_output")
    
    return SimpleNamespace(
        OUT_DIR=model_dir.parent,  # For consistency
        MODEL_DIR=model_dir,
        BOOT_DIR=None,
        TRAIN_DIR=None,
        VALID_DIR=None,
        PRED_OUT=out_base,
    )


def load_inference_configs_from_dict(cfg: Dict[str, Any]) -> Tuple[SimpleNamespace, SimpleNamespace]:
    """
    Populate DATA / OUT from an inference config dict.
    
    Returns only DATA and OUT (no TRAIN needed for inference).
    """
    global DATA, OUT
    _update_ns(DATA, _build_inference_data_ns(cfg))
    _update_ns(OUT, _build_inference_out_ns(cfg))
    return DATA, OUT


def load_inference_configs_from_yaml(path: str | Path) -> Tuple[SimpleNamespace, SimpleNamespace]:
    """Convenience that reads inference YAML then delegates to load_inference_configs_from_dict."""
    cfg = load_full_yaml(path)
    return load_inference_configs_from_dict(cfg)

================================================================================
FILE: preprocess_pipeline\ingredient_ner\data_prep.py
================================================================================
import json
import math
import warnings
from pathlib import Path
from typing import List, Tuple

import pandas as pd
import spacy
from sklearn.model_selection import train_test_split
from spacy.tokens import Doc, DocBin
from tqdm import tqdm

from .config import DATA, TRAIN, OUT
from .utils import (
    load_data,
    parse_listlike,
    join_with_offsets,
    normalize_token,
)


def docs_from_list_column(df: pd.DataFrame, col: str) -> List[Doc]:
    """Create spaCy Docs from a list-like ingredient column."""
    blank = spacy.blank("en")
    out: List[Doc] = []
    for lst in tqdm(df[col].tolist(), desc="Synthesizing from list column"):
        toks = parse_listlike(lst)
        if not toks:
            out.append(blank.make_doc(""))
            continue
        text, offs = join_with_offsets(toks)
        d = blank.make_doc(text)
        ents = []
        for (a, b) in offs:
            sp = d.char_span(a, b, label="INGREDIENT", alignment_mode="contract")
            if sp is not None:
                ents.append(sp)
        d.ents = spacy.util.filter_spans(ents)
        out.append(d)
    return out


def load_lexicon(path: Path | None) -> list[str]:
    if path is None:
        return []
    p = Path(path)
    if not p.exists():
        warnings.warn(f"Lexicon not found at {p}. Skipping.")
        return []
    with open(p, "r", encoding="utf-8") as f:
        data = json.load(f)
    # Expect either {"terms": [...]} or a simple list [...]
    if isinstance(data, dict) and "terms" in data:
        terms = data["terms"]
    else:
        terms = data
    # normalize
    terms = [normalize_token(t) for t in terms if str(t).strip()]
    terms = sorted(set(terms))
    print(f"Loaded {len(terms):,} lexicon terms.")
    return terms


def build_entity_ruler(nlp: spacy.language.Language, phrases: list[str]):
    ruler = nlp.add_pipe("entity_ruler")
    patterns = [{"label": "INGREDIENT", "pattern": t} for t in phrases]
    ruler.add_patterns(patterns)
    return ruler


def docs_from_text_plus_lexicon(df: pd.DataFrame, text_col: str, lexicon_terms: list[str]) -> List[Doc]:
    """Bootstrap labels from raw text using an EntityRuler over a lexicon."""
    nlp = spacy.blank("en")
    if not lexicon_terms:
        raise ValueError("No lexicon terms provided; cannot bootstrap from raw text.")
    build_entity_ruler(nlp, lexicon_terms)
    out: List[Doc] = []
    for text in tqdm(df[text_col].astype(str).tolist(), desc="Bootstrapping with EntityRuler"):
        d = nlp.make_doc(text)
        d = nlp(d)  # apply ruler
        # Keep only INGREDIENT, deduplicate spans
        d.ents = spacy.util.filter_spans([e for e in d.ents if e.label_ == "INGREDIENT"])
        out.append(d)
    return out


def build_docs_from_config() -> Tuple[list[Doc], list[Doc], str]:
    """High-level helper: build train & valid docs based on DATA + TRAIN config."""
    # Debug info
    print(f"[DEBUG] ===== Data Loading Configuration =====")
    print(f"[DEBUG] TRAIN_PATH: {DATA.TRAIN_PATH}")
    print(f"[DEBUG] Absolute path: {DATA.TRAIN_PATH.resolve()}")
    print(f"[DEBUG] File exists: {DATA.TRAIN_PATH.exists()}")
    print(f"[DEBUG] Data format: {'Parquet' if DATA.DATA_IS_PARQUET else 'CSV'}")
    print(f"[DEBUG] NER_LIST_COL: {DATA.NER_LIST_COL}")
    print(f"[DEBUG] TEXT_COL: {DATA.TEXT_COL}")
    print(f"[DEBUG] LEXICON_JSON: {DATA.LEXICON_JSON}")
    print(f"[DEBUG] ======================================")

    # Decide source mode
    if DATA.DATA_IS_PARQUET:
        df_sample = pd.read_parquet(DATA.TRAIN_PATH).head(1)
    else:
        df_sample = pd.read_csv(DATA.TRAIN_PATH, nrows=0, dtype=str)  # Read header only

    print(f"[DEBUG] Sample columns: {list(df_sample.columns)}")

    if DATA.NER_LIST_COL and DATA.NER_LIST_COL in df_sample.columns:
        print(f"[DEBUG] Using list-column mode on '{DATA.NER_LIST_COL}'")
        print(f"[DEBUG] This teaches the model to handle real-world messy input.")
        df_list = load_data(DATA.TRAIN_PATH, DATA.DATA_IS_PARQUET, DATA.NER_LIST_COL, max_rows=DATA.MAX_ROWS)
        docs_all = docs_from_list_column(df_list, DATA.NER_LIST_COL)
        source_mode = "list-column"
        
        # Apply max_train_docs limit if set (for debug mode)
        if hasattr(TRAIN, 'MAX_TRAIN_DOCS') and TRAIN.MAX_TRAIN_DOCS is not None:
            if len(docs_all) > TRAIN.MAX_TRAIN_DOCS:
                print(f"[DEBUG] Limiting training docs to {TRAIN.MAX_TRAIN_DOCS} for debug mode")
                docs_all = docs_all[:TRAIN.MAX_TRAIN_DOCS]
    elif DATA.TEXT_COL and DATA.LEXICON_JSON:
        print(f"[DEBUG] Using text+lexicon mode with TEXT_COL='{DATA.TEXT_COL}'")
        df_text = load_data(DATA.TRAIN_PATH, DATA.DATA_IS_PARQUET, DATA.TEXT_COL)
        lex = load_lexicon(DATA.LEXICON_JSON)
        docs_all = docs_from_text_plus_lexicon(df_text, DATA.TEXT_COL, lex)
        source_mode = "text+lexicon"
    else:
        raise RuntimeError(
            "No valid data source inferred. Set DATA.NER_LIST_COL (list-like labels) "
            "or DATA.TEXT_COL + DATA.LEXICON_JSON (bootstrapping)."
        )

    print(f"Docs prepared: {len(docs_all):,} | Source mode: {source_mode}")
    print("Total labeled entities:", sum(len(d.ents) for d in docs_all))

    # Optionally cap docs for local testing, if you want:
    # MAX_DOCS = 500
    # docs_all = docs_all[:MAX_DOCS]

    print(f"[DEBUG] Using only first {len(docs_all)} docs for training")

    train_docs, valid_docs = train_test_split(
        docs_all, test_size=TRAIN.VALID_FRACTION, random_state=TRAIN.RANDOM_SEED
    )
    print(f"train={len(train_docs):,} | valid={len(valid_docs):,}")

    return train_docs, valid_docs, source_mode


from pathlib import Path


def write_docbins(docs: list[Doc], out_dir: Path, shard_size: int) -> None:
    """Write docs into sharded DocBin files."""

    def clean_dir(path: Path):
        path.mkdir(parents=True, exist_ok=True)
        for p in path.glob("*.spacy"):
            print(f"[CLEAN] Removing old shard: {p}")
            p.unlink()
    clean_dir(out_dir)

    out_dir.mkdir(parents=True, exist_ok=True)
    n = len(docs)
    shards = max(1, math.ceil(n / max(1, shard_size)))
    for i in range(shards):
        db = DocBin(store_user_data=False)
        for d in docs[i * shard_size: (i + 1) * shard_size]:
            db.add(d)
        db.to_disk(out_dir / f"shard_{i:04d}.spacy")
    print(f"Wrote {n} docs to {out_dir} in {shards} shard(s).")


def prepare_docbins_from_config() -> None:
    """End-to-end: build docs and write train/valid DocBins according to config."""
    train_docs, valid_docs, _ = build_docs_from_config()
    write_docbins(train_docs, OUT.TRAIN_DIR, shard_size=TRAIN.SHARD_SIZE)
    write_docbins(valid_docs, OUT.VALID_DIR, shard_size=TRAIN.SHARD_SIZE)


__all__ = [
    "docs_from_list_column",
    "load_lexicon",
    "build_entity_ruler",
    "docs_from_text_plus_lexicon",
    "build_docs_from_config",
    "write_docbins",
    "prepare_docbins_from_config",
]

================================================================================
FILE: preprocess_pipeline\ingredient_ner\inference.py
================================================================================
from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Optional, Dict, List, Tuple

import pandas as pd

try:
    import pyarrow as pa
    import pyarrow.parquet as pq
    _HAS_PA = True
except Exception:
    _HAS_PA = False

import spacy
from tqdm import tqdm

from .config import DATA, OUT
from .utils import load_data, normalize_token, parse_listlike, join_with_offsets
from .normalization import apply_dedupe, load_jsonl_map, load_encoder_maps

# Import spaCy normalizer for consistent normalization
# This ensures inference uses the same normalization as the training pipeline
try:
    import sys
    from pathlib import Path
    # Add pipeline to path if not already there (for imports when running as script)
    pipeline_path = Path(__file__).parent.parent
    if str(pipeline_path) not in sys.path:
        sys.path.insert(0, str(pipeline_path))
    from ingrnorm.spacy_normalizer import SpacyIngredientNormalizer
    _HAS_SPACY_NORM = True
except (ImportError, ModuleNotFoundError) as e:
    _HAS_SPACY_NORM = False
    SpacyIngredientNormalizer = None
    # Will be handled gracefully in the code


def _unique_preserve_order(seq):
    seen = set()
    out = []
    for x in seq:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out


def _extract_ingredient_rows(
    doc, 
    dedupe: Optional[dict] = None, 
    tok2id: Optional[dict] = None,
    spacy_normalizer: Optional[Any] = None,
):
    """
    Return a list of per-entity dicts with offsets + normalized/canonical forms.
    
    Args:
        doc: spaCy Doc with entities
        dedupe: Dedupe mapping dict (variant → canonical)
        tok2id: Token to ID mapping dict
        spacy_normalizer: Optional SpacyIngredientNormalizer instance for consistent normalization
    """
    rows: List[Dict] = []
    for ent in doc.ents:
        if ent.label_ != "INGREDIENT":
            continue
        raw = ent.text
        
        # Apply spaCy normalization if available (matches training pipeline)
        # Otherwise fall back to simple normalize_token
        if spacy_normalizer is not None:
            norm_result = spacy_normalizer._normalize_phrase(raw)
            norm = norm_result if norm_result else normalize_token(raw)
        else:
            norm = normalize_token(raw)
        
        # Apply dedupe mapping (variant → canonical)
        canon = apply_dedupe(norm, dedupe)
        
        # Map canonical form to ID
        tok_id = tok2id.get(canon, 0) if tok2id else None
        
        rows.append(
            {
                "raw": raw,
                "start": int(ent.start_char),
                "end": int(ent.end_char),
                "label": ent.label_,
                "norm": norm,
                "canonical": canon,
                "id": int(tok_id) if tok_id is not None else None,
            }
        )
    return rows


def predict_normalize_encode_structured(
    nlp_dir: Path,
    data_path: Path,
    is_parquet: bool,
    text_col: str,
    dedupe: Optional[dict] = None,
    tok2id: Optional[dict] = None,
    out_path: Optional[Path] = None,
    batch_size: int = 256,
    # sampling knobs (use exactly one)
    sample_n: Optional[int] = None,
    sample_frac: Optional[float] = None,
    head_n: Optional[int] = None,
    start: int = 0,
    stop: Optional[int] = None,
    sample_seed: int = 42,
    # performance
    n_process: int = 1,  # keep 1 for GPU/transformers
    # normalization
    use_spacy_normalizer: bool = True,  # Use spaCy normalizer to match training pipeline
    spacy_model: str = "en_core_web_sm",  # spaCy model for normalizer
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Returns:
      df_wide: one row per input, columns=[text_col, NER_raw, NER_clean, Ingredients?, spans_json]
      df_tall: one row per extracted entity with offsets and normalized/canonical forms
    If out_path is set, writes two parquet files: <stem>_wide.parquet and <stem>_tall.parquet.
    """
    if not nlp_dir.exists():
        raise FileNotFoundError(f"Model directory not found: {nlp_dir}")

    nlp = spacy.load(nlp_dir)
    
    # Initialize spaCy normalizer if requested (for consistent normalization with training)
    # CRITICAL: This ensures inference uses the same normalization as training pipeline
    # Without this, "buttermilk cornbread" stays as-is instead of becoming "cornbread"
    spacy_normalizer = None
    if use_spacy_normalizer and _HAS_SPACY_NORM:
        try:
            spacy_normalizer = SpacyIngredientNormalizer(model=spacy_model, batch_size=128, n_process=1)
            print(f"✓ Using spaCy normalizer (model={spacy_model}) for consistent normalization with training pipeline")
            print(f"  This ensures ingredients like 'buttermilk cornbread' → 'cornbread' (matching training data)")
        except Exception as e:
            print(f"⚠ Warning: Could not initialize spaCy normalizer: {e}. Falling back to simple normalization.")
            print(f"  This may cause ID=0 for many ingredients if they don't match encoder vocabulary.")
            spacy_normalizer = None
    elif use_spacy_normalizer and not _HAS_SPACY_NORM:
        print("⚠ Warning: spaCy normalizer not available. Using simple normalization.")
        print("  This may cause ID=0 for many ingredients. Install ingrnorm package for full normalization.")

    df_in = load_data(data_path, is_parquet, text_col)

    # Apply ONE sampling strategy
    if head_n is not None:
        df_in = df_in.head(head_n)
    elif sample_n is not None:
        df_in = df_in.sample(n=min(sample_n, len(df_in)), random_state=sample_seed)
    elif sample_frac is not None:
        df_in = df_in.sample(
            frac=min(max(sample_frac, 0.0), 1.0), random_state=sample_seed
        )
    elif start != 0 or stop is not None:
        df_in = df_in.iloc[start:stop]

    # Process each row: parse ingredient list and run NER on each ingredient separately
    # The model is trained on individual ingredients, not joined lists
    raw_texts = df_in[text_col].astype(str).tolist()
    
    wide_rows: List[Dict] = []
    tall_records: List[Dict] = []

    for i, raw_text in enumerate(
        tqdm(raw_texts, total=len(raw_texts), desc="Infer (structured)")
    ):
        # Parse the input into individual ingredients
        ingredients = parse_listlike(raw_text)
        
        # Handle "and" at the end (e.g., "salt, pepper and butter")
        if len(ingredients) > 0:
            last_ing = ingredients[-1]
            if " and " in last_ing.lower():
                # Split on " and " and add the parts
                parts = [p.strip() for p in last_ing.rsplit(" and ", 1)]
                if len(parts) == 2 and parts[1]:
                    ingredients[-1] = parts[0]
                    ingredients.append(parts[1])
        
        # Initialize empty lists for this row
        all_raw = []
        all_clean = []
        all_ids = []
        all_rows = []
        
        # Run NER on each ingredient separately
        if ingredients:
            # Process all ingredients in a batch for efficiency
            docs = list(nlp.pipe(ingredients, batch_size=batch_size, n_process=n_process))
            
            for ing_text, doc in zip(ingredients, docs):
                # Extract entities from this ingredient's doc
                rows = _extract_ingredient_rows(
                    doc, dedupe=dedupe, tok2id=tok2id, spacy_normalizer=spacy_normalizer
                )
                
                if rows:
                    # If NER found entities, use them
                    for r in rows:
                        all_raw.append(r["raw"])
                        if r["canonical"]:
                            all_clean.append(r["canonical"])
                        if tok2id:
                            all_ids.append(r["id"] if r["id"] is not None else 0)
                        all_rows.append(r)
                else:
                    # If NER found nothing, treat the whole ingredient text as the entity
                    # This handles cases where the model doesn't detect anything
                    norm_result = None
                    if spacy_normalizer is not None:
                        norm_result = spacy_normalizer._normalize_phrase(ing_text)
                    norm = norm_result if norm_result else normalize_token(ing_text)
                    canon = apply_dedupe(norm, dedupe)
                    tok_id = tok2id.get(canon, 0) if tok2id else None
                    
                    all_raw.append(ing_text)
                    if canon:
                        all_clean.append(canon)
                    if tok2id:
                        all_ids.append(int(tok_id) if tok_id is not None else 0)
                    all_rows.append({
                        "raw": ing_text,
                        "start": 0,
                        "end": len(ing_text),
                        "label": "INGREDIENT",
                        "norm": norm,
                        "canonical": canon,
                        "id": int(tok_id) if tok_id is not None else None,
                    })
        
        # Deduplicate clean list while preserving order
        clean_list = _unique_preserve_order(all_clean)
        
        # wide entry (compact)
        wide_entry: Dict = {
            text_col: raw_text,  # Store original input format
            "NER_raw": all_raw,
            "NER_clean": clean_list,
            "spans_json": json.dumps(all_rows, ensure_ascii=False),
        }
        if tok2id:
            wide_entry["Ingredients"] = all_ids
        wide_rows.append(wide_entry)

        # tall entries (one row per entity, great for QA/exploration)
        for r in all_rows:
            tall_records.append(
                {
                    "row_id": i,
                    text_col: raw_text,  # Store original input format
                    "ent_text": r["raw"],
                    "start": r["start"],
                    "end": r["end"],
                    "label": r["label"],
                    "norm": r["norm"],
                    "canonical": r["canonical"],
                    "id": r["id"],
                }
            )

    df_wide = pd.DataFrame(wide_rows)
    df_tall = pd.DataFrame(tall_records)
    
    # Diagnostic: count how many got ID=0 (not found in encoder)
    if tok2id:
        zero_ids = sum(1 for r in tall_records if r.get("id") == 0)
        total_entities = len(tall_records)
        if total_entities > 0:
            zero_pct = (zero_ids / total_entities) * 100
            print(f"\n[Diagnostic] ID mapping results:")
            print(f"  Total entities: {total_entities:,}")
            print(f"  Entities with ID=0 (not found): {zero_ids:,} ({zero_pct:.1f}%)")
            print(f"  Entities with valid ID: {total_entities - zero_ids:,} ({100-zero_pct:.1f}%)")
            
            # Check for recipes with only 1 ingredient ID
            recipes_with_one_id = sum(1 for w in wide_rows if tok2id and len(w.get("Ingredients", [])) == 1)
            total_recipes = len(wide_rows)
            if total_recipes > 0:
                one_id_pct = (recipes_with_one_id / total_recipes) * 100
                print(f"\n[Diagnostic] Recipe-level analysis:")
                print(f"  Total recipes: {total_recipes:,}")
                print(f"  Recipes with only 1 ingredient ID: {recipes_with_one_id:,} ({one_id_pct:.1f}%)")
                if one_id_pct > 10:
                    print(f"  ⚠ Note: Some recipes have only 1 ingredient ID.")
                    print(f"    This can happen if:")
                    print(f"    - Multiple ingredients normalize to the same canonical form")
                    print(f"    - Most ingredients get ID=0 (not in encoder vocabulary)")
                    print(f"    - NER model only detected 1 ingredient")
                    # Show a sample
                    sample = next((w for w in wide_rows if tok2id and len(w.get("Ingredients", [])) == 1 and len(w.get("NER_raw", [])) > 1), None)
                    if sample:
                        print(f"\n  Example recipe with 1 ID but multiple raw ingredients:")
                        print(f"    NER_raw ({len(sample.get('NER_raw', []))} items): {sample.get('NER_raw', [])[:5]}")
                        print(f"    NER_clean ({len(sample.get('NER_clean', []))} items): {sample.get('NER_clean', [])[:5]}")
                        print(f"    Ingredients ({len(sample.get('Ingredients', []))} IDs): {sample.get('Ingredients', [])}")
            
            if zero_pct > 50:
                print(f"\n  ⚠ Warning: >50% entities have ID=0. This suggests normalization mismatch.")
                print(f"    Ensure use_spacy_normalizer=True to match training pipeline normalization.")

    if out_path is not None:
        if not _HAS_PA:
            raise RuntimeError("pyarrow is required to write Parquet files.")
        base = Path(out_path)
        wide_path = base.with_name(base.stem + "_wide.parquet")
        tall_path = base.with_name(base.stem + "_tall.parquet")
        pq.write_table(
            pa.Table.from_pandas(df_wide, preserve_index=False).replace_schema_metadata(None),
            wide_path,
        )
        pq.write_table(
            pa.Table.from_pandas(df_tall, preserve_index=False).replace_schema_metadata(None),
            tall_path,
        )
        print(f"Wrote → {wide_path.name} and {tall_path.name} in {wide_path.parent}")

    return df_wide, df_tall


def load_dedupe_and_maps_from_config() -> Tuple[Optional[dict], Optional[dict]]:
    """
    Load dedupe map and token→ID mapping from config paths.
    
    Returns:
        Tuple of (dedupe_dict, tok2id_dict). Either can be None if files don't exist.
        - dedupe_dict: Maps normalized variant phrases → canonical forms
        - tok2id_dict: Maps canonical tokens → integer IDs
    
    Note: The dedupe map and encoder maps are built from spaCy-normalized tokens (NER_clean).
    Therefore, inference must also use spaCy normalization to match these tokens.
    """
    dedupe = None
    tok2id = None
    
    # Load dedupe map (JSONL format)
    if DATA.DEDUPE_JSONL and DATA.DEDUPE_JSONL.exists():
        dedupe = load_jsonl_map(DATA.DEDUPE_JSONL)
        print(f"✓ Loaded dedupe map: {len(dedupe):,} mappings from {DATA.DEDUPE_JSONL}")
        # Show a sample for debugging
        if dedupe and len(dedupe) > 0:
            sample_items = list(dedupe.items())[:3]
            print(f"  Sample entries: {sample_items}")
    else:
        print(f"⚠ Dedupe map not found at {DATA.DEDUPE_JSONL} (skipping deduplication)")
    
    # Load encoder maps (token ↔ ID)
    if DATA.ING_TOK2ID_JSON and DATA.ING_TOK2ID_JSON.exists():
        _, tok2id = load_encoder_maps(DATA.ING_ID2TOK_JSON, DATA.ING_TOK2ID_JSON)
        if tok2id:
            print(f"✓ Loaded token→ID map: {len(tok2id):,} tokens from {DATA.ING_TOK2ID_JSON}")
            # Show a sample for debugging
            if tok2id and len(tok2id) > 0:
                sample_items = list(tok2id.items())[:3]
                print(f"  Sample entries: {sample_items}")
        else:
            print(f"⚠ Token→ID map not found or empty at {DATA.ING_TOK2ID_JSON}")
    else:
        print(f"⚠ Token→ID map not found at {DATA.ING_TOK2ID_JSON} (skipping ID encoding)")
    
    return dedupe, tok2id


def run_full_inference_from_config(
    text_col: str,
    out_base: Path,
    data_path: Optional[Path] = None,
    sample_n: Optional[int] = None,
    sample_frac: Optional[float] = None,
    head_n: Optional[int] = None,
    batch_size: int = 256,
    n_process: int = 1,
    use_spacy_normalizer: bool = True,
    spacy_model: str = "en_core_web_sm",
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    High-level helper: run inference using config paths and settings.
    
    Args:
        text_col: Column name containing raw ingredient text
        out_base: Base path for output files (will write <base>_wide.parquet and <base>_tall.parquet)
        data_path: Optional override for input data (defaults to DATA.TRAIN_PATH)
        sample_n: Optional number of rows to sample
        sample_frac: Optional fraction of rows to sample
        head_n: Optional number of rows from head
        batch_size: Batch size for spaCy processing
        n_process: Number of processes (keep 1 for GPU/transformers, >1 may not work on Windows)
    
    Returns:
        Tuple of (df_wide, df_tall) DataFrames
    """
    # Determine input path
    if data_path is None:
        data_path = DATA.TRAIN_PATH
    if not data_path.exists():
        raise FileNotFoundError(f"Input data not found: {data_path}")
    
    # Determine if parquet or CSV
    is_parquet = DATA.DATA_IS_PARQUET if hasattr(DATA, 'DATA_IS_PARQUET') else (data_path.suffix.lower() == ".parquet")
    
    # Load dedupe and token→ID maps from config
    dedupe, tok2id = load_dedupe_and_maps_from_config()
    
    # Run inference
    return predict_normalize_encode_structured(
        nlp_dir=OUT.MODEL_DIR,
        data_path=data_path,
        is_parquet=is_parquet,
        text_col=text_col,
        dedupe=dedupe,
        tok2id=tok2id,
        out_path=out_base,
        batch_size=batch_size,
        sample_n=sample_n,
        sample_frac=sample_frac,
        head_n=head_n,
        n_process=n_process,
        use_spacy_normalizer=use_spacy_normalizer,
        spacy_model=spacy_model,
    )


__all__ = ["predict_normalize_encode_structured", "load_dedupe_and_maps_from_config", "run_full_inference_from_config"]

================================================================================
FILE: preprocess_pipeline\ingredient_ner\normalization.py
================================================================================
import json
from pathlib import Path
from typing import Dict, Optional, Tuple, Union

from .utils import normalize_token


def load_jsonl_map(path: Union[str, Path, None]) -> Dict[str, str]:
    """Load dedupe map from JSONL lines: {'from': '...', 'to': '...'}."""
    mapping: Dict[str, str] = {}
    if path is None:
        return mapping
    p = Path(path)
    if not p.exists():
        return mapping
    with open(p, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                obj = json.loads(line)
                src = normalize_token(str(obj.get("from", "")))
                dst = normalize_token(str(obj.get("to", "")))
                if src and dst:
                    mapping[src] = dst
    return mapping


def load_encoder_maps(
    id2tok_path: Optional[Path],
    tok2id_path: Optional[Path],
) -> Tuple[Optional[dict], Optional[dict]]:
    """Load IngredientEncoder id2tok / tok2id maps."""
    if not id2tok_path or not tok2id_path:
        return None, None
    if (not Path(id2tok_path).exists()) or (not Path(tok2id_path).exists()):
        return None, None
    with open(id2tok_path, "r", encoding="utf-8") as f:
        id2tok_raw = json.load(f)
    with open(tok2id_path, "r", encoding="utf-8") as f:
        tok2id_raw = json.load(f)
    id2tok = {int(k): str(v) for k, v in id2tok_raw.items()}
    tok2id = {str(k): int(v) for k, v in tok2id_raw.items()}
    return id2tok, tok2id


def apply_dedupe(tok: str, mapping: Optional[Dict[str, str]]) -> str:
    return mapping.get(tok, tok) if mapping else tok


__all__ = ["load_jsonl_map", "load_encoder_maps", "apply_dedupe"]

================================================================================
FILE: preprocess_pipeline\ingredient_ner\training.py
================================================================================
import logging
import random
import warnings
from pathlib import Path
from typing import Iterable, List, Tuple

import spacy
from spacy.language import Language
from spacy.tokens import DocBin
from spacy.training import Example

from .config import TRAIN, OUT
from .utils import configure_device, set_global_seed

logger = logging.getLogger(__name__)

try:
    import torch
except Exception:
    torch = None

# ---------- Simple in-memory training (optional) ----------

def train_spacy_ner_simple(
    train_docs: List[spacy.tokens.Doc],
    valid_docs: List[spacy.tokens.Doc],
    n_epochs: int = 10,
    lr: float = 0.001,
    dropout: float = 0.2,
    batch_size: int = 128,
) -> Language:
    """Simple CPU-only NER training for quick experiments."""
    nlp = spacy.blank("en")
    ner = nlp.add_pipe("ner")
    ner.add_label("INGREDIENT")

    # Convert docs to Examples (NER only)
    train_examples: List[Example] = []
    for d in train_docs:
        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in d.ents]
        train_examples.append(Example.from_dict(nlp.make_doc(d.text), {"entities": ents}))

    valid_examples: List[Example] = []
    for d in valid_docs:
        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in d.ents]
        valid_examples.append(Example.from_dict(nlp.make_doc(d.text), {"entities": ents}))

    optimizer = nlp.initialize(lambda: train_examples)
    logger.info(f"Initialized pipeline: {nlp.pipe_names}")

    for epoch in range(n_epochs):
        random.shuffle(train_examples)
        losses = {}
        for i in range(0, len(train_examples), batch_size):
            batch = train_examples[i: i + batch_size]
            nlp.update(batch, sgd=optimizer, drop=dropout, losses=losses)
        with nlp.select_pipes(disable=[p for p in nlp.pipe_names if p != "ner"]):
            scores = nlp.evaluate(valid_examples)
        logger.info(
            f"Epoch {epoch + 1:02d}/{n_epochs} - Losses: {losses} - "
            f"P/R/F1: {scores['ents_p']:.3f}/{scores['ents_r']:.3f}/{scores['ents_f']:.3f}"
        )
    return nlp


# ---------- Transformer / tok2vec training using DocBins ----------

def build_nlp_transformer() -> spacy.language.Language:
    """Build a small-window transformer + NER with optional layer freezing."""
    try:
        import spacy_transformers  # noqa: F401
    except Exception as e:
        raise RuntimeError("spacy-transformers is not available.") from e

    from .config import TRAIN as _TRAIN

    nlp = spacy.blank("en")
    trf_cfg = {
        "model": {
            "@architectures": "spacy-transformers.TransformerModel.v3",
            "name": _TRAIN.TRANSFORMER_MODEL,
            "tokenizer_config": {"use_fast": True},
            "transformer_config": {},
            "mixed_precision": bool(_TRAIN.USE_AMP),
            "grad_scaler_config": {"enabled": bool(_TRAIN.USE_AMP)},
            "get_spans": {
                "@span_getters": "spacy-transformers.strided_spans.v1",
                "window": int(_TRAIN.WINDOW),
                "stride": int(_TRAIN.STRIDE),
            },
        },
        "set_extra_annotations": {
            "@annotation_setters": "spacy-transformers.null_annotation_setter.v1"
        },
        "max_batch_items": 4096,
    }
    nlp.add_pipe("transformer", config=trf_cfg)
    ner = nlp.add_pipe("ner")
    ner.add_label("INGREDIENT")

    # Optional layer freezing
    if _TRAIN.FREEZE_LAYERS > 0:
        try:
            trf = nlp.get_pipe("transformer").model
            hf = trf.transformer.model
            blocks = None
            if hasattr(hf, "transformer") and hasattr(hf.transformer, "layer"):  # distilbert
                blocks = hf.transformer.layer
            elif hasattr(hf, "encoder") and hasattr(hf.encoder, "layer"):        # bert/roberta
                blocks = hf.encoder.layer
            if blocks is not None:
                k = min(_TRAIN.FREEZE_LAYERS, len(blocks))
                for i in range(k):
                    for p in blocks[i].parameters():
                        p.requires_grad = False
                logger.info(f"[transformer] Froze {k} lower layer(s).")
        except Exception as e:
            warnings.warn(f"Could not freeze layers: {e}")
    return nlp


def build_nlp_tok2vec() -> spacy.language.Language:
    """CPU-friendly tok2vec + NER fallback."""
    nlp = spacy.blank("en")
    nlp.add_pipe("tok2vec")
    ner = nlp.add_pipe("ner")
    ner.add_label("INGREDIENT")
    logger.info("Using tok2vec fallback (no transformers).")
    return nlp


def choose_nlp() -> Tuple[Language, str]:
    """Choose transformer or tok2vec pipeline based on config and available deps."""
    from .config import TRAIN as _TRAIN

    # Check if debug mode forces tok2vec
    if hasattr(_TRAIN, 'USE_TOK2VEC_DEBUG') and _TRAIN.USE_TOK2VEC_DEBUG:
        logger.info("Debug mode: using tok2vec instead of transformers (faster, CPU-friendly)")
        return build_nlp_tok2vec(), "tok2vec"

    if torch is not None:
        has_trf = True
        try:
            import spacy_transformers  # noqa
        except Exception:
            has_trf = False
        if has_trf:
            try:
                return build_nlp_transformer(), "transformer"
            except Exception as e:
                warnings.warn(f"Falling back to tok2vec due to: {e}")
                return build_nlp_tok2vec(), "tok2vec"
    # No torch or transformers
    return build_nlp_tok2vec(), "tok2vec"


from typing import Optional

try:
    from torch.utils.data import Dataset, DataLoader
except ImportError:
    Dataset = None
    DataLoader = None


class DocBinDataset(Dataset):
    """PyTorch Dataset wrapper for DocBin files to enable parallel loading.
    
    This dataset loads examples from DocBin shards. Each worker process will
    load shards independently, enabling parallel I/O.
    """
    
    def __init__(self, nlp: Language, dir_path: Path, shuffle: bool = False, max_docs: Optional[int] = None):
        self.nlp = nlp
        self.dir_path = dir_path
        self.shuffle = shuffle
        self.max_docs = max_docs
        
        # Store shard paths - each worker will load these independently
        self.shard_paths = sorted(p for p in dir_path.glob("*.spacy"))
        if shuffle:
            random.shuffle(self.shard_paths)
        
        # Pre-compute total length by counting docs in each shard
        # Note: This is done once during initialization, not during training iterations
        self.length = 0
        self.shard_sizes = []
        for sp_path in self.shard_paths:
            db = DocBin().from_disk(sp_path)
            n_docs = sum(1 for _ in db.get_docs(nlp.vocab))
            self.shard_sizes.append(n_docs)
            self.length += n_docs
            if max_docs is not None and self.length >= max_docs:
                self.length = max_docs
                break
        
        logger.debug(f"DocBinDataset: {len(self.shard_paths)} shard(s), {self.length} total examples")
    
    def __len__(self):
        return self.length
    
    def __getitem__(self, idx):
        """Get a single example by index.
        
        Note: This loads shards on-demand. With DataLoader num_workers > 0,
        each worker will cache shards independently, enabling parallel loading.
        """
        if idx >= self.length:
            raise IndexError(f"Index {idx} out of range for dataset of size {self.length}")
        
        # Find which shard contains this index
        current_idx = 0
        for shard_idx, (sp_path, shard_size) in enumerate(zip(self.shard_paths, self.shard_sizes)):
            if current_idx <= idx < current_idx + shard_size:
                # Load the shard and get the specific doc
                db = DocBin().from_disk(sp_path)
                docs = list(db.get_docs(self.nlp.vocab))
                local_idx = idx - current_idx
                if local_idx < len(docs):
                    d = docs[local_idx]
                    ents = [(e.start_char, e.end_char, e.label_) for e in d.ents]
                    return Example.from_dict(self.nlp.make_doc(d.text), {"entities": ents})
            current_idx += shard_size
            if self.max_docs is not None and current_idx >= self.max_docs:
                break
        
        raise IndexError(f"Could not find example at index {idx}")


def iter_examples_from_docbins(
    nlp: Language,
    dir_path: Path,
    shuffle: bool = False,
    max_docs: Optional[int] = None,
) -> Iterable[Example]:
    shard_paths = sorted(p for p in dir_path.glob("*.spacy"))
    logger.debug(f"iter_examples_from_docbins: found {len(shard_paths)} shard(s) in {dir_path}")
    if shuffle:
        random.shuffle(shard_paths)

    count = 0
    for sp_i, sp_path in enumerate(shard_paths, start=1):
        db = DocBin().from_disk(sp_path)
        for d in db.get_docs(nlp.vocab):
            ents = [(e.start_char, e.end_char, e.label_) for e in d.ents]
            yield Example.from_dict(nlp.make_doc(d.text), {"entities": ents})
            count += 1
            if max_docs is not None and count >= max_docs:
                logger.debug(f"max_docs={max_docs} reached, stopping iterator.")
                return


def sample_validation(nlp: Language, dir_path: Path, cap: int = 1500) -> List[Example]:
    out: List[Example] = []
    for eg in iter_examples_from_docbins(nlp, dir_path, shuffle=False):
        out.append(eg)
        if len(out) >= cap:
            break
    return out


def compounding_batch(epoch: int, total_epochs: int, start: int = 8, end: int = 16) -> int:
    if total_epochs <= 1:
        return end
    r = epoch / (total_epochs - 1)
    return max(1, int(round(start * ((end / start) ** r))))

def count_examples_in_docbins(nlp: Language, dir_path: Path) -> int:
    total = 0
    shard_paths = sorted(p for p in dir_path.glob("*.spacy"))
    logger.debug(f"Counting examples in {dir_path}, found {len(shard_paths)} shard(s)")
    for sp_i, sp_path in enumerate(shard_paths, start=1):
        db = DocBin().from_disk(sp_path)
        n_docs = sum(1 for _ in db.get_docs(nlp.vocab))
        total += n_docs
        logger.debug(f"  shard #{sp_i}: {sp_path.name} has {n_docs} docs")
    logger.debug(f"Total train examples in {dir_path}: {total}")
    return total

def train_ner_from_docbins(
    train_dir: Path | None = None,
    valid_dir: Path | None = None,
    out_model_dir: Path | None = None,
) -> Language:
    from .config import TRAIN as _TRAIN, OUT as _OUT
    import time

    train_dir = train_dir or _OUT.TRAIN_DIR
    valid_dir = valid_dir or _OUT.VALID_DIR
    out_model_dir = out_model_dir or _OUT.MODEL_DIR

    logger.debug(f"train_dir={train_dir}")
    logger.debug(f"valid_dir={valid_dir}")
    logger.debug(f"out_model_dir={out_model_dir}")

    # Configure device FIRST (before creating pipeline)
    configure_device()
    set_global_seed(_TRAIN.RANDOM_SEED)

    t0 = time.time()
    logger.debug("Choosing pipeline...")
    nlp, mode = choose_nlp()
    
    # Log device info for transformer models
    if mode == "transformer" and torch is not None:
        try:
            trf = nlp.get_pipe("transformer")
            if hasattr(trf, "model"):
                model_ref = trf.model.get_ref("model")
                if hasattr(model_ref, "device"):
                    logger.info(f"[device] Transformer model device: {model_ref.device}")
                
                # Also check the underlying PyTorch model
                if hasattr(trf, "model") and hasattr(trf.model, "get_ref"):
                    try:
                        transformer_model = trf.model.get_ref("model")
                        if hasattr(transformer_model, "transformer"):
                            # Check the actual PyTorch model device
                            pytorch_model = transformer_model.transformer.model
                            if hasattr(pytorch_model, "device"):
                                logger.info(f"[device] PyTorch model device: {pytorch_model.device}")
                            # Check first parameter device
                            if hasattr(pytorch_model, "parameters"):
                                first_param = next(pytorch_model.parameters(), None)
                                if first_param is not None:
                                    logger.info(f"[device] Model parameters on device: {first_param.device}")
                    except Exception as e:
                        logger.debug(f"Could not inspect PyTorch model device: {e}")
        except Exception as e:
            logger.debug(f"Could not determine transformer device: {e}")
    
    logger.debug(f"choose_nlp() done in {time.time() - t0:.1f}s")
    logger.info(f"Pipeline mode: {mode}")
    
    # Log debug mode status
    if hasattr(_TRAIN, 'MAX_TRAIN_DOCS') and _TRAIN.MAX_TRAIN_DOCS is not None:
        logger.info(f"Debug mode: max_train_docs={_TRAIN.MAX_TRAIN_DOCS}")

    # Warm init
    logger.debug("Collecting warm-up examples...")
    t0 = time.time()
    warm: List[Example] = []
    for eg_i, eg in enumerate(
        iter_examples_from_docbins(nlp, train_dir, shuffle=True),
        start=1,
    ):
        warm.append(eg)
        if eg_i % 50 == 0:
            logger.debug(f"warm example #{eg_i}")
        if eg_i >= min(256, max(16, 100)):
            break
    logger.debug(f"Collected {len(warm)} warm examples in {time.time() - t0:.1f}s")

    logger.debug("Calling nlp.initialize(...)")
    t0 = time.time()
    optimizer = nlp.initialize(lambda: warm)
    logger.debug(f"nlp.initialize(...) finished in {time.time() - t0:.1f}s")

    if hasattr(optimizer, "learn_rate"):
        optimizer.learn_rate = float(_TRAIN.LR)
        logger.debug(f"Set optimizer learn_rate={optimizer.learn_rate}")

    logger.debug("Building validation snapshot...")
    t0 = time.time()
    valid_snapshot = sample_validation(nlp, valid_dir, cap=_TRAIN.EVAL_SNAPSHOT_MAX)
    logger.debug(f"Validation snapshot size={len(valid_snapshot)} "
          f"built in {time.time() - t0:.1f}s")

    best_f1 = -1.0
    bad_epochs = 0

    logger.debug("Sanity-checking training set size...")
    _ = count_examples_in_docbins(nlp, train_dir)

    # Setup DataLoader for parallel data loading if PyTorch is available
    use_dataloader = (Dataset is not None and DataLoader is not None and 
                     torch is not None and _TRAIN.DATA_LOADER_WORKERS > 0)
    
    if use_dataloader:
        logger.info(f"Using DataLoader with {_TRAIN.DATA_LOADER_WORKERS} worker(s) for parallel data loading")
        # Use a reasonable batch_size for prefetching (larger than micro-batch for efficiency)
        # This is just for prefetching - we still accumulate into micro-batches
        dataloader_batch_size = max(_TRAIN.BATCH_SIZE // 4, 32)  # Prefetch batch size
        logger.debug(f"DataLoader prefetch batch_size={dataloader_batch_size}")
    else:
        logger.info("Using sequential data loading (no DataLoader)")
        if Dataset is None or DataLoader is None:
            logger.debug("PyTorch DataLoader not available")
        elif _TRAIN.DATA_LOADER_WORKERS == 0:
            logger.debug("DataLoader workers set to 0, using sequential loading")

    for epoch in range(_TRAIN.N_EPOCHS):
        logger.info(f"===== Epoch {epoch + 1}/{_TRAIN.N_EPOCHS} =====")
        losses: dict = {}
        micro_bs = compounding_batch(epoch, _TRAIN.N_EPOCHS, start=8, end=16)
        logger.debug(f"micro-batch size (μbs) = {micro_bs}")
        buf: List[Example] = []
        updates = 0

        t_epoch = time.time()
        
        if use_dataloader:
            # Use DataLoader for parallel data loading
            dataset = DocBinDataset(nlp, train_dir, shuffle=True, max_docs=_TRAIN.MAX_TRAIN_DOCS)
            
            # Custom collate function that just returns the list as-is (no tensor collation)
            def collate_examples(batch):
                """Collate function that preserves spaCy Examples without converting to tensors."""
                return batch
            
            dataloader = DataLoader(
                dataset,
                batch_size=dataloader_batch_size,
                shuffle=False,  # We shuffle at the dataset level
                num_workers=_TRAIN.DATA_LOADER_WORKERS,
                pin_memory=False,  # spaCy Examples may not be tensors
                drop_last=False,
                collate_fn=collate_examples,
            )
            
            eg_i = 0
            for batch in dataloader:
                # DataLoader returns batches as lists (due to custom collate_fn)
                for eg in batch:
                    eg_i += 1
                    buf.append(eg)
                    if len(buf) < micro_bs:
                        continue
                    nlp.update(buf, sgd=optimizer, drop=_TRAIN.DROPOUT, losses=losses)
                    buf.clear()
                    updates += 1

                    if updates % 20 == 0:
                        logger.debug(f"epoch={epoch+1} updates={updates} "
                              f"seen_examples≈{eg_i} loss={losses.get('ner', 0):.2f}")

                    if (torch is not None) and torch.cuda.is_available() \
                            and updates % _TRAIN.CLEAR_CACHE_EVERY == 0:
                        logger.debug("emptying CUDA cache...")
                        torch.cuda.empty_cache()
        else:
            # Fallback to sequential loading
            for eg_i, eg in enumerate(iter_examples_from_docbins(nlp, train_dir, shuffle=True), start=1):
                if _TRAIN.MAX_TRAIN_DOCS is not None and eg_i > _TRAIN.MAX_TRAIN_DOCS:
                    break
                buf.append(eg)
                if len(buf) < micro_bs:
                    continue
                nlp.update(buf, sgd=optimizer, drop=_TRAIN.DROPOUT, losses=losses)
                buf.clear()
                updates += 1

                if updates % 20 == 0:
                    logger.debug(f"epoch={epoch+1} updates={updates} "
                          f"seen_examples≈{eg_i} loss={losses.get('ner', 0):.2f}")

                if (torch is not None) and torch.cuda.is_available() \
                        and updates % _TRAIN.CLEAR_CACHE_EVERY == 0:
                    logger.debug("emptying CUDA cache...")
                    torch.cuda.empty_cache()

        if buf:
            logger.debug(f"Flushing last batch of size {len(buf)}")
            nlp.update(buf, sgd=optimizer, drop=_TRAIN.DROPOUT, losses=losses)
            buf.clear()

        logger.debug(f"Finished epoch {epoch+1} updates={updates} "
              f"in {time.time() - t_epoch:.1f}s")

        logger.debug("Running evaluation on validation snapshot...")
        with nlp.select_pipes(disable=[p for p in nlp.pipe_names if p != "ner"]):
            scores = nlp.evaluate(valid_snapshot)

        p = float(scores.get("ents_p") or 0.0)
        r = float(scores.get("ents_r") or 0.0)
        f1 = float(scores.get("ents_f") or 0.0)

        logger.info(
            f"Epoch {epoch + 1:02d}/{_TRAIN.N_EPOCHS} | μbs={micro_bs:<3d} "
            f"| loss={losses.get('ner', 0):.1f} | P/R/F1={p:.3f}/{r:.3f}/{f1:.3f}"
        )

        improved = f1 > best_f1 + 1e-6
        if improved:
            best_f1 = f1
            bad_epochs = 0
            out_model_dir.mkdir(parents=True, exist_ok=True)
            nlp.to_disk(out_model_dir)
            logger.info(f"  ↳ Saved model-best → {out_model_dir} (F1={f1:.3f})")
        else:
            bad_epochs += 1
            if bad_epochs >= _TRAIN.EARLY_STOPPING_PATIENCE:
                logger.info(f"Early stopping after {bad_epochs} non-improving epoch(s).")
                break

    logger.info(f"Best F1 observed: {best_f1 if best_f1 >= 0 else 0.0}")
    return nlp


__all__ = [
    "train_spacy_ner_simple",
    "build_nlp_transformer",
    "build_nlp_tok2vec",
    "choose_nlp",
    "iter_examples_from_docbins",
    "sample_validation",
    "compounding_batch",
    "train_ner_from_docbins",
]

================================================================================
FILE: preprocess_pipeline\ingredient_ner\utils.py
================================================================================
import ast
import json
import logging
import random
import warnings
from pathlib import Path
from typing import Any, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
import spacy

try:
    import pyarrow.parquet as pq
    _HAS_PA = True
except ImportError:
    _HAS_PA = False

try:
    import torch
except ImportError:
    torch = None

# We use a global logger for this module
logger = logging.getLogger(__name__)

def set_global_seed(seed: int = 42) -> None:
    """Set seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    if torch is not None:
        try:
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(seed)
        except Exception:
            pass

def configure_device(use_gpu: bool = True) -> None:
    """
    Configure device for training/inference.
    
    This version respects the user's config but avoids spacy.require_gpu() 
    if it causes conflicts, relying on PyTorch directly.
    """
    if not use_gpu:
        logger.info("[device] GPU disabled by config. Using CPU.")
        return

    if torch is None:
        logger.warning("[device] Torch not installed. Using CPU.")
        return

    if torch.cuda.is_available():
        dev_name = torch.cuda.get_device_name(0)
        logger.info(f"[device] CUDA available. PyTorch will use GPU: {dev_name}")
        # NOTE: We intentionally do NOT call spacy.require_gpu() here 
        # to avoid conflicts with thinc/cupy if they aren't perfectly aligned.
    else:
        logger.info("[device] CUDA not available. Using CPU.")

def join_with_offsets(tokens: List[str], sep: str = ", "): 
    """Join tokens with a separator and track character offsets of each token.""" 
    text, spans, pos = [], [], 0 
    for i, tok in enumerate(tokens): 
        start, end = pos, pos + len(tok) 
        text.append(tok) 
        spans.append((start, end))
        pos = end 
        if i < len(tokens) - 1: 
            text.append(sep) 
            pos += len(sep) 
    return "".join(text), spans

def _read_csv_with_fallback(path: Path, dtype=str, nrows: Optional[int] = None) -> Tuple[pd.DataFrame, str]:
    """
    Internal helper: Tries to read CSV using UTF-8, then cp1252 (Windows), then latin-1.
    Returns (DataFrame, encoding_used).
    """
    encodings = ["utf-8", "cp1252", "latin-1"]
    
    for enc in encodings:
        try:
            # Use default C engine for speed
            df = pd.read_csv(path, dtype=dtype, encoding=enc, nrows=nrows)
            return df, enc
        except UnicodeDecodeError:
            continue # Try next encoding
        except Exception as e:
            logger.warning(f"Error reading with {enc}: {e}")
            raise

    # Last resort: Python engine with 'replace' (lossy but works)
    logger.warning("All standard encodings failed. Trying utf-8 with errors='replace'.")
    df = pd.read_csv(
        path, 
        dtype=dtype, 
        encoding="utf-8", 
        engine="python", 
        on_bad_lines="warn", 
        nrows=nrows
    )
    return df, "utf-8-replace"

def load_data(path: Union[str, Path], is_parquet: bool, col: str, max_rows: Optional[int] = None) -> pd.DataFrame:
    """
    Load a single column from CSV/Parquet and return a clean DataFrame.
    
    Handles Windows encoding (0xae / ®) issues automatically.
    """
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"Input file not found: {path}")

    df = pd.DataFrame()

    if is_parquet:
        if not _HAS_PA:
            raise RuntimeError("pyarrow is required to read Parquet files.")
        
        # Parquet reading logic
        if max_rows:
            pf = pq.ParquetFile(str(path))
            rows_read = 0
            frames = []
            for i in range(pf.num_row_groups):
                if rows_read >= max_rows: break
                chunk = pf.read_row_group(i, columns=[col]).to_pandas()
                frames.append(chunk)
                rows_read += len(chunk)
            df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
            if len(df) > max_rows:
                df = df.head(max_rows)
        else:
            df = pd.read_parquet(path, columns=[col])
            
    else:
        # CSV Reading with Fallback Logic
        # FIX: We unpack the tuple here so we return ONLY the dataframe
        df, used_encoding = _read_csv_with_fallback(path, dtype=str, nrows=max_rows)
        logger.info(f"Loaded CSV using encoding: {used_encoding}")

    # Column Validation
    if col not in df.columns:
        raise KeyError(f"Column '{col}' not found. Available: {list(df.columns)[:5]}...")
    
    # Return clean, non-empty dataframe
    return df[[col]].dropna().reset_index(drop=True)

def parse_listlike(v: Any) -> List[str]:
    """Parse values that may be python-lists, JSON-lists, or comma-separated strings."""
    
    # 1. Handle list-likes (arrays, lists, tuples) FIRST.
    # We do this check first to avoid passing an array to pd.isna(), which causes the ValueError.
    if isinstance(v, (list, tuple, np.ndarray)):
        # Convert elements to strings and filter empty ones
        return [str(x).strip() for x in v if str(x).strip()]
    
    # 2. Handle scalars (strings, floats, None).
    # Now we are safe to use pd.isna() because we know v is not an array.
    if pd.isna(v):
        return []
    
    s = str(v).strip()
    if not s:
        return []
        
    # 3. Try JSON/Literal eval (e.g. "['salt', 'pepper']")
    if s.startswith("[") and s.endswith("]"):
        for parser in (json.loads, ast.literal_eval):
            try:
                out = parser(s)
                # Ensure the result is actually a list/iterable
                if isinstance(out, (list, tuple, np.ndarray)):
                    return [str(x).strip() for x in out if str(x).strip()]
            except Exception:
                pass
                
    # 4. Fallback to comma separation
    return [x.strip() for x in s.split(",") if x.strip()]

def normalize_token(s: str) -> str:
    """Lowercase, trim and collapse whitespace for consistent string keys."""
    return " ".join(str(s).strip().lower().split())

================================================================================
FILE: preprocess_pipeline\ingredient_ner\viz.py
================================================================================
import json

import pandas as pd
from IPython.display import display, HTML
from html import escape


def preview_side_by_side(df_wide: pd.DataFrame, text_col: str, n: int = 8):
    """Simple tabular 'original vs cleaned' preview."""
    cols = [text_col, "NER_raw", "NER_clean"] + (
        ["Ingredients"] if "Ingredients" in df_wide.columns else []
    )
    display(df_wide.loc[:, cols].head(n))


def _render_marked(text: str, spans: list[dict]) -> str:
    """Mark entities inline; tooltip shows norm/canonical/id for quick QA."""
    spans = sorted(spans, key=lambda r: r["start"])
    pos = 0
    out = []
    for r in spans:
        out.append(escape(text[pos: r["start"]]))
        frag = escape(text[r["start"]: r["end"]])
        tip = (
            f'norm="{r["norm"]}" | canonical="{r["canonical"]}" | '
            f'id={r["id"] if r["id"] is not None else "-"}'
        )
        out.append(f'<mark title="{escape(tip)}">{frag}</mark>')
        pos = r["end"]
    out.append(escape(text[pos:]))
    return "".join(out)


def html_preview(df_wide: pd.DataFrame, text_col: str, n: int = 8):
    """Inline HTML with highlighted entities and cleaned list below."""
    rows = []
    for _, row in df_wide.head(n).iterrows():
        spans = json.loads(row["spans_json"])
        marked = _render_marked(row[text_col], spans)
        cleaned = ", ".join(row.get("NER_clean") or [])
        rows.append(
            f"""
        <div class="one">
          <div class="orig">{marked}</div>
          <div class="clean"><strong>NER_clean:</strong> {escape(cleaned)}</div>
        </div>
        """
        )
    style = """
    <style>
      .one{border:1px solid #ddd; padding:10px; margin:8px 0; border-radius:6px;}
      .orig{margin-bottom:6px; line-height:1.5}
      mark{padding:0 2px; border-radius:3px}
      .clean{font-family:monospace}
    </style>
    """
    display(HTML(style + "\n".join(rows)))


def describe_predictions(df_wide: pd.DataFrame, top_k: int = 20):
    """Small summary to sanity-check output distribution."""
    s = df_wide["NER_clean"].explode().value_counts().head(top_k)
    print(
        f"Rows: {len(df_wide):,} | "
        f"rows with ≥1 pred: {(df_wide['NER_clean'].map(bool)).sum():,}"
    )
    print(
        f"Mean #unique preds/row: {df_wide['NER_clean'].map(len).mean():.2f}"
    )
    display(s.to_frame("freq"))


__all__ = ["preview_side_by_side", "html_preview", "describe_predictions"]

================================================================================
FILE: preprocess_pipeline\ingrnorm\__init__.py
================================================================================

from .io import materialize_parquet_source
from .parquet_utils import vocab_from_parquet_listcol
from .spacy_normalizer import apply_spacy_normalizer_to_parquet, SpacyIngredientNormalizer
from .w2v_dedupe import w2v_dedupe
from .sbert_dedupe import sbert_dedupe
from .dedupe_map import load_jsonl_map, write_jsonl_map, apply_map_to_parquet_streaming
from .encoder import IngredientEncoder

__all__ = [
    "materialize_parquet_source",
    "vocab_from_parquet_listcol",
    "apply_spacy_normalizer_to_parquet",
    "SpacyIngredientNormalizer",
    "w2v_dedupe",
    "sbert_dedupe",
    "load_jsonl_map",
    "write_jsonl_map",
    "apply_map_to_parquet_streaming",
    "IngredientEncoder",
]

================================================================================
FILE: preprocess_pipeline\ingrnorm\dedupe_map.py
================================================================================

from __future__ import annotations
import json
from pathlib import Path
from typing import Dict, Union
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq

def load_jsonl_map(path: Union[str, Path]) -> Dict[str, str]:
    mapping: Dict[str, str] = {}
    p = Path(path)
    if not p.exists():
        return mapping
    with open(p, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            obj = json.loads(line)
            mapping[str(obj.get("from",""))] = str(obj.get("to",""))
    return mapping

def write_jsonl_map(mapping: Dict[str, str], out_path: Union[str, Path]) -> Path:
    out_path = Path(out_path); out_path.parent.mkdir(parents=True, exist_ok=True)
    data = "\n".join(json.dumps({"from": k, "to": v}) for k, v in mapping.items())
    # Write directly - file comparison is inefficient for large files
    # If needed, use --force flag in scripts to rebuild
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(data + "\n")
    return out_path

def apply_map_to_parquet_streaming(
    in_path: Union[str, Path],
    out_path: Union[str, Path],
    mapping: Union[Dict[str, str], str, Path],
    list_col: str = "NER_clean",
    compression: str = "zstd",
) -> None:
    if not isinstance(mapping, dict):
        mapping = load_jsonl_map(mapping)

    pf = pq.ParquetFile(str(in_path))
    writer = None
    out_path = Path(out_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    for rg in range(pf.num_row_groups):
        df = pf.read_row_group(rg).to_pandas()
        if list_col in df.columns:
            # Handle numpy arrays and other list-like types
            df[list_col] = [
                [mapping.get(str(tok), str(tok)) for tok in (lst if isinstance(lst, (list, tuple)) else list(lst))]
                if isinstance(lst, (list, tuple, np.ndarray)) and len(lst) > 0
                else (lst if isinstance(lst, (list, tuple)) else [])
                for lst in df[list_col]
            ]
        table = pa.Table.from_pandas(df, preserve_index=False).replace_schema_metadata(None)
        if writer is None:
            writer = pq.ParquetWriter(str(out_path), table.schema, compression=compression)
        writer.write_table(table)
    if writer is not None:
        writer.close()

================================================================================
FILE: preprocess_pipeline\ingrnorm\encoder.py
================================================================================

from __future__ import annotations
from pathlib import Path
from typing import Iterable, Any, Optional, Dict, Union, List
from collections import Counter
import json
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

class IngredientEncoder:
    """Deterministic token→id encoder for already-normalized ingredient tokens."""
    def __init__(self, min_freq: int = 1):
        self.min_freq = int(min_freq)
        self.token_to_id: Dict[str, int] = {"<UNK>": 0}
        self.id_to_token: Dict[int, str] = {0: "<UNK>"}
        self._frozen = False

    def _fit_from_counter(self, cnt: Counter, min_freq: int) -> "IngredientEncoder":
        """Internal helper to add tokens from a counter to the encoder."""
        for tok, c in cnt.items():
            if c >= min_freq and tok not in self.token_to_id:
                idx = len(self.token_to_id)
                self.token_to_id[tok] = idx
                self.id_to_token[idx] = tok
        return self

    def _count_tokens(self, series: Iterable[Any]) -> Counter:
        """Count tokens from an iterable of lists."""
        cnt = Counter()
        for lst in series:
            if isinstance(lst, (list, tuple, np.ndarray)):
                for t in lst:
                    s = str(t).strip().lower()
                    if s:
                        cnt[s] += 1
        return cnt

    def fit_from_series(self, series: Iterable[Any], min_freq: Optional[int] = None) -> "IngredientEncoder":
        mf = self.min_freq if min_freq is None else int(min_freq)
        cnt = self._count_tokens(series)
        return self._fit_from_counter(cnt, mf)

    def fit_from_parquet_streaming(self, parquet_path: Union[str, Path], col: str = "NER_clean", min_freq: Optional[int] = None) -> "IngredientEncoder":
        parquet_path = Path(parquet_path)
        if not parquet_path.exists():
            raise FileNotFoundError(f"Parquet file not found: {parquet_path}")
        mf = self.min_freq if min_freq is None else int(min_freq)
        pf = pq.ParquetFile(parquet_path)
        if pf.num_row_groups == 0:
            raise ValueError(f"Parquet file has no row groups: {parquet_path}")
        cnt = Counter()
        for rg in range(pf.num_row_groups):
            df_rg = pf.read_row_group(rg, columns=[col]).to_pandas()
            if col not in df_rg.columns:
                raise ValueError(f"Column '{col}' not found in parquet file {parquet_path} (row group {rg})")
            for lst in df_rg[col]:
                if isinstance(lst, (list, tuple, np.ndarray)):
                    for t in lst:
                        s = str(t).strip().lower()
                        if s:
                            cnt[s] += 1
        return self._fit_from_counter(cnt, mf)

    def freeze(self) -> "IngredientEncoder":
        self._frozen = True
        return self

    def transform_series_to_idlists(self, series: Iterable[Any]) -> List[List[int]]:
        out: List[List[int]] = []
        for lst in series:
            ids: List[int] = []
            if isinstance(lst, (list, tuple, np.ndarray)):
                for t in lst:
                    tok = str(t).strip().lower()
                    if tok:
                        ids.append(self.token_to_id.get(tok, 0))
            else:
                ids = [0]
            out.append(ids if ids else [0])
        return out

    def transform_df(self, df: pd.DataFrame, ingredients_col: str = "NER_clean", dataset_id: int = 1) -> pd.DataFrame:
        id_lists = self.transform_series_to_idlists(df[ingredients_col])
        res = pd.DataFrame({
            "Dataset ID": np.int32(dataset_id),
            "Index": np.arange(len(df), dtype=np.int64),
            "Ingredients": id_lists,
        })
        return res

    def encode_parquet_streaming(self, parquet_path: Union[str, Path], out_parquet_path: Union[str, Path], dataset_id: int = 1, col: str = "NER_clean", compression: str = "zstd") -> Path:
        parquet_path = Path(parquet_path)
        out_parquet_path = Path(out_parquet_path)
        pf = pq.ParquetFile(parquet_path)
        target_schema = pa.schema([
            pa.field("Dataset ID", pa.int32()),
            pa.field("Index", pa.int64()),
            pa.field("Ingredients", pa.list_(pa.int64())),
        ])
        out_parquet_path.parent.mkdir(parents=True, exist_ok=True)
        writer = pq.ParquetWriter(out_parquet_path, target_schema, compression=compression)
        global_index_start = 0
        for rg in range(pf.num_row_groups):
            df_rg = pf.read_row_group(rg, columns=[col]).to_pandas()
            id_lists = self.transform_series_to_idlists(df_rg[col])
            ds_ids = pa.array(np.full(len(id_lists), dataset_id, dtype=np.int32))
            idxs   = pa.array(np.arange(global_index_start, global_index_start + len(id_lists), dtype=np.int64))
            ingr   = pa.array([(lst if isinstance(lst, (list, tuple, np.ndarray)) else []) for lst in id_lists], type=pa.list_(pa.int64()))
            tbl = pa.Table.from_arrays([ds_ids, idxs, ingr], names=["Dataset ID", "Index", "Ingredients"])
            writer.write_table(tbl)
            global_index_start += len(id_lists)
        writer.close()
        return out_parquet_path

    def save_maps(self, id_to_token_path: Union[str, Path], token_to_id_path: Optional[Union[str, Path]] = None) -> None:
        id_to_token_path = Path(id_to_token_path)
        token_to_id_path = Path(token_to_id_path) if token_to_id_path else id_to_token_path.with_name("ingredient_token_to_id.json")
        id_to_token_path.parent.mkdir(parents=True, exist_ok=True)
        with open(id_to_token_path, "w", encoding="utf-8") as f:
            json.dump({str(k): v for k, v in self.id_to_token.items()}, f, indent=2)
        with open(token_to_id_path, "w", encoding="utf-8") as f:
            json.dump(self.token_to_id, f, indent=2)

    @classmethod
    def load_maps(cls, id_to_token_path: Union[str, Path], token_to_id_path: Optional[Union[str, Path]] = None) -> "IngredientEncoder":
        id_to_token_path = Path(id_to_token_path)
        token_to_id_path = Path(token_to_id_path) if token_to_id_path else id_to_token_path.with_name("ingredient_token_to_id.json")
        with open(id_to_token_path, "r", encoding="utf-8") as f:
            id_to_token_raw = json.load(f)
        with open(token_to_id_path, "r", encoding="utf-8") as f:
            token_to_id_raw = json.load(f)
        enc = cls()
        enc.id_to_token = {int(k): str(v) for k, v in id_to_token_raw.items()}
        enc.token_to_id = {str(k): int(v) for k, v in token_to_id_raw.items()}
        enc._frozen = True
        return enc

================================================================================
FILE: preprocess_pipeline\ingrnorm\io.py
================================================================================

from __future__ import annotations
import ast, json
from pathlib import Path
from typing import List
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

def parse_listish(v) -> list[str]:
    """Accept list/tuple/np.ndarray; try JSON/Python list in string; fallback to single-item list."""
    if v is None or (isinstance(v, float) and pd.isna(v)):
        return []
    if isinstance(v, (list, tuple, np.ndarray)):
        return [str(x) for x in v if str(x).strip()]
    s = str(v).strip()
    if not s:
        return []
    if (s.startswith("[") and s.endswith("]")) or (s.startswith("(") and s.endswith(")")):
        parsed = None
        try:
            parsed = json.loads(s)
        except Exception:
            try:
                parsed = ast.literal_eval(s)
            except Exception:
                parsed = None
        if isinstance(parsed, (list, tuple, np.ndarray)):
            return [str(x) for x in parsed if str(x).strip()]
    return [s]

def materialize_parquet_source(input_path: Path, ner_col: str, chunksize: int, tmp_out: Path) -> Path:
    """Ensure we have a Parquet file with a list<string> column `ner_col` from CSV/Excel/Parquet."""
    suffix = input_path.suffix.lower()
    if suffix == ".parquet":
        return input_path

    if suffix == ".csv":
        tmp_out.parent.mkdir(parents=True, exist_ok=True)
        schema = pa.schema([pa.field(ner_col, pa.list_(pa.string()))])
        writer = pq.ParquetWriter(str(tmp_out), schema, compression="zstd")
        for chunk in pd.read_csv(input_path, chunksize=chunksize, dtype=str):
            col = chunk[ner_col] if ner_col in chunk.columns else pd.Series([None] * len(chunk))
            lists = [parse_listish(x) for x in col]
            arr = pa.array(lists, type=pa.list_(pa.string()))
            tbl = pa.Table.from_arrays([arr], names=[ner_col])
            writer.write_table(tbl)
        writer.close()
        return tmp_out

    if suffix in (".xlsx", ".xls"):
        df = pd.read_excel(input_path, dtype=str)
        col = df[ner_col] if ner_col in df.columns else pd.Series([None] * len(df))
        lists = [parse_listish(x) for x in col]
        arr = pa.array(lists, type=pa.list_(pa.string()))
        tbl = pa.Table.from_arrays([arr], names=[ner_col])
        tmp_out.parent.mkdir(parents=True, exist_ok=True)
        pq.write_table(tbl, str(tmp_out), compression="zstd")
        return tmp_out

    raise ValueError(f"Unsupported file type: {input_path.name}")

================================================================================
FILE: preprocess_pipeline\ingrnorm\parquet_utils.py
================================================================================

from __future__ import annotations
from collections import Counter
from pathlib import Path
from typing import Dict, Union
import numpy as np
import pyarrow.parquet as pq

def vocab_from_parquet_listcol(path: Union[str, Path], col: str = "NER_clean", min_freq: int = 1) -> Dict[str, int]:
    path = Path(path)
    pf = pq.ParquetFile(path)
    cnt = Counter()
    for rg in range(pf.num_row_groups):
        df = pf.read_row_group(rg, columns=[col]).to_pandas()
        if col not in df.columns:
            continue
        for v in df[col]:
            if isinstance(v, (list, tuple, np.ndarray)):
                for t in v:
                    s = str(t).strip()
                    if s:
                        cnt[s] += 1
    if min_freq > 1:
        cnt = Counter({k: c for k, c in cnt.items() if c >= min_freq})
    return dict(cnt)

================================================================================
FILE: preprocess_pipeline\ingrnorm\sbert_dedupe.py
================================================================================

from __future__ import annotations
from pathlib import Path
from typing import Dict, List
import json
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import NearestNeighbors

GENERIC = {"salt", "water", "oil", "sugar"}
def _tokset(s: str) -> set:
    return set(s.split())

def sbert_dedupe(
    vocab_counter: Dict[str, int],
    out_path: Path | str,
    model_name: str = "all-MiniLM-L6-v2",
    threshold: float = 0.88,
    topk: int = 25,
    min_len: int = 2,
    require_token_overlap: bool = True,
    block_generic_as_canon: bool = True,
) -> Dict[str, str]:
    phrases = [p.strip().lower() for p, c in vocab_counter.items() if c > 0]
    freqs   = np.array([vocab_counter[p] for p in phrases], dtype=np.int64)
    if not phrases:
        raise ValueError("sbert_dedupe: empty vocab")

    model = SentenceTransformer(model_name)
    X = model.encode(phrases, normalize_embeddings=True, show_progress_bar=True)

    nn = NearestNeighbors(n_neighbors=min(topk + 1, len(phrases)), metric="cosine", algorithm="auto")
    nn.fit(X)
    dists, idxs = nn.kneighbors(X, return_distance=True)

    # Centroid-based clustering: avoid union-find chaining
    # Sort phrases by frequency (descending) to process most common first
    phrase_indices = list(range(len(phrases)))
    phrase_indices.sort(key=lambda i: (-freqs[i], len(phrases[i]), phrases[i]))
    
    clusters: Dict[int, List[int]] = {}  # cluster_id -> list of phrase indices
    cluster_centroids: Dict[int, int] = {}  # cluster_id -> centroid phrase index
    cluster_vectors: Dict[int, np.ndarray] = {}  # cluster_id -> centroid vector
    next_cluster_id = 0
    
    # Verification threshold: members must be within this distance of centroid
    centroid_verification_threshold = threshold * 0.95  # Slightly stricter than merge threshold
    
    for i in phrase_indices:
        assigned = False
        phrase_i = phrases[i]
        vec_i = X[i]
        
        # Check if phrase_i can join an existing cluster
        for cluster_id, centroid_idx in cluster_centroids.items():
            centroid_vec = cluster_vectors[cluster_id]
            centroid_phrase = phrases[centroid_idx]
            
            # Check similarity to centroid
            cos_sim = float(np.dot(vec_i, centroid_vec))
            if cos_sim < threshold:
                continue
            
            # Check lexical requirements
            if len(phrase_i) < min_len or len(centroid_phrase) < min_len:
                continue
            if require_token_overlap and not (_tokset(phrase_i) & _tokset(centroid_phrase)):
                continue
            
            # Verify member is actually close to centroid (not just a neighbor)
            if cos_sim >= centroid_verification_threshold:
                clusters[cluster_id].append(i)
                assigned = True
                break
        
        # If not assigned, create new cluster with this phrase as centroid
        if not assigned:
            cluster_id = next_cluster_id
            next_cluster_id += 1
            clusters[cluster_id] = [i]
            cluster_centroids[cluster_id] = i
            cluster_vectors[cluster_id] = vec_i

    mapping: Dict[str, str] = {}
    for cluster_id, members in clusters.items():
        # Use centroid as canonical form (already selected as most representative)
        centroid_idx = cluster_centroids[cluster_id]
        canon = phrases[centroid_idx]
        
        # Block generic as canon if there are alternatives
        if block_generic_as_canon and canon in GENERIC and len(members) > 1:
            # Find highest frequency non-generic member
            members_sorted = sorted(members, key=lambda m: (-freqs[m], len(phrases[m]), phrases[m]))
            alt = next((phrases[m] for m in members_sorted if phrases[m] not in GENERIC), canon)
            canon = alt
        
        # Map all members to canonical form
        for m in members:
            mapping[phrases[m]] = canon

    out_path = Path(out_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        for src, tgt in mapping.items():
            if src and tgt and src != tgt:
                f.write(json.dumps({"from": src, "to": tgt}) + "\n")
    return mapping

================================================================================
FILE: preprocess_pipeline\ingrnorm\spacy_normalizer.py
================================================================================
from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, List, Set, Union, Sequence

import time
import re
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import spacy
from spacy.tokens import Token, Doc
import logging

DROP_ADJ: Set[str] = {
    "fresh","ripe","frozen","thawed","organic","unsalted","salted","sweetened","unsweetened",
    "minced","chopped","diced","sliced","crushed","ground","grated","shredded","powdered",
    "large","small","medium","extra","virgin","golden","dark","light","skinless","boneless",
}
UNITS: Set[str] = {"cup","cups","tbsp","tablespoon","tablespoons","tsp","teaspoon","teaspoons",
                   "g","kg","oz","ounce","ounces","ml","l","lb","lbs","pound","pounds"}
ALNUM = re.compile(r"[a-z0-9]+")
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Regex patterns for bronze layer cleaning
PARENTHESES_PATTERN = re.compile(r'\([^)]*\)')
COMMERCIAL_SYMBOLS_PATTERN = re.compile(r'[®™©]')
BRAND_ARTIFACTS_PATTERN = re.compile(r'\b(Inc\.|LLC|Ltd\.|Corp\.|Corporation|Company|Co\.)\b', re.IGNORECASE)
MULTISPACE_PATTERN = re.compile(r'\s+')


@dataclass
class SpacyIngredientNormalizer:
    model: str = "en_core_web_sm"
    batch_size: int = 128
    n_process: int = 1  # Multiprocessing: 0=auto, 1=single-threaded, >1=multiprocess (may not work on Windows)

    def __post_init__(self):
        # Disable components we don't need for ingredient normalization
        # We only need tokenizer + parser for dependency parsing
        self.nlp = spacy.load(
            self.model,
            disable=["ner", "textcat", "lemmatizer", "senter", "attribute_ruler"],
        )
        # Optimize for speed: disable unnecessary pipeline components
        # We only need tokenizer and parser for dependency analysis
        Token.set_extension("keep", default=True, force=True)
    
    @staticmethod
    def clean_raw_text(text: str) -> str:
        """
        Bronze layer cleaner: Remove artifacts from raw ingredient text.
        
        Removes:
        - Text inside parentheses (e.g., "Spinach (frozen)" -> "Spinach")
        - Commercial symbols: ®, ™, ©
        - Brand artifacts: Inc., LLC, Ltd., Corp., etc.
        - Collapses multiple spaces to one
        
        Args:
            text: Raw ingredient text
            
        Returns:
            Cleaned text
        """
        if not text or not isinstance(text, str):
            return text if text else ""
        
        # Remove text inside parentheses
        cleaned = PARENTHESES_PATTERN.sub('', text)
        
        # Remove commercial symbols
        cleaned = COMMERCIAL_SYMBOLS_PATTERN.sub('', cleaned)
        
        # Remove brand artifacts
        cleaned = BRAND_ARTIFACTS_PATTERN.sub('', cleaned)
        
        # Collapse multiple spaces to one
        cleaned = MULTISPACE_PATTERN.sub(' ', cleaned)
        
        return cleaned.strip()

    def _basic_tokens(self, s: str) -> List[str]:
        return ALNUM.findall(s.lower())

    def _is_unit_or_number(self, t: str) -> bool:
        return t.isdigit() or t in UNITS

    def _normalize_doc(self, doc: Doc, raw: str) -> Optional[str]:
        """Core normalize logic, reusing an existing Doc."""
        # Apply bronze layer cleaning first
        raw = self.clean_raw_text(raw)
        s = raw.strip().lower()
        if not s:
            return None

        head: Optional[Token] = None
        for token in doc:
            if token.dep_ == "ROOT":
                head = token
                break
        if head is None:
            return None

        compounds = [t for t in head.lefts if t.dep_ in ("compound",)]
        parts: List[str] = []
        for t in compounds + [head]:
            tok = t.text.lower()
            if tok in DROP_ADJ:
                continue
            if self._is_unit_or_number(tok):
                continue
            if not ALNUM.fullmatch(tok):
                continue
            parts.append(tok)

        # Special cases like "powder", "sauce", etc.
        if len(parts) == 1 and parts[0] in {"powder","sauce","paste","oil","vinegar","cheese"}:
            for child in head.lefts:
                if child.dep_ in ("amod","compound") and ALNUM.fullmatch(child.text.lower()):
                    mod = child.text.lower()
                    if mod not in DROP_ADJ:
                        parts.insert(0, mod)
                        break

        if not parts:
            toks = [
                t for t in self._basic_tokens(s)
                if t not in DROP_ADJ and not self._is_unit_or_number(t)
            ]
            if not toks:
                return None
            parts = toks[-2:] if len(toks) >= 2 else toks

        return " ".join(parts).strip()

    def _normalize_phrase(self, s: str) -> Optional[str]:
        """Single-phrase wrapper; still used by normalize_list."""
        # Apply bronze layer cleaning first
        s = self.clean_raw_text(s)
        s = s.strip()
        if not s:
            return None
        doc = self.nlp(s.lower())
        return self._normalize_doc(doc, s)

    def normalize_list(self, lst: Union[List[str], np.ndarray, tuple]) -> List[str]:
        """Existing API: normalize a single list of ingredient strings."""
        if not isinstance(lst, (list, tuple, np.ndarray)):
            return []
        out: List[str] = []
        seen: Set[str] = set()
        for x in lst:
            norm = self._normalize_phrase(str(x))
            if norm and norm not in seen:
                out.append(norm)
                seen.add(norm)
        return out

    def normalize_batch(
        self,
        lists: Sequence[Union[List[str], np.ndarray, tuple]],
    ) -> List[List[str]]:
        """
        New batched API: normalize many ingredient lists at once using nlp.pipe.
        Returns a list of normalized lists, same length as `lists`.
        """
        # Flatten all phrases and remember boundaries
        flat_phrases: List[str] = []
        boundaries: List[tuple[int, int]] = []

        for lst in lists:
            if not isinstance(lst, (list, tuple, np.ndarray)):
                # mimic normalize_list: non-list -> empty output
                boundaries.append((len(flat_phrases), len(flat_phrases)))
                continue
            start = len(flat_phrases)
            for x in lst:
                # Apply bronze layer cleaning before processing
                cleaned = SpacyIngredientNormalizer.clean_raw_text(str(x))
                flat_phrases.append(cleaned)
            end = len(flat_phrases)
            boundaries.append((start, end))

        if not flat_phrases:
            # no work to do
            return [[] for _ in lists]

        # Run spaCy once over all phrases
        logger.info(f"[spacy_norm] normalize_batch: {len(flat_phrases)} phrases total (batch_size={self.batch_size}, n_process={self.n_process})")
        # Pre-lowercase all texts for better performance
        texts = [p.lower() for p in flat_phrases]
        
        # Process with spaCy pipe - use list() to materialize all docs at once
        # This is more efficient than iterating one-by-one
        # n_process: 0 or None = auto-detect, 1 = single-threaded, >1 = multiprocess
        n_proc = self.n_process if self.n_process > 0 else 1  # Default to 1 for safety
        docs = list(self.nlp.pipe(
            texts,
            batch_size=self.batch_size,
            n_process=n_proc,
        ))

        # Compute normalized forms for each phrase (parallelized if n_process > 1)
        flat_norms: List[Optional[str]] = []
        for raw, doc in zip(flat_phrases, docs):
            flat_norms.append(self._normalize_doc(doc, raw))

        # Rebuild per-list outputs with deduping
        out_lists: List[List[str]] = []
        for start, end in boundaries:
            seen: Set[str] = set()
            cur: List[str] = []
            for i in range(start, end):
                norm = flat_norms[i]
                if norm and norm not in seen:
                    cur.append(norm)
                    seen.add(norm)
            out_lists.append(cur)

        return out_lists


def apply_spacy_normalizer_to_parquet(
    in_parquet: Union[str, Path],
    out_parquet: Union[str, Path],
    list_col: str = "NER",
    out_col: str = "NER_clean",
    compression: str = "zstd",
    spacy_model: str = "en_core_web_sm",
    batch_size: int = 512,  # Increased batch size for better throughput
    n_process: int = 0,  # 0=auto-detect (uses all CPUs), 1=single-threaded
) -> Path:
    """
    Apply spaCy normalization to parquet file.
    
    Args:
        n_process: Number of processes (0=auto, 1=single-threaded, >1=multiprocess)
                  Note: Multiprocessing may not work on Windows due to spawn method.
    """
    normalizer = SpacyIngredientNormalizer(spacy_model, batch_size=batch_size, n_process=n_process)
    pf = pq.ParquetFile(str(in_parquet))
    out_parquet = Path(out_parquet)
    out_parquet.parent.mkdir(parents=True, exist_ok=True)

    writer = None
    schema = pa.schema([pa.field(out_col, pa.list_(pa.string()))])

    total_row_groups = pf.num_row_groups
    for rg in range(total_row_groups):
        t0 = time.time()
        logger.info(f"[spacy_norm] RG{rg+1}/{total_row_groups} start")

        # Only read the column we need
        tbl = pf.read_row_group(rg, columns=[list_col])
        df = tbl.to_pandas()
        n_rows = len(df)
        logger.info(f"[spacy_norm] RG{rg+1}/{total_row_groups}: {n_rows:,} rows")

        if list_col not in df.columns:
            df[list_col] = [[] for _ in range(n_rows)]

        # Ensure we pass list-like objects (or [] for bad types) into normalize_batch
        # Use list comprehension for better performance
        lists = [
            list(x) if isinstance(x, (list, tuple, np.ndarray)) else []
            for x in df[list_col]
        ]

        t1 = time.time()
        logger.info(f"[spacy_norm] RG{rg+1}/{total_row_groups}: running normalizer.normalize_batch…")
        cleaned_lists = normalizer.normalize_batch(lists)
        elapsed = time.time() - t1
        logger.info(f"[spacy_norm] RG{rg+1}/{total_row_groups}: normalizer step took {elapsed:.1f}s ({n_rows/elapsed:.0f} rows/sec)")

        df[out_col] = cleaned_lists

        table = pa.Table.from_pandas(df[[out_col]], preserve_index=False).replace_schema_metadata(None)
        if table.schema != schema:
            table = table.cast(schema, safe=False)

        if writer is None:
            writer = pq.ParquetWriter(str(out_parquet), schema, compression=compression)
        writer.write_table(table)

        logger.info(f"[spacy_norm] RG{rg}: total row group time {time.time() - t0:.1f}s")

    if writer:
        writer.close()
        
    return out_parquet

================================================================================
FILE: preprocess_pipeline\ingrnorm\w2v_dedupe.py
================================================================================


from __future__ import annotations
import json
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
from rapidfuzz.distance import Levenshtein

def _iter_parquet_tokenized_docs(parquet_path: Union[str, Path], list_col: str = "NER_clean") -> Iterable[List[str]]:
    pf = pq.ParquetFile(str(parquet_path))
    for rg in range(pf.num_row_groups):
        tbl = pf.read_row_group(rg, columns=[list_col])
        df = tbl.to_pandas()
        if list_col not in df.columns:
            continue
        for lst in df[list_col]:
            items: List[str] = []
            if isinstance(lst, (list, tuple, np.ndarray)):
                items = [str(x) for x in lst if str(x).strip()]
            elif isinstance(lst, str):
                s = lst.strip()
                if s and (s.startswith("[") and s.endswith("]")):
                    try:
                        parsed = json.loads(s)
                        if isinstance(parsed, list):
                            items = [str(x) for x in parsed if str(x).strip()]
                    except Exception:
                        pass
                if not items and s:
                    items = [s]
            if not items:
                continue
            tokens: List[str] = []
            for phrase in items:
                tokens.extend(simple_preprocess(str(phrase), deacc=True, min_len=2))
            if tokens:
                yield tokens

def train_or_load_w2v(
    corpus_parquet: Union[str, Path],
    list_col: str = "NER_clean",
    model_cache_path: Optional[Union[str, Path]] = None,
    vector_size: int = 100, window: int = 5, min_count: int = 1, workers: int = 4, sg: int = 1, epochs: int = 8,
) -> Word2Vec:
    """Train or load Word2Vec model. Uses streaming iterator to avoid loading all sentences into memory."""
    model_cache_path = Path(model_cache_path) if model_cache_path else None
    if model_cache_path and model_cache_path.exists():
        return Word2Vec.load(str(model_cache_path))
    
    # Use streaming iterator instead of loading all into memory
    sentences_iter = _iter_parquet_tokenized_docs(corpus_parquet, list_col=list_col)
    model = Word2Vec(
        sentences=sentences_iter,  # Word2Vec accepts iterables directly
        vector_size=vector_size, 
        window=window, 
        min_count=min_count, 
        workers=workers, 
        sg=sg, 
        epochs=epochs,
    )
    if model_cache_path:
        model_cache_path.parent.mkdir(parents=True, exist_ok=True)
        model.save(str(model_cache_path))
    return model

def _tokenize_phrase(phrase: str) -> List[str]:
    return simple_preprocess(str(phrase), deacc=True, min_len=2)

# Stop words to exclude from token overlap check
STOP_WORDS = {"salt", "water", "oil", "sugar", "pepper", "flour", "butter", "garlic", "onion"}

def _tokenize_excluding_stops(phrase: str) -> set:
    """Tokenize phrase and return set of tokens excluding stop words."""
    tokens = _tokenize_phrase(phrase)
    return {t for t in tokens if t not in STOP_WORDS}

def _has_token_overlap(phrase_a: str, phrase_b: str) -> bool:
    """Check if two phrases share at least one non-stop-word token."""
    tokens_a = _tokenize_excluding_stops(phrase_a)
    tokens_b = _tokenize_excluding_stops(phrase_b)
    return bool(tokens_a & tokens_b)

def _levenshtein_ratio(phrase_a: str, phrase_b: str) -> float:
    """Compute normalized Levenshtein similarity ratio (0-1, higher = more similar)."""
    if not phrase_a or not phrase_b:
        return 0.0
    # Use rapidfuzz for normalized similarity (0-1 scale)
    distance = Levenshtein.distance(phrase_a.lower(), phrase_b.lower())
    max_len = max(len(phrase_a), len(phrase_b))
    if max_len == 0:
        return 1.0
    return 1.0 - (distance / max_len)

def _should_merge(phrase_a: str, phrase_b: str, levenshtein_threshold: float = 0.7) -> bool:
    """
    Determine if two phrases should be merged based on lexical guardrails.
    
    Merges if:
    - They share at least one non-stop-word token, OR
    - Their Levenshtein similarity is above threshold
    
    Args:
        phrase_a: First phrase
        phrase_b: Second phrase
        levenshtein_threshold: Minimum Levenshtein similarity ratio (0-1)
        
    Returns:
        True if phrases should be merged
    """
    # Check token overlap
    if _has_token_overlap(phrase_a, phrase_b):
        return True
    
    # Check Levenshtein distance
    lev_ratio = _levenshtein_ratio(phrase_a, phrase_b)
    if lev_ratio >= levenshtein_threshold:
        return True
    
    return False

def phrase_vector(model: Word2Vec, phrase: str, oov_policy: str = "skip") -> Optional[np.ndarray]:
    toks = _tokenize_phrase(phrase)
    vecs = [model.wv[t] for t in toks if t in model.wv]
    if not vecs:
        return None if oov_policy != "zeros" else np.zeros(model.vector_size, dtype=np.float32)
    v = np.mean(np.stack(vecs, 0), 0)
    n = np.linalg.norm(v) + 1e-12
    return (v / n).astype(np.float32)

def _topk_similar_indices(M: np.ndarray, k: int = 25, self_exclude: bool = True, chunk_size: int = 2048) -> Tuple[np.ndarray, np.ndarray]:
    n, d = M.shape
    k = min(k, n - 1 if self_exclude else n)
    topk_idx = np.empty((n, k), dtype=np.int32)
    topk_sc  = np.empty((n, k), dtype=np.float32)
    for start in range(0, n, chunk_size):
        end = min(start + chunk_size, n)
        block = M[start:end]
        sims = block @ M.T
        if self_exclude:
            rows = np.arange(start, end)
            sims[np.arange(end - start), rows] = -np.inf
        part_idx = np.argpartition(-sims, kth=k - 1, axis=1)[:, :k]
        part_scores = np.take_along_axis(sims, part_idx, axis=1)
        order = np.argsort(-part_scores, axis=1)
        block_topk_idx = np.take_along_axis(part_idx, order, axis=1).astype(np.int32)
        block_topk_sc  = np.take_along_axis(part_scores, order, axis=1).astype(np.float32)
        topk_idx[start:end] = block_topk_idx
        topk_sc[start:end]  = block_topk_sc
    return topk_idx, topk_sc

def _uf_build(n: int): return list(range(n))
def _uf_find(p, a): 
    while p[a] != a:
        p[a] = p[p[a]]; a = p[a]
    return a
def _uf_union(p, a, b):
    ra, rb = _uf_find(p, a), _uf_find(p, b)
    if ra != rb: p[rb] = ra

def w2v_dedupe(
    vocab_counter: Dict[str, int],
    *, corpus_parquet: Union[str, Path], list_col: str = "NER_clean",
    model_cache_path: Optional[Union[str, Path]] = None,
    vector_size: int = 100, window: int = 5, min_count: int = 1, workers: int = 4, sg: int = 1, epochs: int = 8,
    threshold: float = 0.85, topk: int = 25, out_path: Union[str, Path] = Path("../data/w2v_dedupe_map.jsonl"),
) -> Dict[str, str]:
    phrases = [p for p, c in vocab_counter.items() if c > 0]
    if not phrases:
        raise ValueError("w2v_dedupe: empty vocabulary.")
    try:
        model = train_or_load_w2v(corpus_parquet=corpus_parquet, list_col=list_col, model_cache_path=model_cache_path,
                                  vector_size=vector_size, window=window, min_count=min_count, workers=workers, sg=sg, epochs=epochs)
    except ValueError:
        from gensim.utils import simple_preprocess
        sentences = [simple_preprocess(p, deacc=True, min_len=2) for p in phrases]
        sentences = [s for s in sentences if s]
        model = Word2Vec(sentences=sentences, vector_size=vector_size, window=window, min_count=1, workers=workers, sg=sg, epochs=max(epochs, 5))

    vecs, kept = [], []
    for ph in phrases:
        v = phrase_vector(model, ph, oov_policy="skip")
        if v is None: continue
        kept.append(ph); vecs.append(v)
    if not vecs:
        raise ValueError("w2v_dedupe: no phrases had in-vocab tokens.")

    M = np.stack(vecs, 0).astype(np.float32)
    nbr_idx, nbr_sc = _topk_similar_indices(M, k=topk, self_exclude=True, chunk_size=2048)
    
    # Centroid-based clustering: avoid union-find chaining
    # Sort phrases by frequency (descending) to process most common first
    freqs = vocab_counter
    phrase_indices = list(range(len(kept)))
    phrase_indices.sort(key=lambda i: (-freqs.get(kept[i], 0), len(kept[i]), kept[i]))
    
    clusters: Dict[int, List[int]] = {}  # cluster_id -> list of phrase indices
    cluster_centroids: Dict[int, int] = {}  # cluster_id -> centroid phrase index
    cluster_vectors: Dict[int, np.ndarray] = {}  # cluster_id -> centroid vector
    next_cluster_id = 0
    
    # Levenshtein threshold for lexical guardrails (0.7 = 70% similarity)
    levenshtein_threshold = 0.7
    # Verification threshold: members must be within this distance of centroid
    centroid_verification_threshold = threshold * 0.95  # Slightly stricter than merge threshold
    
    for i in phrase_indices:
        assigned = False
        phrase_i = kept[i]
        vec_i = M[i]
        
        # Check if phrase_i can join an existing cluster
        for cluster_id, centroid_idx in cluster_centroids.items():
            centroid_vec = cluster_vectors[cluster_id]
            centroid_phrase = kept[centroid_idx]
            
            # Check similarity to centroid
            cos_sim = float(np.dot(vec_i, centroid_vec))
            if cos_sim < threshold:
                continue
            
            # Lexical guardrail: check token overlap or Levenshtein distance
            if not _should_merge(phrase_i, centroid_phrase, levenshtein_threshold):
                continue
            
            # Verify member is actually close to centroid (not just a neighbor)
            if cos_sim >= centroid_verification_threshold:
                clusters[cluster_id].append(i)
                assigned = True
                break
        
        # If not assigned, create new cluster with this phrase as centroid
        if not assigned:
            cluster_id = next_cluster_id
            next_cluster_id += 1
            clusters[cluster_id] = [i]
            cluster_centroids[cluster_id] = i
            cluster_vectors[cluster_id] = vec_i
    
    # Build mapping: all members map to their cluster centroid
    mapping = {}
    for cluster_id, members in clusters.items():
        centroid_idx = cluster_centroids[cluster_id]
        canon = kept[centroid_idx]
        for i in members:
            mapping[kept[i]] = canon
    
    # Ensure all phrases have a mapping (even if not clustered)
    for ph in phrases:
        mapping.setdefault(ph, ph)

    out_path = Path(out_path); out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        for frm, to in mapping.items():
            f.write(json.dumps({"from": frm, "to": to}) + "\n")
    return mapping


================================================================================
FILE: preprocess_pipeline\pipeline\common\logging_setup.py
================================================================================

from __future__ import annotations
import logging
import logging.handlers
from pathlib import Path
from typing import Optional, Dict, Any

def setup_logging(cfg: Optional[Dict[str, Any]] = None, *, force: bool = False) -> None:
    """Initialize root logging once. Safe to call multiple times if force=False."""
    if logging.getLogger().handlers and not force:
        return

    cfg = cfg or {}
    log_cfg = (cfg.get("logging") or {})

    level_name = str(log_cfg.get("level", "INFO")).upper()
    level = getattr(logging, level_name, logging.INFO)

    fmt = log_cfg.get("fmt", "[%(asctime)s] %(levelname)s %(name)s: %(message)s")
    datefmt = log_cfg.get("datefmt", "%Y-%m-%d %H:%M:%S")

    handlers = []

    # Console
    if log_cfg.get("console", True):
        ch = logging.StreamHandler()
        ch.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))
        handlers.append(ch)

    # Rotating file
    file_path = log_cfg.get("file")
    if file_path:
        fp = Path(file_path)
        fp.parent.mkdir(parents=True, exist_ok=True)
        rotate_cfg = log_cfg.get("rotate", {}) or {}
        max_bytes = int(rotate_cfg.get("max_bytes", 10_485_760))  # 10MB
        backup_count = int(rotate_cfg.get("backup_count", 5))
        fh = logging.handlers.RotatingFileHandler(
            filename=str(fp),
            maxBytes=max_bytes,
            backupCount=backup_count,
            encoding="utf-8"
        )
        fh.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))
        handlers.append(fh)

    logging.basicConfig(level=level, handlers=handlers)

    # Quiet noisy deps
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("botocore").setLevel(logging.WARNING)
    logging.getLogger("s3transfer").setLevel(logging.WARNING)

================================================================================
FILE: preprocess_pipeline\pipeline\config\config.yaml
================================================================================
data:
  # Canonical raw data input (used by both ingrnorm + NER unless overridden)
  input_path: "./data/raw/wilmerarltstrmberg_data.csv" # main dataset
  # input_path: "./data/raw/sample_data.csv" # for quick local testing
  ner_col: "NER"           # list-like column if present
  chunksize: 200000

cleanup:
  enabled: true
  paths:
    - "./data/normalized/recipes_data_clean.parquet"
    - "./data/normalized/cosine_dedupe_map.jsonl"
    - "./data/normalized/recipes_data_clean_spell_dedup.parquet"
    - "./data/encoded/datasets_unified.parquet"
    - "./data/encoded/ingredient_id_to_token.json"
    - "./data/encoded/ingredient_token_to_id.json"

output:
  # ingrnorm artifacts
  baseline_parquet: "./data/normalized/recipes_data_clean.parquet"
  dedup_parquet: "./data/normalized/recipes_data_clean_spell_dedup.parquet"
  cosine_map_path: "./data/normalized/cosine_dedupe_map.jsonl"
  list_col_for_vocab: "NER_clean"

  unified_parquet: "./data/encoded/datasets_unified.parquet"
  ingredient_id_to_token: "./data/encoded/ingredient_id_to_token.json"
  ingredient_token_to_id: "./data/encoded/ingredient_token_to_id.json"

  # NER prediction outputs (single source of truth)
  ner_preds_base: "./data/training/predictions.parquet"   # we can derive *_wide / *_tall from this

stages:
  write_parquet: true
  sbert_dedupe: true
  w2v_dedupe: false
  apply_cosine_map: true
  encode_ids: true

sbert:
  model: "all-MiniLM-L6-v2"
  threshold: 0.88
  topk: 25
  min_len: 2
  require_token_overlap: true
  block_generic_as_canon: true
  min_freq_for_vocab: 2
  spacy_model: "en_core_web_sm"
  # Performance tuning for spaCy normalization
  spacy_batch_size: 1024  # Larger batches = better throughput (try 512-1024 for large datasets)
  spacy_n_process: 4     # 1=single-threaded (safe, works on Windows)
                         # >1=multiprocess (faster but may not work on Windows due to spawn method)
                         # If CPU usage is low, try increasing spacy_batch_size first

w2v:
  vector_size: 100
  window: 5
  min_count: 1
  workers: 4
  sg: 1
  epochs: 8
  threshold: 0.85
  topk: 25
  min_freq_for_vocab: 2

encoder:
  min_freq: 1
  dataset_id: 1
  ingredients_col: "NER_clean"

ner:
  enabled: true

  train_path: "./data/normalized/recipes_data_clean.parquet"
  data_is_parquet: true
  max_rows: null  # Set to a number (e.g., 10000) for debug mode

  # IMPORTANT: Use original NER column (messy) for training, not NER_clean (normalized)
  # The model should learn to handle messy input, then normalization happens via dedupe map
  text_col: null
  ner_list_col: "NER"  # Original messy column - model learns to recognize these
  lexicon_json: null

  # Debug mode options (for faster iteration during development)
  use_tok2vec_debug: false  # If true, use tok2vec instead of transformers (faster, CPU-friendly)
  max_train_docs: 5000      # Limit training docs for quick experiments (null = use all)

  random_seed: 42
  valid_fraction: 0.2
  shard_size: 2000
  n_epochs: 20
  lr: 5e-5
  dropout: 0.1
  transformer_model: "distilbert-base-uncased"
  window: 64
  stride: 48
  freeze_layers: 2
  use_amp: true
  early_stopping_patience: 3
  batch_size: 256
  data_loader_workers: 4  # Number of worker processes for DataLoader (0 = single-threaded)

  out_dir: "./models/ingredient_ner_trf"
  model_dir: "./models/ingredient_ner_trf/model-best"
  # NOTE: dedupe map & encoder maps are **not duplicated here**;
  # ingredient_ner.py will read them from `output.cosine_map_path` & `output.ingredient_*`

logging:
  level: INFO
  console: true
  file: pipeline/logs/ingrnorm.log
  rotate:
    max_bytes: 10485760
    backup_count: 5
  fmt: "[%(asctime)s] %(levelname)s %(name)s: %(message)s"
  datefmt: "%Y-%m-%d %H:%M:%S"


================================================================================
FILE: preprocess_pipeline\pipeline\config\ingredient_ner.yaml
================================================================================
# Ingredient NER Training Configuration
# This config is used by run_ingredient_ner.py for training the NER model

# Optional: fallback input path (if train_path is not set)
data:
  input_path: "./data/raw/wilmerarltstrmberg_data.csv"  # Fallback if ner.train_path is not set
  ner_col: "NER_clean"  # Used for fallback column name

# Output paths for artifacts created by ingrnorm pipeline
# These are READ by the NER pipeline (dedupe map, encoder maps)
output:
  cosine_map_path: "./data/normalized/cosine_dedupe_map.jsonl"
  ingredient_id_to_token: "./data/encoded/ingredient_id_to_token.json"
  ingredient_token_to_id: "./data/encoded/ingredient_token_to_id.json"
  ner_preds_base: "./data/training/predictions.parquet"   # NER prediction outputs

ner:
  enabled: true

  train_path: "./data/normalized/recipes_data_clean.parquet"
  data_is_parquet: true
  max_rows: null # set to null for all rows - otherwise set to a number of rows to load
  # Use the column that exists in the parquet
  text_col: null
  ner_list_col: "NER_clean"   # <- CHANGE THIS FROM "NER" TO "NER_clean"
  lexicon_json: null

  random_seed: 42
  valid_fraction: 0.2
  shard_size: 2000
  n_epochs: 20
  lr: 5e-5
  dropout: 0.1
  transformer_model: "distilbert-base-uncased"
  window: 64
  stride: 48
  freeze_layers: 2
  use_amp: true
  early_stopping_patience: 3
  batch_size: 1024
  data_loader_workers: 4  # Number of worker processes for DataLoader (0 = single-threaded)

  out_dir: "./models/ingredient_ner_trf"
  model_dir: "./models/ingredient_ner_trf/model-best"


logging:
  level: INFO
  console: true
  file: pipeline/logs/ingredient_ner.log
  rotate:
    max_bytes: 10485760
    backup_count: 5
  fmt: "[%(asctime)s] %(levelname)s %(name)s: %(message)s"
  datefmt: "%Y-%m-%d %H:%M:%S"



================================================================================
FILE: preprocess_pipeline\pipeline\config\ingredient_ner_inference.yaml
================================================================================
# Ingredient NER Inference Configuration
# This config is used by apply_ingredient_ner.py for running inference on new datasets

# Model path (trained NER model)
model:
  model_dir: "./models/ingredient_ner_trf/model-best"  # Path to trained spaCy model

# Input data settings
data:
  # Default input path (can be overridden via --in-path CLI argument)
  input_path: null  # Set to a path if you want a default, or leave null to require --in-path
  data_is_parquet: true  # Auto-detected from file extension if not set

# Output settings
output:
  # Base path for output files (can be overridden via --out-base CLI argument)
  # Will write <base>_wide.parquet and <base>_tall.parquet
  out_base: "./data/inference_output"

# Paths to artifacts created by ingrnorm pipeline
# These are used for normalization and encoding
artifacts:
  # Dedupe map: maps variant phrases → canonical forms
  cosine_map_path: "./data/normalized/cosine_dedupe_map.jsonl"
  # Encoder maps: maps canonical tokens ↔ integer IDs
  ingredient_id_to_token: "./data/encoded/ingredient_id_to_token.json"
  ingredient_token_to_id: "./data/encoded/ingredient_token_to_id.json"

# Inference settings
inference:
  # Default text column name (can be overridden via --text-col CLI argument)
  text_col: null  # Must be specified via CLI or set here
  
  # Performance settings
  batch_size: 256  # Batch size for spaCy processing
  n_process: 1     # Number of processes (keep 1 for GPU/transformers, >1 may not work on Windows)
  use_gpu: false   # Attempt to use GPU (default: CPU for reliability)
  
  # Sampling defaults (can be overridden via CLI)
  sample_n: null      # Randomly sample N rows
  sample_frac: null   # Randomly sample fraction (0.0-1.0)
  head_n: null        # Take first N rows

logging:
  level: INFO
  console: true
  file: pipeline/logs/ingredient_ner_inference.log
  rotate:
    max_bytes: 10485760
    backup_count: 5
  fmt: "[%(asctime)s] %(levelname)s %(name)s: %(message)s"
  datefmt: "%Y-%m-%d %H:%M:%S"



================================================================================
FILE: preprocess_pipeline\pipeline\config\ingrnorm.yaml
================================================================================
data:
  # Canonical raw data input
  input_path: "./data/raw/wilmerarltstrmberg_data.csv" # main dataset
  # input_path: "./data/raw/sample_data.csv" # for quick local testing
  ner_col: "NER"           # list-like column if present
  chunksize: 200000

cleanup:
  enabled: true
  paths:
    - "./data/normalized/recipes_data_clean.parquet"
    - "./data/normalized/cosine_dedupe_map.jsonl"
    - "./data/normalized/recipes_data_clean_spell_dedup.parquet"
    - "./data/encoded/datasets_unified.parquet"
    - "./data/encoded/ingredient_id_to_token.json"
    - "./data/encoded/ingredient_token_to_id.json"

output:
  # ingrnorm artifacts
  baseline_parquet: "./data/normalized/recipes_data_clean.parquet"
  dedup_parquet: "./data/normalized/recipes_data_clean_spell_dedup.parquet"
  cosine_map_path: "./data/normalized/cosine_dedupe_map.jsonl"
  list_col_for_vocab: "NER_clean"

  unified_parquet: "./data/encoded/datasets_unified.parquet"
  ingredient_id_to_token: "./data/encoded/ingredient_id_to_token.json"
  ingredient_token_to_id: "./data/encoded/ingredient_token_to_id.json"

stages:
  write_parquet: true
  sbert_dedupe: true
  w2v_dedupe: false
  apply_cosine_map: true
  encode_ids: true

sbert:
  model: "all-MiniLM-L6-v2"
  threshold: 0.88
  topk: 25
  min_len: 2
  require_token_overlap: true
  block_generic_as_canon: true
  min_freq_for_vocab: 2
  spacy_model: "en_core_web_sm"
  # Performance tuning for spaCy normalization
  spacy_batch_size: 1024  # Larger batches = better throughput (try 512-1024 for large datasets)
  spacy_n_process: 4     # 1=single-threaded (safe, works on Windows)
                         # >1=multiprocess (faster but may not work on Windows due to spawn method)
                         # If CPU usage is low, try increasing spacy_batch_size first

w2v:
  vector_size: 100
  window: 5
  min_count: 1
  workers: 4
  sg: 1
  epochs: 8
  threshold: 0.85
  topk: 25
  min_freq_for_vocab: 2

encoder:
  min_freq: 1
  dataset_id: 1
  ingredients_col: "NER_clean"

logging:
  level: INFO
  console: true
  file: pipeline/logs/ingrnorm.log
  rotate:
    max_bytes: 10485760
    backup_count: 5
  fmt: "[%(asctime)s] %(levelname)s %(name)s: %(message)s"
  datefmt: "%Y-%m-%d %H:%M:%S"



================================================================================
FILE: preprocess_pipeline\pipeline\ingredient_ner\config.py
================================================================================
# pipeline/ingredient_ner/config.py
from __future__ import annotations

from pathlib import Path
from types import SimpleNamespace
from typing import Any, Dict, Tuple

import yaml

# Public globals other modules import
DATA = SimpleNamespace()
TRAIN = SimpleNamespace()
OUT = SimpleNamespace()


def _to_path(v: Any) -> Path | None:
    if v is None:
        return None
    return Path(str(v))


def load_full_yaml(path: str | Path) -> Dict[str, Any]:
    """Load the full pipeline YAML."""
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"Config not found: {path}")
    with open(path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if not isinstance(cfg, dict):
        raise ValueError(f"YAML at {path} did not parse to a dict.")
    return cfg


def _build_data_ns(cfg: Dict[str, Any]) -> SimpleNamespace:
    data_cfg = cfg.get("data") or {}
    ner_cfg = cfg.get("ner") or {}
    out_cfg = cfg.get("output") or {}

    # 1) training file path
    # Prefer ner.train_path, fallback to data.input_path, then default
    train_path_str = ner_cfg.get("train_path")
    if not train_path_str:
        train_path_str = data_cfg.get("input_path")
    if not train_path_str:
        train_path_str = "./data/raw/wilmerarltstrmberg_data.csv"
    train_path = Path(train_path_str)

    # 2) parquet vs csv
    if "data_is_parquet" in ner_cfg:
        data_is_parquet = bool(ner_cfg["data_is_parquet"])
    else:
        data_is_parquet = train_path.suffix.lower() == ".parquet"

    # 3) max rows
    max_rows_raw = ner_cfg.get("max_rows")
    max_rows = int(max_rows_raw) if max_rows_raw is not None else None

    # 4) NER columns
    ner_list_col = ner_cfg.get("ner_list_col") or data_cfg.get("ner_col") or None
    text_col = ner_cfg.get("text_col") or None
    lexicon_json = _to_path(ner_cfg.get("lexicon_json")) if ner_cfg.get("lexicon_json") else None

    # 5) other artifacts
    dedupe_jsonl = _to_path(out_cfg.get("cosine_map_path"))
    ing_id2tok = _to_path(out_cfg.get("ingredient_id_to_token"))
    ing_tok2id = _to_path(out_cfg.get("ingredient_token_to_id"))

    return SimpleNamespace(
        DATA_IS_PARQUET=data_is_parquet,
        TRAIN_PATH=train_path,
        MAX_ROWS=max_rows,
        NER_LIST_COL=ner_list_col,
        TEXT_COL=text_col,
        LEXICON_JSON=lexicon_json,
        DEDUPE_JSONL=dedupe_jsonl,
        ING_ID2TOK_JSON=ing_id2tok,
        ING_TOK2ID_JSON=ing_tok2id,
    )


def _build_train_ns(cfg: Dict[str, Any]) -> SimpleNamespace:
    """Build TRAIN namespace from ner config. Uses UPPERCASE keys to match existing code."""
    ner_cfg = cfg.get("ner") or {}

    # Debug mode: if use_tok2vec_debug is True, we'll use tok2vec instead of transformers
    use_tok2vec_debug = bool(ner_cfg.get("use_tok2vec_debug", False))
    
    # If debug mode, reduce epochs for faster iteration
    n_epochs = int(ner_cfg.get("n_epochs", 20))
    if use_tok2vec_debug:
        n_epochs = min(n_epochs, 5)  # Cap at 5 epochs in debug mode

    return SimpleNamespace(
        RANDOM_SEED=int(ner_cfg.get("random_seed", 42)),
        VALID_FRACTION=float(ner_cfg.get("valid_fraction", 0.2)),
        SHARD_SIZE=int(ner_cfg.get("shard_size", 2000)),
        BATCH_SIZE=int(ner_cfg.get("batch_size", 256)),
        TRANSFORMER_MODEL=str(ner_cfg.get("transformer_model", "distilbert-base-uncased")),
        WINDOW=int(ner_cfg.get("window", 64)),
        STRIDE=int(ner_cfg.get("stride", 48)),
        LR=float(ner_cfg.get("lr", 5e-5)),
        DROPOUT=float(ner_cfg.get("dropout", 0.1)),
        N_EPOCHS=n_epochs,
        FREEZE_LAYERS=int(ner_cfg.get("freeze_layers", 2)),
        USE_AMP=bool(ner_cfg.get("use_amp", True)),
        EARLY_STOPPING_PATIENCE=int(ner_cfg.get("early_stopping_patience", 3)),
        EVAL_SNAPSHOT_MAX=int(ner_cfg.get("eval_snapshot_max", 1500)),
        CLEAR_CACHE_EVERY=int(ner_cfg.get("clear_cache_every", 200)),
        USE_TOK2VEC_DEBUG=use_tok2vec_debug,
        MAX_TRAIN_DOCS=int(ner_cfg.get("max_train_docs")) if ner_cfg.get("max_train_docs") is not None else None,
    )


def _build_out_ns(cfg: Dict[str, Any]) -> SimpleNamespace:
    ner_cfg = cfg.get("ner") or {}
    out_cfg = cfg.get("output") or {}

    out_dir = _to_path(ner_cfg.get("out_dir")) or Path("./models/ingredient_ner_trf")
    model_dir = _to_path(ner_cfg.get("model_dir")) or (out_dir / "model-best")

    boot_dir = out_dir / "bootstrapped"
    train_dir = boot_dir / "train"
    valid_dir = boot_dir / "valid"

    pred_out = _to_path(out_cfg.get("ner_preds_base")) or Path("./data/training/predictions.parquet")

    return SimpleNamespace(
        OUT_DIR=out_dir,
        MODEL_DIR=model_dir,
        BOOT_DIR=boot_dir,
        TRAIN_DIR=train_dir,
        VALID_DIR=valid_dir,
        PRED_OUT=pred_out,
    )


def _update_ns(target: SimpleNamespace, src: SimpleNamespace) -> None:
    """Mutate an existing SimpleNamespace to have src's attributes."""
    target.__dict__.clear()
    target.__dict__.update(src.__dict__)


# ---------------------- public API ---------------------- #

def load_configs_from_dict(cfg: Dict[str, Any]) -> Tuple[SimpleNamespace, SimpleNamespace, SimpleNamespace]:
    """
    Populate DATA / TRAIN / OUT from an in-memory YAML dict.

    IMPORTANT: we mutate the existing SimpleNamespace objects so
    any 'from ingredient_ner.config import TRAIN' references see updates.
    """
    global DATA, TRAIN, OUT
    _update_ns(DATA, _build_data_ns(cfg))
    _update_ns(TRAIN, _build_train_ns(cfg))
    _update_ns(OUT, _build_out_ns(cfg))
    return DATA, TRAIN, OUT


def load_configs_from_yaml(path: str | Path) -> Tuple[SimpleNamespace, SimpleNamespace, SimpleNamespace]:
    """Convenience that reads YAML then delegates to load_configs_from_dict."""
    cfg = load_full_yaml(path)
    return load_configs_from_dict(cfg)


def print_configs() -> None:
    from pprint import pprint
    print("DATA:")
    pprint(vars(DATA))
    print("\nTRAIN:")
    pprint(vars(TRAIN))
    print("\nOUT:")
    pprint(vars(OUT))


# ---------------------- Inference-specific config ---------------------- #

def _build_inference_data_ns(cfg: Dict[str, Any]) -> SimpleNamespace:
    """Build DATA namespace for inference from inference config YAML."""
    data_cfg = cfg.get("data") or {}
    artifacts_cfg = cfg.get("artifacts") or {}
    
    input_path_str = data_cfg.get("input_path")
    input_path = _to_path(input_path_str) if input_path_str else None
    
    # Determine if parquet or CSV
    if "data_is_parquet" in data_cfg:
        data_is_parquet = bool(data_cfg["data_is_parquet"])
    elif input_path:
        data_is_parquet = input_path.suffix.lower() == ".parquet"
    else:
        data_is_parquet = True  # Default to parquet
    
    return SimpleNamespace(
        DATA_IS_PARQUET=data_is_parquet,
        TRAIN_PATH=input_path,  # Reusing TRAIN_PATH name for consistency
        MAX_ROWS=None,
        NER_LIST_COL=None,
        TEXT_COL=None,
        LEXICON_JSON=None,
        DEDUPE_JSONL=_to_path(artifacts_cfg.get("cosine_map_path")),
        ING_ID2TOK_JSON=_to_path(artifacts_cfg.get("ingredient_id_to_token")),
        ING_TOK2ID_JSON=_to_path(artifacts_cfg.get("ingredient_token_to_id")),
    )


def _build_inference_out_ns(cfg: Dict[str, Any]) -> SimpleNamespace:
    """Build OUT namespace for inference from inference config YAML."""
    model_cfg = cfg.get("model") or {}
    out_cfg = cfg.get("output") or {}
    
    model_dir = _to_path(model_cfg.get("model_dir")) or Path("./models/ingredient_ner_trf/model-best")
    out_base = _to_path(out_cfg.get("out_base")) or Path("./data/inference_output")
    
    return SimpleNamespace(
        OUT_DIR=model_dir.parent,  # For consistency
        MODEL_DIR=model_dir,
        BOOT_DIR=None,
        TRAIN_DIR=None,
        VALID_DIR=None,
        PRED_OUT=out_base,
    )


def load_inference_configs_from_dict(cfg: Dict[str, Any]) -> Tuple[SimpleNamespace, SimpleNamespace]:
    """
    Populate DATA / OUT from an inference config dict.
    
    Returns only DATA and OUT (no TRAIN needed for inference).
    """
    global DATA, OUT
    _update_ns(DATA, _build_inference_data_ns(cfg))
    _update_ns(OUT, _build_inference_out_ns(cfg))
    return DATA, OUT


def load_inference_configs_from_yaml(path: str | Path) -> Tuple[SimpleNamespace, SimpleNamespace]:
    """Convenience that reads inference YAML then delegates to load_inference_configs_from_dict."""
    cfg = load_full_yaml(path)
    return load_inference_configs_from_dict(cfg)

================================================================================
FILE: preprocess_pipeline\pipeline\ingredient_ner\data_prep.py
================================================================================
import json
import math
import warnings
from pathlib import Path
from typing import List, Tuple

import pandas as pd
import spacy
from sklearn.model_selection import train_test_split
from spacy.tokens import Doc, DocBin
from tqdm import tqdm

from .config import DATA, TRAIN, OUT
from .utils import (
    load_data,
    parse_listlike,
    join_with_offsets,
    normalize_token,
)


def docs_from_list_column(df: pd.DataFrame, col: str) -> List[Doc]:
    """Create spaCy Docs from a list-like ingredient column."""
    blank = spacy.blank("en")
    out: List[Doc] = []
    for lst in tqdm(df[col].tolist(), desc="Synthesizing from list column"):
        toks = parse_listlike(lst)
        if not toks:
            out.append(blank.make_doc(""))
            continue
        text, offs = join_with_offsets(toks)
        d = blank.make_doc(text)
        ents = []
        for (a, b) in offs:
            sp = d.char_span(a, b, label="INGREDIENT", alignment_mode="contract")
            if sp is not None:
                ents.append(sp)
        d.ents = spacy.util.filter_spans(ents)
        out.append(d)
    return out


def load_lexicon(path: Path | None) -> list[str]:
    if path is None:
        return []
    p = Path(path)
    if not p.exists():
        warnings.warn(f"Lexicon not found at {p}. Skipping.")
        return []
    with open(p, "r", encoding="utf-8") as f:
        data = json.load(f)
    # Expect either {"terms": [...]} or a simple list [...]
    if isinstance(data, dict) and "terms" in data:
        terms = data["terms"]
    else:
        terms = data
    # normalize
    terms = [normalize_token(t) for t in terms if str(t).strip()]
    terms = sorted(set(terms))
    print(f"Loaded {len(terms):,} lexicon terms.")
    return terms


def build_entity_ruler(nlp: spacy.language.Language, phrases: list[str]):
    ruler = nlp.add_pipe("entity_ruler")
    patterns = [{"label": "INGREDIENT", "pattern": t} for t in phrases]
    ruler.add_patterns(patterns)
    return ruler


def docs_from_text_plus_lexicon(df: pd.DataFrame, text_col: str, lexicon_terms: list[str]) -> List[Doc]:
    """Bootstrap labels from raw text using an EntityRuler over a lexicon."""
    nlp = spacy.blank("en")
    if not lexicon_terms:
        raise ValueError("No lexicon terms provided; cannot bootstrap from raw text.")
    build_entity_ruler(nlp, lexicon_terms)
    out: List[Doc] = []
    for text in tqdm(df[text_col].astype(str).tolist(), desc="Bootstrapping with EntityRuler"):
        d = nlp.make_doc(text)
        d = nlp(d)  # apply ruler
        # Keep only INGREDIENT, deduplicate spans
        d.ents = spacy.util.filter_spans([e for e in d.ents if e.label_ == "INGREDIENT"])
        out.append(d)
    return out


def build_docs_from_config() -> Tuple[list[Doc], list[Doc], str]:
    """High-level helper: build train & valid docs based on DATA + TRAIN config."""
    # Debug info
    print(f"[DEBUG] ===== Data Loading Configuration =====")
    print(f"[DEBUG] TRAIN_PATH: {DATA.TRAIN_PATH}")
    print(f"[DEBUG] Absolute path: {DATA.TRAIN_PATH.resolve()}")
    print(f"[DEBUG] File exists: {DATA.TRAIN_PATH.exists()}")
    print(f"[DEBUG] Data format: {'Parquet' if DATA.DATA_IS_PARQUET else 'CSV'}")
    print(f"[DEBUG] NER_LIST_COL: {DATA.NER_LIST_COL}")
    print(f"[DEBUG] TEXT_COL: {DATA.TEXT_COL}")
    print(f"[DEBUG] LEXICON_JSON: {DATA.LEXICON_JSON}")
    print(f"[DEBUG] ======================================")

    # Decide source mode
    if DATA.DATA_IS_PARQUET:
        df_sample = pd.read_parquet(DATA.TRAIN_PATH).head(1)
    else:
        df_sample = pd.read_csv(DATA.TRAIN_PATH, nrows=0, dtype=str)  # Read header only

    print(f"[DEBUG] Sample columns: {list(df_sample.columns)}")

    if DATA.NER_LIST_COL and DATA.NER_LIST_COL in df_sample.columns:
        print(f"[DEBUG] Using list-column mode on '{DATA.NER_LIST_COL}'")
        print(f"[DEBUG] This teaches the model to handle real-world messy input.")
        df_list = load_data(DATA.TRAIN_PATH, DATA.DATA_IS_PARQUET, DATA.NER_LIST_COL, max_rows=DATA.MAX_ROWS)
        docs_all = docs_from_list_column(df_list, DATA.NER_LIST_COL)
        source_mode = "list-column"
        
        # Apply max_train_docs limit if set (for debug mode)
        if hasattr(TRAIN, 'MAX_TRAIN_DOCS') and TRAIN.MAX_TRAIN_DOCS is not None:
            if len(docs_all) > TRAIN.MAX_TRAIN_DOCS:
                print(f"[DEBUG] Limiting training docs to {TRAIN.MAX_TRAIN_DOCS} for debug mode")
                docs_all = docs_all[:TRAIN.MAX_TRAIN_DOCS]
    elif DATA.TEXT_COL and DATA.LEXICON_JSON:
        print(f"[DEBUG] Using text+lexicon mode with TEXT_COL='{DATA.TEXT_COL}'")
        df_text = load_data(DATA.TRAIN_PATH, DATA.DATA_IS_PARQUET, DATA.TEXT_COL)
        lex = load_lexicon(DATA.LEXICON_JSON)
        docs_all = docs_from_text_plus_lexicon(df_text, DATA.TEXT_COL, lex)
        source_mode = "text+lexicon"
    else:
        raise RuntimeError(
            "No valid data source inferred. Set DATA.NER_LIST_COL (list-like labels) "
            "or DATA.TEXT_COL + DATA.LEXICON_JSON (bootstrapping)."
        )

    print(f"Docs prepared: {len(docs_all):,} | Source mode: {source_mode}")
    print("Total labeled entities:", sum(len(d.ents) for d in docs_all))

    # Optionally cap docs for local testing, if you want:
    # MAX_DOCS = 500
    # docs_all = docs_all[:MAX_DOCS]

    print(f"[DEBUG] Using only first {len(docs_all)} docs for training")

    train_docs, valid_docs = train_test_split(
        docs_all, test_size=TRAIN.VALID_FRACTION, random_state=TRAIN.RANDOM_SEED
    )
    print(f"train={len(train_docs):,} | valid={len(valid_docs):,}")

    return train_docs, valid_docs, source_mode


from pathlib import Path


def write_docbins(docs: list[Doc], out_dir: Path, shard_size: int) -> None:
    """Write docs into sharded DocBin files."""

    def clean_dir(path: Path):
        path.mkdir(parents=True, exist_ok=True)
        for p in path.glob("*.spacy"):
            print(f"[CLEAN] Removing old shard: {p}")
            p.unlink()
    clean_dir(out_dir)

    out_dir.mkdir(parents=True, exist_ok=True)
    n = len(docs)
    shards = max(1, math.ceil(n / max(1, shard_size)))
    for i in range(shards):
        db = DocBin(store_user_data=False)
        for d in docs[i * shard_size: (i + 1) * shard_size]:
            db.add(d)
        db.to_disk(out_dir / f"shard_{i:04d}.spacy")
    print(f"Wrote {n} docs to {out_dir} in {shards} shard(s).")


def prepare_docbins_from_config() -> None:
    """End-to-end: build docs and write train/valid DocBins according to config."""
    train_docs, valid_docs, _ = build_docs_from_config()
    write_docbins(train_docs, OUT.TRAIN_DIR, shard_size=TRAIN.SHARD_SIZE)
    write_docbins(valid_docs, OUT.VALID_DIR, shard_size=TRAIN.SHARD_SIZE)


__all__ = [
    "docs_from_list_column",
    "load_lexicon",
    "build_entity_ruler",
    "docs_from_text_plus_lexicon",
    "build_docs_from_config",
    "write_docbins",
    "prepare_docbins_from_config",
]

================================================================================
FILE: preprocess_pipeline\pipeline\ingredient_ner\inference.py
================================================================================
from __future__ import annotations

import json
from pathlib import Path
from typing import Optional, Dict, List, Tuple

import pandas as pd

try:
    import pyarrow as pa
    import pyarrow.parquet as pq
    _HAS_PA = True
except Exception:
    _HAS_PA = False

import spacy
from tqdm import tqdm

from .config import DATA, OUT
from .utils import load_data, normalize_token, parse_listlike, join_with_offsets
from .normalization import apply_dedupe, load_jsonl_map, load_encoder_maps

# Import spaCy normalizer for consistent normalization
# This ensures inference uses the same normalization as the training pipeline
try:
    import sys
    from pathlib import Path
    # Add pipeline to path if not already there (for imports when running as script)
    pipeline_path = Path(__file__).parent.parent
    if str(pipeline_path) not in sys.path:
        sys.path.insert(0, str(pipeline_path))
    from ingrnorm.spacy_normalizer import SpacyIngredientNormalizer
    _HAS_SPACY_NORM = True
except (ImportError, ModuleNotFoundError) as e:
    _HAS_SPACY_NORM = False
    SpacyIngredientNormalizer = None
    # Will be handled gracefully in the code


def _unique_preserve_order(seq):
    seen = set()
    out = []
    for x in seq:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out


def _extract_ingredient_rows(
    doc, 
    dedupe: Optional[dict] = None, 
    tok2id: Optional[dict] = None,
    spacy_normalizer: Optional[Any] = None,
):
    """
    Return a list of per-entity dicts with offsets + normalized/canonical forms.
    
    Args:
        doc: spaCy Doc with entities
        dedupe: Dedupe mapping dict (variant → canonical)
        tok2id: Token to ID mapping dict
        spacy_normalizer: Optional SpacyIngredientNormalizer instance for consistent normalization
    """
    rows: List[Dict] = []
    for ent in doc.ents:
        if ent.label_ != "INGREDIENT":
            continue
        raw = ent.text
        
        # Apply spaCy normalization if available (matches training pipeline)
        # Otherwise fall back to simple normalize_token
        if spacy_normalizer is not None:
            norm_result = spacy_normalizer._normalize_phrase(raw)
            norm = norm_result if norm_result else normalize_token(raw)
        else:
            norm = normalize_token(raw)
        
        # Apply dedupe mapping (variant → canonical)
        canon = apply_dedupe(norm, dedupe)
        
        # Map canonical form to ID
        tok_id = tok2id.get(canon, 0) if tok2id else None
        
        rows.append(
            {
                "raw": raw,
                "start": int(ent.start_char),
                "end": int(ent.end_char),
                "label": ent.label_,
                "norm": norm,
                "canonical": canon,
                "id": int(tok_id) if tok_id is not None else None,
            }
        )
    return rows


def predict_normalize_encode_structured(
    nlp_dir: Path,
    data_path: Path,
    is_parquet: bool,
    text_col: str,
    dedupe: Optional[dict] = None,
    tok2id: Optional[dict] = None,
    out_path: Optional[Path] = None,
    batch_size: int = 256,
    # sampling knobs (use exactly one)
    sample_n: Optional[int] = None,
    sample_frac: Optional[float] = None,
    head_n: Optional[int] = None,
    start: int = 0,
    stop: Optional[int] = None,
    sample_seed: int = 42,
    # performance
    n_process: int = 1,  # keep 1 for GPU/transformers
    # normalization
    use_spacy_normalizer: bool = True,  # Use spaCy normalizer to match training pipeline
    spacy_model: str = "en_core_web_sm",  # spaCy model for normalizer
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Returns:
      df_wide: one row per input, columns=[text_col, NER_raw, NER_clean, Ingredients?, spans_json]
      df_tall: one row per extracted entity with offsets and normalized/canonical forms
    If out_path is set, writes two parquet files: <stem>_wide.parquet and <stem>_tall.parquet.
    """
    if not nlp_dir.exists():
        raise FileNotFoundError(f"Model directory not found: {nlp_dir}")

    nlp = spacy.load(nlp_dir)
    
    # Initialize spaCy normalizer if requested (for consistent normalization with training)
    # CRITICAL: This ensures inference uses the same normalization as training pipeline
    # Without this, "buttermilk cornbread" stays as-is instead of becoming "cornbread"
    spacy_normalizer = None
    if use_spacy_normalizer and _HAS_SPACY_NORM:
        try:
            spacy_normalizer = SpacyIngredientNormalizer(model=spacy_model, batch_size=128, n_process=1)
            print(f"✓ Using spaCy normalizer (model={spacy_model}) for consistent normalization with training pipeline")
            print(f"  This ensures ingredients like 'buttermilk cornbread' → 'cornbread' (matching training data)")
        except Exception as e:
            print(f"⚠ Warning: Could not initialize spaCy normalizer: {e}. Falling back to simple normalization.")
            print(f"  This may cause ID=0 for many ingredients if they don't match encoder vocabulary.")
            spacy_normalizer = None
    elif use_spacy_normalizer and not _HAS_SPACY_NORM:
        print("⚠ Warning: spaCy normalizer not available. Using simple normalization.")
        print("  This may cause ID=0 for many ingredients. Install ingrnorm package for full normalization.")

    df_in = load_data(data_path, is_parquet, text_col)

    # Apply ONE sampling strategy
    if head_n is not None:
        df_in = df_in.head(head_n)
    elif sample_n is not None:
        df_in = df_in.sample(n=min(sample_n, len(df_in)), random_state=sample_seed)
    elif sample_frac is not None:
        df_in = df_in.sample(
            frac=min(max(sample_frac, 0.0), 1.0), random_state=sample_seed
        )
    elif start != 0 or stop is not None:
        df_in = df_in.iloc[start:stop]

    # Process each row: parse ingredient list and run NER on each ingredient separately
    # The model is trained on individual ingredients, not joined lists
    raw_texts = df_in[text_col].astype(str).tolist()
    
    wide_rows: List[Dict] = []
    tall_records: List[Dict] = []

    for i, raw_text in enumerate(
        tqdm(raw_texts, total=len(raw_texts), desc="Infer (structured)")
    ):
        # Parse the input into individual ingredients
        ingredients = parse_listlike(raw_text)
        
        # Handle "and" at the end (e.g., "salt, pepper and butter")
        if len(ingredients) > 0:
            last_ing = ingredients[-1]
            if " and " in last_ing.lower():
                # Split on " and " and add the parts
                parts = [p.strip() for p in last_ing.rsplit(" and ", 1)]
                if len(parts) == 2 and parts[1]:
                    ingredients[-1] = parts[0]
                    ingredients.append(parts[1])
        
        # Initialize empty lists for this row
        all_raw = []
        all_clean = []
        all_ids = []
        all_rows = []
        
        # Run NER on each ingredient separately
        if ingredients:
            # Process all ingredients in a batch for efficiency
            docs = list(nlp.pipe(ingredients, batch_size=batch_size, n_process=n_process))
            
            for ing_text, doc in zip(ingredients, docs):
                # Extract entities from this ingredient's doc
                rows = _extract_ingredient_rows(
                    doc, dedupe=dedupe, tok2id=tok2id, spacy_normalizer=spacy_normalizer
                )
                
                if rows:
                    # If NER found entities, use them
                    for r in rows:
                        all_raw.append(r["raw"])
                        if r["canonical"]:
                            all_clean.append(r["canonical"])
                        if tok2id:
                            all_ids.append(r["id"] if r["id"] is not None else 0)
                        all_rows.append(r)
                else:
                    # If NER found nothing, treat the whole ingredient text as the entity
                    # This handles cases where the model doesn't detect anything
                    norm_result = None
                    if spacy_normalizer is not None:
                        norm_result = spacy_normalizer._normalize_phrase(ing_text)
                    norm = norm_result if norm_result else normalize_token(ing_text)
                    canon = apply_dedupe(norm, dedupe)
                    tok_id = tok2id.get(canon, 0) if tok2id else None
                    
                    all_raw.append(ing_text)
                    if canon:
                        all_clean.append(canon)
                    if tok2id:
                        all_ids.append(int(tok_id) if tok_id is not None else 0)
                    all_rows.append({
                        "raw": ing_text,
                        "start": 0,
                        "end": len(ing_text),
                        "label": "INGREDIENT",
                        "norm": norm,
                        "canonical": canon,
                        "id": int(tok_id) if tok_id is not None else None,
                    })
        
        # Deduplicate clean list while preserving order
        clean_list = _unique_preserve_order(all_clean)
        
        # wide entry (compact)
        wide_entry: Dict = {
            text_col: raw_text,  # Store original input format
            "NER_raw": all_raw,
            "NER_clean": clean_list,
            "spans_json": json.dumps(all_rows, ensure_ascii=False),
        }
        if tok2id:
            wide_entry["Ingredients"] = all_ids
        wide_rows.append(wide_entry)

        # tall entries (one row per entity, great for QA/exploration)
        for r in all_rows:
            tall_records.append(
                {
                    "row_id": i,
                    text_col: raw_text,  # Store original input format
                    "ent_text": r["raw"],
                    "start": r["start"],
                    "end": r["end"],
                    "label": r["label"],
                    "norm": r["norm"],
                    "canonical": r["canonical"],
                    "id": r["id"],
                }
            )

    df_wide = pd.DataFrame(wide_rows)
    df_tall = pd.DataFrame(tall_records)
    
    # Diagnostic: count how many got ID=0 (not found in encoder)
    if tok2id:
        zero_ids = sum(1 for r in tall_records if r.get("id") == 0)
        total_entities = len(tall_records)
        if total_entities > 0:
            zero_pct = (zero_ids / total_entities) * 100
            print(f"\n[Diagnostic] ID mapping results:")
            print(f"  Total entities: {total_entities:,}")
            print(f"  Entities with ID=0 (not found): {zero_ids:,} ({zero_pct:.1f}%)")
            print(f"  Entities with valid ID: {total_entities - zero_ids:,} ({100-zero_pct:.1f}%)")
            
            # Check for recipes with only 1 ingredient ID
            recipes_with_one_id = sum(1 for w in wide_rows if tok2id and len(w.get("Ingredients", [])) == 1)
            total_recipes = len(wide_rows)
            if total_recipes > 0:
                one_id_pct = (recipes_with_one_id / total_recipes) * 100
                print(f"\n[Diagnostic] Recipe-level analysis:")
                print(f"  Total recipes: {total_recipes:,}")
                print(f"  Recipes with only 1 ingredient ID: {recipes_with_one_id:,} ({one_id_pct:.1f}%)")
                if one_id_pct > 10:
                    print(f"  ⚠ Note: Some recipes have only 1 ingredient ID.")
                    print(f"    This can happen if:")
                    print(f"    - Multiple ingredients normalize to the same canonical form")
                    print(f"    - Most ingredients get ID=0 (not in encoder vocabulary)")
                    print(f"    - NER model only detected 1 ingredient")
                    # Show a sample
                    sample = next((w for w in wide_rows if tok2id and len(w.get("Ingredients", [])) == 1 and len(w.get("NER_raw", [])) > 1), None)
                    if sample:
                        print(f"\n  Example recipe with 1 ID but multiple raw ingredients:")
                        print(f"    NER_raw ({len(sample.get('NER_raw', []))} items): {sample.get('NER_raw', [])[:5]}")
                        print(f"    NER_clean ({len(sample.get('NER_clean', []))} items): {sample.get('NER_clean', [])[:5]}")
                        print(f"    Ingredients ({len(sample.get('Ingredients', []))} IDs): {sample.get('Ingredients', [])}")
            
            if zero_pct > 50:
                print(f"\n  ⚠ Warning: >50% entities have ID=0. This suggests normalization mismatch.")
                print(f"    Ensure use_spacy_normalizer=True to match training pipeline normalization.")

    if out_path is not None:
        if not _HAS_PA:
            raise RuntimeError("pyarrow is required to write Parquet files.")
        base = Path(out_path)
        wide_path = base.with_name(base.stem + "_wide.parquet")
        tall_path = base.with_name(base.stem + "_tall.parquet")
        pq.write_table(
            pa.Table.from_pandas(df_wide, preserve_index=False).replace_schema_metadata(None),
            wide_path,
        )
        pq.write_table(
            pa.Table.from_pandas(df_tall, preserve_index=False).replace_schema_metadata(None),
            tall_path,
        )
        print(f"Wrote → {wide_path.name} and {tall_path.name} in {wide_path.parent}")

    return df_wide, df_tall


def load_dedupe_and_maps_from_config() -> Tuple[Optional[dict], Optional[dict]]:
    """
    Load dedupe map and token→ID mapping from config paths.
    
    Returns:
        Tuple of (dedupe_dict, tok2id_dict). Either can be None if files don't exist.
        - dedupe_dict: Maps normalized variant phrases → canonical forms
        - tok2id_dict: Maps canonical tokens → integer IDs
    
    Note: The dedupe map and encoder maps are built from spaCy-normalized tokens (NER_clean).
    Therefore, inference must also use spaCy normalization to match these tokens.
    """
    dedupe = None
    tok2id = None
    
    # Load dedupe map (JSONL format)
    if DATA.DEDUPE_JSONL and DATA.DEDUPE_JSONL.exists():
        dedupe = load_jsonl_map(DATA.DEDUPE_JSONL)
        print(f"✓ Loaded dedupe map: {len(dedupe):,} mappings from {DATA.DEDUPE_JSONL}")
        # Show a sample for debugging
        if dedupe and len(dedupe) > 0:
            sample_items = list(dedupe.items())[:3]
            print(f"  Sample entries: {sample_items}")
    else:
        print(f"⚠ Dedupe map not found at {DATA.DEDUPE_JSONL} (skipping deduplication)")
    
    # Load encoder maps (token ↔ ID)
    if DATA.ING_TOK2ID_JSON and DATA.ING_TOK2ID_JSON.exists():
        _, tok2id = load_encoder_maps(DATA.ING_ID2TOK_JSON, DATA.ING_TOK2ID_JSON)
        if tok2id:
            print(f"✓ Loaded token→ID map: {len(tok2id):,} tokens from {DATA.ING_TOK2ID_JSON}")
            # Show a sample for debugging
            if tok2id and len(tok2id) > 0:
                sample_items = list(tok2id.items())[:3]
                print(f"  Sample entries: {sample_items}")
        else:
            print(f"⚠ Token→ID map not found or empty at {DATA.ING_TOK2ID_JSON}")
    else:
        print(f"⚠ Token→ID map not found at {DATA.ING_TOK2ID_JSON} (skipping ID encoding)")
    
    return dedupe, tok2id


def run_full_inference_from_config(
    text_col: str,
    out_base: Path,
    data_path: Optional[Path] = None,
    sample_n: Optional[int] = None,
    sample_frac: Optional[float] = None,
    head_n: Optional[int] = None,
    batch_size: int = 256,
    n_process: int = 1,
    use_spacy_normalizer: bool = True,
    spacy_model: str = "en_core_web_sm",
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    High-level helper: run inference using config paths and settings.
    
    Args:
        text_col: Column name containing raw ingredient text
        out_base: Base path for output files (will write <base>_wide.parquet and <base>_tall.parquet)
        data_path: Optional override for input data (defaults to DATA.TRAIN_PATH)
        sample_n: Optional number of rows to sample
        sample_frac: Optional fraction of rows to sample
        head_n: Optional number of rows from head
        batch_size: Batch size for spaCy processing
        n_process: Number of processes (keep 1 for GPU/transformers, >1 may not work on Windows)
    
    Returns:
        Tuple of (df_wide, df_tall) DataFrames
    """
    # Determine input path
    if data_path is None:
        data_path = DATA.TRAIN_PATH
    if not data_path.exists():
        raise FileNotFoundError(f"Input data not found: {data_path}")
    
    # Determine if parquet or CSV
    is_parquet = DATA.DATA_IS_PARQUET if hasattr(DATA, 'DATA_IS_PARQUET') else (data_path.suffix.lower() == ".parquet")
    
    # Load dedupe and token→ID maps from config
    dedupe, tok2id = load_dedupe_and_maps_from_config()
    
    # Run inference
    return predict_normalize_encode_structured(
        nlp_dir=OUT.MODEL_DIR,
        data_path=data_path,
        is_parquet=is_parquet,
        text_col=text_col,
        dedupe=dedupe,
        tok2id=tok2id,
        out_path=out_base,
        batch_size=batch_size,
        sample_n=sample_n,
        sample_frac=sample_frac,
        head_n=head_n,
        n_process=n_process,
        use_spacy_normalizer=use_spacy_normalizer,
        spacy_model=spacy_model,
    )


__all__ = ["predict_normalize_encode_structured", "load_dedupe_and_maps_from_config", "run_full_inference_from_config"]

================================================================================
FILE: preprocess_pipeline\pipeline\ingredient_ner\normalization.py
================================================================================
import json
from pathlib import Path
from typing import Dict, Optional, Tuple, Union

from .utils import normalize_token


def load_jsonl_map(path: Union[str, Path, None]) -> Dict[str, str]:
    """Load dedupe map from JSONL lines: {'from': '...', 'to': '...'}."""
    mapping: Dict[str, str] = {}
    if path is None:
        return mapping
    p = Path(path)
    if not p.exists():
        return mapping
    with open(p, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                obj = json.loads(line)
                src = normalize_token(str(obj.get("from", "")))
                dst = normalize_token(str(obj.get("to", "")))
                if src and dst:
                    mapping[src] = dst
    return mapping


def load_encoder_maps(
    id2tok_path: Optional[Path],
    tok2id_path: Optional[Path],
) -> Tuple[Optional[dict], Optional[dict]]:
    """Load IngredientEncoder id2tok / tok2id maps."""
    if not id2tok_path or not tok2id_path:
        return None, None
    if (not Path(id2tok_path).exists()) or (not Path(tok2id_path).exists()):
        return None, None
    with open(id2tok_path, "r", encoding="utf-8") as f:
        id2tok_raw = json.load(f)
    with open(tok2id_path, "r", encoding="utf-8") as f:
        tok2id_raw = json.load(f)
    id2tok = {int(k): str(v) for k, v in id2tok_raw.items()}
    tok2id = {str(k): int(v) for k, v in tok2id_raw.items()}
    return id2tok, tok2id


def apply_dedupe(tok: str, mapping: Optional[Dict[str, str]]) -> str:
    return mapping.get(tok, tok) if mapping else tok


__all__ = ["load_jsonl_map", "load_encoder_maps", "apply_dedupe"]

================================================================================
FILE: preprocess_pipeline\pipeline\ingredient_ner\training.py
================================================================================
import logging
import random
import warnings
from pathlib import Path
from typing import Iterable, List, Tuple

import spacy
from spacy.language import Language
from spacy.tokens import DocBin
from spacy.training import Example

from .config import TRAIN, OUT
from .utils import configure_device, set_global_seed

logger = logging.getLogger(__name__)

try:
    import torch
except Exception:
    torch = None

# ---------- Simple in-memory training (optional) ----------

def train_spacy_ner_simple(
    train_docs: List[spacy.tokens.Doc],
    valid_docs: List[spacy.tokens.Doc],
    n_epochs: int = 10,
    lr: float = 0.001,
    dropout: float = 0.2,
    batch_size: int = 128,
) -> Language:
    """Simple CPU-only NER training for quick experiments."""
    nlp = spacy.blank("en")
    ner = nlp.add_pipe("ner")
    ner.add_label("INGREDIENT")

    # Convert docs to Examples (NER only)
    train_examples: List[Example] = []
    for d in train_docs:
        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in d.ents]
        train_examples.append(Example.from_dict(nlp.make_doc(d.text), {"entities": ents}))

    valid_examples: List[Example] = []
    for d in valid_docs:
        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in d.ents]
        valid_examples.append(Example.from_dict(nlp.make_doc(d.text), {"entities": ents}))

    optimizer = nlp.initialize(lambda: train_examples)
    logger.info(f"Initialized pipeline: {nlp.pipe_names}")

    for epoch in range(n_epochs):
        random.shuffle(train_examples)
        losses = {}
        for i in range(0, len(train_examples), batch_size):
            batch = train_examples[i: i + batch_size]
            nlp.update(batch, sgd=optimizer, drop=dropout, losses=losses)
        with nlp.select_pipes(disable=[p for p in nlp.pipe_names if p != "ner"]):
            scores = nlp.evaluate(valid_examples)
        logger.info(
            f"Epoch {epoch + 1:02d}/{n_epochs} - Losses: {losses} - "
            f"P/R/F1: {scores['ents_p']:.3f}/{scores['ents_r']:.3f}/{scores['ents_f']:.3f}"
        )
    return nlp


# ---------- Transformer / tok2vec training using DocBins ----------

def build_nlp_transformer() -> spacy.language.Language:
    """Build a small-window transformer + NER with optional layer freezing."""
    try:
        import spacy_transformers  # noqa: F401
    except Exception as e:
        raise RuntimeError("spacy-transformers is not available.") from e

    from .config import TRAIN as _TRAIN

    nlp = spacy.blank("en")
    trf_cfg = {
        "model": {
            "@architectures": "spacy-transformers.TransformerModel.v3",
            "name": _TRAIN.TRANSFORMER_MODEL,
            "tokenizer_config": {"use_fast": True},
            "transformer_config": {},
            "mixed_precision": bool(_TRAIN.USE_AMP),
            "grad_scaler_config": {"enabled": bool(_TRAIN.USE_AMP)},
            "get_spans": {
                "@span_getters": "spacy-transformers.strided_spans.v1",
                "window": int(_TRAIN.WINDOW),
                "stride": int(_TRAIN.STRIDE),
            },
        },
        "set_extra_annotations": {
            "@annotation_setters": "spacy-transformers.null_annotation_setter.v1"
        },
        "max_batch_items": 4096,
    }
    nlp.add_pipe("transformer", config=trf_cfg)
    ner = nlp.add_pipe("ner")
    ner.add_label("INGREDIENT")

    # Optional layer freezing
    if _TRAIN.FREEZE_LAYERS > 0:
        try:
            trf = nlp.get_pipe("transformer").model
            hf = trf.transformer.model
            blocks = None
            if hasattr(hf, "transformer") and hasattr(hf.transformer, "layer"):  # distilbert
                blocks = hf.transformer.layer
            elif hasattr(hf, "encoder") and hasattr(hf.encoder, "layer"):        # bert/roberta
                blocks = hf.encoder.layer
            if blocks is not None:
                k = min(_TRAIN.FREEZE_LAYERS, len(blocks))
                for i in range(k):
                    for p in blocks[i].parameters():
                        p.requires_grad = False
                logger.info(f"[transformer] Froze {k} lower layer(s).")
        except Exception as e:
            warnings.warn(f"Could not freeze layers: {e}")
    return nlp


def build_nlp_tok2vec() -> spacy.language.Language:
    """CPU-friendly tok2vec + NER fallback."""
    nlp = spacy.blank("en")
    nlp.add_pipe("tok2vec")
    ner = nlp.add_pipe("ner")
    ner.add_label("INGREDIENT")
    logger.info("Using tok2vec fallback (no transformers).")
    return nlp


def choose_nlp() -> Tuple[Language, str]:
    """Choose transformer or tok2vec pipeline based on config and available deps."""
    from .config import TRAIN as _TRAIN

    # Check if debug mode forces tok2vec
    if hasattr(_TRAIN, 'USE_TOK2VEC_DEBUG') and _TRAIN.USE_TOK2VEC_DEBUG:
        logger.info("Debug mode: using tok2vec instead of transformers (faster, CPU-friendly)")
        return build_nlp_tok2vec(), "tok2vec"

    if torch is not None:
        has_trf = True
        try:
            import spacy_transformers  # noqa
        except Exception:
            has_trf = False
        if has_trf:
            try:
                return build_nlp_transformer(), "transformer"
            except Exception as e:
                warnings.warn(f"Falling back to tok2vec due to: {e}")
                return build_nlp_tok2vec(), "tok2vec"
    # No torch or transformers
    return build_nlp_tok2vec(), "tok2vec"


from typing import Optional

def iter_examples_from_docbins(
    nlp: Language,
    dir_path: Path,
    shuffle: bool = False,
    max_docs: Optional[int] = None,
) -> Iterable[Example]:
    shard_paths = sorted(p for p in dir_path.glob("*.spacy"))
    logger.debug(f"iter_examples_from_docbins: found {len(shard_paths)} shard(s) in {dir_path}")
    if shuffle:
        random.shuffle(shard_paths)

    count = 0
    for sp_i, sp_path in enumerate(shard_paths, start=1):
        db = DocBin().from_disk(sp_path)
        for d in db.get_docs(nlp.vocab):
            ents = [(e.start_char, e.end_char, e.label_) for e in d.ents]
            yield Example.from_dict(nlp.make_doc(d.text), {"entities": ents})
            count += 1
            if max_docs is not None and count >= max_docs:
                logger.debug(f"max_docs={max_docs} reached, stopping iterator.")
                return


def sample_validation(nlp: Language, dir_path: Path, cap: int = 1500) -> List[Example]:
    out: List[Example] = []
    for eg in iter_examples_from_docbins(nlp, dir_path, shuffle=False):
        out.append(eg)
        if len(out) >= cap:
            break
    return out


def compounding_batch(epoch: int, total_epochs: int, start: int = 8, end: int = 16) -> int:
    if total_epochs <= 1:
        return end
    r = epoch / (total_epochs - 1)
    return max(1, int(round(start * ((end / start) ** r))))

def count_examples_in_docbins(nlp: Language, dir_path: Path) -> int:
    total = 0
    shard_paths = sorted(p for p in dir_path.glob("*.spacy"))
    logger.debug(f"Counting examples in {dir_path}, found {len(shard_paths)} shard(s)")
    for sp_i, sp_path in enumerate(shard_paths, start=1):
        db = DocBin().from_disk(sp_path)
        n_docs = sum(1 for _ in db.get_docs(nlp.vocab))
        total += n_docs
        logger.debug(f"  shard #{sp_i}: {sp_path.name} has {n_docs} docs")
    logger.debug(f"Total train examples in {dir_path}: {total}")
    return total

def train_ner_from_docbins(
    train_dir: Path | None = None,
    valid_dir: Path | None = None,
    out_model_dir: Path | None = None,
) -> Language:
    from .config import TRAIN as _TRAIN, OUT as _OUT
    import time

    train_dir = train_dir or _OUT.TRAIN_DIR
    valid_dir = valid_dir or _OUT.VALID_DIR
    out_model_dir = out_model_dir or _OUT.MODEL_DIR

    logger.debug(f"train_dir={train_dir}")
    logger.debug(f"valid_dir={valid_dir}")
    logger.debug(f"out_model_dir={out_model_dir}")

    # Configure device FIRST (before creating pipeline)
    configure_device()
    set_global_seed(_TRAIN.RANDOM_SEED)

    t0 = time.time()
    logger.debug("Choosing pipeline...")
    nlp, mode = choose_nlp()
    
    # Log device info for transformer models
    if mode == "transformer" and torch is not None:
        try:
            trf = nlp.get_pipe("transformer")
            if hasattr(trf, "model"):
                model_ref = trf.model.get_ref("model")
                if hasattr(model_ref, "device"):
                    logger.info(f"Transformer model device: {model_ref.device}")
        except Exception as e:
            logger.debug(f"Could not determine transformer device: {e}")
    
    logger.debug(f"choose_nlp() done in {time.time() - t0:.1f}s")
    logger.info(f"Pipeline mode: {mode}")
    
    # Log debug mode status
    if hasattr(_TRAIN, 'MAX_TRAIN_DOCS') and _TRAIN.MAX_TRAIN_DOCS is not None:
        logger.info(f"Debug mode: max_train_docs={_TRAIN.MAX_TRAIN_DOCS}")

    # Warm init
    logger.debug("Collecting warm-up examples...")
    t0 = time.time()
    warm: List[Example] = []
    for eg_i, eg in enumerate(
        iter_examples_from_docbins(nlp, train_dir, shuffle=True),
        start=1,
    ):
        warm.append(eg)
        if eg_i % 50 == 0:
            logger.debug(f"warm example #{eg_i}")
        if eg_i >= min(256, max(16, 100)):
            break
    logger.debug(f"Collected {len(warm)} warm examples in {time.time() - t0:.1f}s")

    logger.debug("Calling nlp.initialize(...)")
    t0 = time.time()
    optimizer = nlp.initialize(lambda: warm)
    logger.debug(f"nlp.initialize(...) finished in {time.time() - t0:.1f}s")

    if hasattr(optimizer, "learn_rate"):
        optimizer.learn_rate = float(_TRAIN.LR)
        logger.debug(f"Set optimizer learn_rate={optimizer.learn_rate}")

    logger.debug("Building validation snapshot...")
    t0 = time.time()
    valid_snapshot = sample_validation(nlp, valid_dir, cap=_TRAIN.EVAL_SNAPSHOT_MAX)
    logger.debug(f"Validation snapshot size={len(valid_snapshot)} "
          f"built in {time.time() - t0:.1f}s")

    best_f1 = -1.0
    bad_epochs = 0

    logger.debug("Sanity-checking training set size...")
    _ = count_examples_in_docbins(nlp, train_dir)

    for epoch in range(_TRAIN.N_EPOCHS):
        logger.info(f"===== Epoch {epoch + 1}/{_TRAIN.N_EPOCHS} =====")
        losses: dict = {}
        micro_bs = compounding_batch(epoch, _TRAIN.N_EPOCHS, start=8, end=16)
        logger.debug(f"micro-batch size (μbs) = {micro_bs}")
        buf: List[Example] = []
        updates = 0

        t_epoch = time.time()
        for eg_i, eg in enumerate(iter_examples_from_docbins(nlp, train_dir, shuffle=True), start=1):
            buf.append(eg)
            if len(buf) < micro_bs:
                continue
            nlp.update(buf, sgd=optimizer, drop=_TRAIN.DROPOUT, losses=losses)
            buf.clear()
            updates += 1

            if updates % 20 == 0:
                logger.debug(f"epoch={epoch+1} updates={updates} "
                      f"seen_examples≈{eg_i} loss={losses.get('ner', 0):.2f}")

            if (torch is not None) and torch.cuda.is_available() \
                    and updates % _TRAIN.CLEAR_CACHE_EVERY == 0:
                logger.debug("emptying CUDA cache...")
                torch.cuda.empty_cache()

        if buf:
            logger.debug(f"Flushing last batch of size {len(buf)}")
            nlp.update(buf, sgd=optimizer, drop=_TRAIN.DROPOUT, losses=losses)
            buf.clear()

        logger.debug(f"Finished epoch {epoch+1} updates={updates} "
              f"in {time.time() - t_epoch:.1f}s")

        logger.debug("Running evaluation on validation snapshot...")
        with nlp.select_pipes(disable=[p for p in nlp.pipe_names if p != "ner"]):
            scores = nlp.evaluate(valid_snapshot)

        p = float(scores.get("ents_p") or 0.0)
        r = float(scores.get("ents_r") or 0.0)
        f1 = float(scores.get("ents_f") or 0.0)

        logger.info(
            f"Epoch {epoch + 1:02d}/{_TRAIN.N_EPOCHS} | μbs={micro_bs:<3d} "
            f"| loss={losses.get('ner', 0):.1f} | P/R/F1={p:.3f}/{r:.3f}/{f1:.3f}"
        )

        improved = f1 > best_f1 + 1e-6
        if improved:
            best_f1 = f1
            bad_epochs = 0
            out_model_dir.mkdir(parents=True, exist_ok=True)
            nlp.to_disk(out_model_dir)
            logger.info(f"  ↳ Saved model-best → {out_model_dir} (F1={f1:.3f})")
        else:
            bad_epochs += 1
            if bad_epochs >= _TRAIN.EARLY_STOPPING_PATIENCE:
                logger.info(f"Early stopping after {bad_epochs} non-improving epoch(s).")
                break

    logger.info(f"Best F1 observed: {best_f1 if best_f1 >= 0 else 0.0}")
    return nlp


__all__ = [
    "train_spacy_ner_simple",
    "build_nlp_transformer",
    "build_nlp_tok2vec",
    "choose_nlp",
    "iter_examples_from_docbins",
    "sample_validation",
    "compounding_batch",
    "train_ner_from_docbins",
]

================================================================================
FILE: preprocess_pipeline\pipeline\ingredient_ner\utils.py
================================================================================
import ast
import json
import math
import os
import random
import warnings
from pathlib import Path
from typing import Any, List

import numpy as np
import pandas as pd

try:
    import pyarrow.parquet as pq
    _HAS_PA = True
except Exception:
    _HAS_PA = False

import spacy
from typing import Optional
try:
    import torch
except Exception:  # torch is optional
    torch = None
    warnings.warn(
        "Torch not available. Training will fall back to CPU tok2vec "
        "if transformers cannot be used."
    )


def set_global_seed(seed: int = 42) -> None:
    random.seed(seed)
    np.random.seed(seed)
    if torch is not None:
        try:
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(seed)
        except Exception:
            pass


import logging
import spacy

try:
    import torch
except Exception:
    torch = None

from .config import TRAIN

logger = logging.getLogger("ingredient_ner.training")


def configure_device() -> None:
    """
    Configure device for training.

    - If USE_TOK2VEC_DEBUG is True, we just stay on CPU.
    - If torch.cuda.is_available() is True, the transformer will use GPU via PyTorch.
    - We *do not* call spacy.require_gpu(), so CuPy is no longer a hard requirement.
    """
    if getattr(TRAIN, "USE_TOK2VEC_DEBUG", False):
        logger.info("[device] Debug/tok2vec mode – forcing CPU (tok2vec).")
        return

    if torch is None:
        logger.info("[device] torch not available – using CPU.")
        return

    if torch.cuda.is_available():
        dev_name = torch.cuda.get_device_name(0)
        logger.info(f"[device] CUDA available – PyTorch will use GPU: {dev_name}")
        # Important: DON'T call spacy.require_gpu() here.
        # spacy-transformers uses PyTorch directly.
    else:
        logger.info("[device] CUDA not available – using CPU.")

def load_data(path: Path, is_parquet: bool, col: str, max_rows: Optional[int] = None) -> pd.DataFrame:
    """
    Load a single column from CSV/Parquet and return a clean DataFrame.
    
    Args:
        path: Path to the data file
        is_parquet: Whether the file is Parquet format
        col: Column name to extract
        max_rows: Optional limit on number of rows to load (None = load all)
    """
    def read_csv_with_fallback(path,dtype=str, nrows=None):
        """
        Tries to read CSV using UTF-8, then cp1252 (windows-1252), then latin-1.
        Returns a DataFrame and the encoding used (string).
        """
        tried = []
        encodings = ["utf-8", "cp1252", "latin-1"]
        for enc in encodings:
            try:
                logger.info(f"Attempting to read CSV with encoding={enc}")
                df = pd.read_csv(path, dtype=dtype, encoding=enc, nrows=nrows)
                logger.info(f"Successfully read CSV with encoding={enc}")
                return df, enc
            except UnicodeDecodeError as e:
                tried.append((enc, str(e)))
                logger.warning(f"Failed with encoding={enc}: {e}")
            except Exception as e:
                # Non-encoding errors should be raised
                logger.exception(f"Unexpected error reading CSV with encoding={enc}: {e}")
                raise

        # Last resort: read with errors='replace' so no exception but some characters may be lost
        logger.warning("All standard encodings failed. Trying utf-8 with errors='replace'.")
        df = pd.read_csv(path, dtype=dtype, encoding="utf-8", engine="python", error_bad_lines=False, warn_bad_lines=True)
        return df, "utf-8 (errors=replace)"

    if is_parquet:
        if not _HAS_PA:
            raise RuntimeError("pyarrow is required to read Parquet files. Please install pyarrow.")
        pf = pq.ParquetFile(str(path))
        frames = []
        total_rows = 0
        for i in range(pf.num_row_groups):
            if max_rows is not None and total_rows >= max_rows:
                break
            frame = pf.read_row_group(i).to_pandas()
            if max_rows is not None:
                remaining = max_rows - total_rows
                if len(frame) > remaining:
                    frame = frame.head(remaining)
                total_rows += len(frame)
            frames.append(frame)
        df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
    else:
        df, used_encoding = read_csv_with_fallback(path, dtype=str, nrows=max_rows)
        logger.info(f"Loaded CSV using encoding: {used_encoding}")
    if col not in df.columns:
        raise KeyError(f"Column '{col}' not found in {list(df.columns)[:20]}...")
    return df[[col]].dropna().reset_index(drop=True)


def parse_listlike(v: Any) -> List[str]:
    """Parse values that may be python-lists, JSON-lists, or comma-separated strings."""
    if isinstance(v, (list, tuple)):
        return [str(x).strip() for x in v if str(x).strip()]
    s = str(v).strip()
    if not s:
        return []
    for parser in (ast.literal_eval, json.loads):
        try:
            out = parser(s)
            if isinstance(out, (list, tuple)):
                return [str(x).strip() for x in out if str(x).strip()]
        except Exception:
            pass
    return [x.strip() for x in s.split(",") if x.strip()]


def join_with_offsets(tokens: List[str], sep: str = ", "):
    """Join tokens with a separator and track character offsets of each token."""
    text, spans, pos = [], [], 0
    for i, tok in enumerate(tokens):
        start, end = pos, pos + len(tok)
        text.append(tok)
        spans.append((start, end))
        pos = end
        if i < len(tokens) - 1:
            text.append(sep)
            pos += len(sep)
    return "".join(text), spans


def normalize_token(s: str) -> str:
    """Lowercase, trim and collapse whitespace for consistent string keys."""
    return " ".join(str(s).strip().lower().split())

================================================================================
FILE: preprocess_pipeline\pipeline\ingredient_ner\viz.py
================================================================================
import json

import pandas as pd
from IPython.display import display, HTML
from html import escape


def preview_side_by_side(df_wide: pd.DataFrame, text_col: str, n: int = 8):
    """Simple tabular 'original vs cleaned' preview."""
    cols = [text_col, "NER_raw", "NER_clean"] + (
        ["Ingredients"] if "Ingredients" in df_wide.columns else []
    )
    display(df_wide.loc[:, cols].head(n))


def _render_marked(text: str, spans: list[dict]) -> str:
    """Mark entities inline; tooltip shows norm/canonical/id for quick QA."""
    spans = sorted(spans, key=lambda r: r["start"])
    pos = 0
    out = []
    for r in spans:
        out.append(escape(text[pos: r["start"]]))
        frag = escape(text[r["start"]: r["end"]])
        tip = (
            f'norm="{r["norm"]}" | canonical="{r["canonical"]}" | '
            f'id={r["id"] if r["id"] is not None else "-"}'
        )
        out.append(f'<mark title="{escape(tip)}">{frag}</mark>')
        pos = r["end"]
    out.append(escape(text[pos:]))
    return "".join(out)


def html_preview(df_wide: pd.DataFrame, text_col: str, n: int = 8):
    """Inline HTML with highlighted entities and cleaned list below."""
    rows = []
    for _, row in df_wide.head(n).iterrows():
        spans = json.loads(row["spans_json"])
        marked = _render_marked(row[text_col], spans)
        cleaned = ", ".join(row.get("NER_clean") or [])
        rows.append(
            f"""
        <div class="one">
          <div class="orig">{marked}</div>
          <div class="clean"><strong>NER_clean:</strong> {escape(cleaned)}</div>
        </div>
        """
        )
    style = """
    <style>
      .one{border:1px solid #ddd; padding:10px; margin:8px 0; border-radius:6px;}
      .orig{margin-bottom:6px; line-height:1.5}
      mark{padding:0 2px; border-radius:3px}
      .clean{font-family:monospace}
    </style>
    """
    display(HTML(style + "\n".join(rows)))


def describe_predictions(df_wide: pd.DataFrame, top_k: int = 20):
    """Small summary to sanity-check output distribution."""
    s = df_wide["NER_clean"].explode().value_counts().head(top_k)
    print(
        f"Rows: {len(df_wide):,} | "
        f"rows with ≥1 pred: {(df_wide['NER_clean'].map(bool)).sum():,}"
    )
    print(
        f"Mean #unique preds/row: {df_wide['NER_clean'].map(len).mean():.2f}"
    )
    display(s.to_frame("freq"))


__all__ = ["preview_side_by_side", "html_preview", "describe_predictions"]

================================================================================
FILE: preprocess_pipeline\pipeline\ingrnorm\__init__.py
================================================================================

from .io import materialize_parquet_source
from .parquet_utils import vocab_from_parquet_listcol
from .spacy_normalizer import apply_spacy_normalizer_to_parquet, SpacyIngredientNormalizer
from .w2v_dedupe import w2v_dedupe
from .sbert_dedupe import sbert_dedupe
from .dedupe_map import load_jsonl_map, write_jsonl_map, apply_map_to_parquet_streaming
from .encoder import IngredientEncoder

__all__ = [
    "materialize_parquet_source",
    "vocab_from_parquet_listcol",
    "apply_spacy_normalizer_to_parquet",
    "SpacyIngredientNormalizer",
    "w2v_dedupe",
    "sbert_dedupe",
    "load_jsonl_map",
    "write_jsonl_map",
    "apply_map_to_parquet_streaming",
    "IngredientEncoder",
]

================================================================================
FILE: preprocess_pipeline\pipeline\ingrnorm\dedupe_map.py
================================================================================

from __future__ import annotations
import json
from pathlib import Path
from typing import Dict, Union
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq

def load_jsonl_map(path: Union[str, Path]) -> Dict[str, str]:
    mapping: Dict[str, str] = {}
    p = Path(path)
    if not p.exists():
        return mapping
    with open(p, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            obj = json.loads(line)
            mapping[str(obj.get("from",""))] = str(obj.get("to",""))
    return mapping

def write_jsonl_map(mapping: Dict[str, str], out_path: Union[str, Path]) -> Path:
    out_path = Path(out_path); out_path.parent.mkdir(parents=True, exist_ok=True)
    data = "\n".join(json.dumps({"from": k, "to": v}) for k, v in mapping.items())
    # Write directly - file comparison is inefficient for large files
    # If needed, use --force flag in scripts to rebuild
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(data + "\n")
    return out_path

def apply_map_to_parquet_streaming(
    in_path: Union[str, Path],
    out_path: Union[str, Path],
    mapping: Union[Dict[str, str], str, Path],
    list_col: str = "NER_clean",
    compression: str = "zstd",
) -> None:
    if not isinstance(mapping, dict):
        mapping = load_jsonl_map(mapping)

    pf = pq.ParquetFile(str(in_path))
    writer = None
    out_path = Path(out_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    for rg in range(pf.num_row_groups):
        df = pf.read_row_group(rg).to_pandas()
        if list_col in df.columns:
            # Handle numpy arrays and other list-like types
            df[list_col] = [
                [mapping.get(str(tok), str(tok)) for tok in (lst if isinstance(lst, (list, tuple)) else list(lst))]
                if isinstance(lst, (list, tuple, np.ndarray)) and len(lst) > 0
                else (lst if isinstance(lst, (list, tuple)) else [])
                for lst in df[list_col]
            ]
        table = pa.Table.from_pandas(df, preserve_index=False).replace_schema_metadata(None)
        if writer is None:
            writer = pq.ParquetWriter(str(out_path), table.schema, compression=compression)
        writer.write_table(table)
    if writer is not None:
        writer.close()

================================================================================
FILE: preprocess_pipeline\pipeline\ingrnorm\encoder.py
================================================================================

from __future__ import annotations
from pathlib import Path
from typing import Iterable, Any, Optional, Dict, Union, List
from collections import Counter
import json
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

class IngredientEncoder:
    """Deterministic token→id encoder for already-normalized ingredient tokens."""
    def __init__(self, min_freq: int = 1):
        self.min_freq = int(min_freq)
        self.token_to_id: Dict[str, int] = {"<UNK>": 0}
        self.id_to_token: Dict[int, str] = {0: "<UNK>"}
        self._frozen = False

    def _fit_from_counter(self, cnt: Counter, min_freq: int) -> "IngredientEncoder":
        """Internal helper to add tokens from a counter to the encoder."""
        for tok, c in cnt.items():
            if c >= min_freq and tok not in self.token_to_id:
                idx = len(self.token_to_id)
                self.token_to_id[tok] = idx
                self.id_to_token[idx] = tok
        return self

    def _count_tokens(self, series: Iterable[Any]) -> Counter:
        """Count tokens from an iterable of lists."""
        cnt = Counter()
        for lst in series:
            if isinstance(lst, (list, tuple, np.ndarray)):
                for t in lst:
                    s = str(t).strip().lower()
                    if s:
                        cnt[s] += 1
        return cnt

    def fit_from_series(self, series: Iterable[Any], min_freq: Optional[int] = None) -> "IngredientEncoder":
        mf = self.min_freq if min_freq is None else int(min_freq)
        cnt = self._count_tokens(series)
        return self._fit_from_counter(cnt, mf)

    def fit_from_parquet_streaming(self, parquet_path: Union[str, Path], col: str = "NER_clean", min_freq: Optional[int] = None) -> "IngredientEncoder":
        parquet_path = Path(parquet_path)
        if not parquet_path.exists():
            raise FileNotFoundError(f"Parquet file not found: {parquet_path}")
        mf = self.min_freq if min_freq is None else int(min_freq)
        pf = pq.ParquetFile(parquet_path)
        if pf.num_row_groups == 0:
            raise ValueError(f"Parquet file has no row groups: {parquet_path}")
        cnt = Counter()
        for rg in range(pf.num_row_groups):
            df_rg = pf.read_row_group(rg, columns=[col]).to_pandas()
            if col not in df_rg.columns:
                raise ValueError(f"Column '{col}' not found in parquet file {parquet_path} (row group {rg})")
            for lst in df_rg[col]:
                if isinstance(lst, (list, tuple, np.ndarray)):
                    for t in lst:
                        s = str(t).strip().lower()
                        if s:
                            cnt[s] += 1
        return self._fit_from_counter(cnt, mf)

    def freeze(self) -> "IngredientEncoder":
        self._frozen = True
        return self

    def transform_series_to_idlists(self, series: Iterable[Any]) -> List[List[int]]:
        out: List[List[int]] = []
        for lst in series:
            ids: List[int] = []
            if isinstance(lst, (list, tuple, np.ndarray)):
                for t in lst:
                    tok = str(t).strip().lower()
                    if tok:
                        ids.append(self.token_to_id.get(tok, 0))
            else:
                ids = [0]
            out.append(ids if ids else [0])
        return out

    def transform_df(self, df: pd.DataFrame, ingredients_col: str = "NER_clean", dataset_id: int = 1) -> pd.DataFrame:
        id_lists = self.transform_series_to_idlists(df[ingredients_col])
        res = pd.DataFrame({
            "Dataset ID": np.int32(dataset_id),
            "Index": np.arange(len(df), dtype=np.int64),
            "Ingredients": id_lists,
        })
        return res

    def encode_parquet_streaming(self, parquet_path: Union[str, Path], out_parquet_path: Union[str, Path], dataset_id: int = 1, col: str = "NER_clean", compression: str = "zstd") -> Path:
        parquet_path = Path(parquet_path)
        out_parquet_path = Path(out_parquet_path)
        pf = pq.ParquetFile(parquet_path)
        target_schema = pa.schema([
            pa.field("Dataset ID", pa.int32()),
            pa.field("Index", pa.int64()),
            pa.field("Ingredients", pa.list_(pa.int64())),
        ])
        out_parquet_path.parent.mkdir(parents=True, exist_ok=True)
        writer = pq.ParquetWriter(out_parquet_path, target_schema, compression=compression)
        global_index_start = 0
        for rg in range(pf.num_row_groups):
            df_rg = pf.read_row_group(rg, columns=[col]).to_pandas()
            id_lists = self.transform_series_to_idlists(df_rg[col])
            ds_ids = pa.array(np.full(len(id_lists), dataset_id, dtype=np.int32))
            idxs   = pa.array(np.arange(global_index_start, global_index_start + len(id_lists), dtype=np.int64))
            ingr   = pa.array([(lst if isinstance(lst, (list, tuple, np.ndarray)) else []) for lst in id_lists], type=pa.list_(pa.int64()))
            tbl = pa.Table.from_arrays([ds_ids, idxs, ingr], names=["Dataset ID", "Index", "Ingredients"])
            writer.write_table(tbl)
            global_index_start += len(id_lists)
        writer.close()
        return out_parquet_path

    def save_maps(self, id_to_token_path: Union[str, Path], token_to_id_path: Optional[Union[str, Path]] = None) -> None:
        id_to_token_path = Path(id_to_token_path)
        token_to_id_path = Path(token_to_id_path) if token_to_id_path else id_to_token_path.with_name("ingredient_token_to_id.json")
        id_to_token_path.parent.mkdir(parents=True, exist_ok=True)
        with open(id_to_token_path, "w", encoding="utf-8") as f:
            json.dump({str(k): v for k, v in self.id_to_token.items()}, f, indent=2)
        with open(token_to_id_path, "w", encoding="utf-8") as f:
            json.dump(self.token_to_id, f, indent=2)

    @classmethod
    def load_maps(cls, id_to_token_path: Union[str, Path], token_to_id_path: Optional[Union[str, Path]] = None) -> "IngredientEncoder":
        id_to_token_path = Path(id_to_token_path)
        token_to_id_path = Path(token_to_id_path) if token_to_id_path else id_to_token_path.with_name("ingredient_token_to_id.json")
        with open(id_to_token_path, "r", encoding="utf-8") as f:
            id_to_token_raw = json.load(f)
        with open(token_to_id_path, "r", encoding="utf-8") as f:
            token_to_id_raw = json.load(f)
        enc = cls()
        enc.id_to_token = {int(k): str(v) for k, v in id_to_token_raw.items()}
        enc.token_to_id = {str(k): int(v) for k, v in token_to_id_raw.items()}
        enc._frozen = True
        return enc

================================================================================
FILE: preprocess_pipeline\pipeline\ingrnorm\io.py
================================================================================

from __future__ import annotations
import ast, json
from pathlib import Path
from typing import List
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

def parse_listish(v) -> list[str]:
    """Accept list/tuple/np.ndarray; try JSON/Python list in string; fallback to single-item list."""
    if v is None or (isinstance(v, float) and pd.isna(v)):
        return []
    if isinstance(v, (list, tuple, np.ndarray)):
        return [str(x) for x in v if str(x).strip()]
    s = str(v).strip()
    if not s:
        return []
    if (s.startswith("[") and s.endswith("]")) or (s.startswith("(") and s.endswith(")")):
        parsed = None
        try:
            parsed = json.loads(s)
        except Exception:
            try:
                parsed = ast.literal_eval(s)
            except Exception:
                parsed = None
        if isinstance(parsed, (list, tuple, np.ndarray)):
            return [str(x) for x in parsed if str(x).strip()]
    return [s]

def materialize_parquet_source(input_path: Path, ner_col: str, chunksize: int, tmp_out: Path) -> Path:
    """Ensure we have a Parquet file with a list<string> column `ner_col` from CSV/Excel/Parquet."""
    suffix = input_path.suffix.lower()
    if suffix == ".parquet":
        return input_path

    if suffix == ".csv":
        tmp_out.parent.mkdir(parents=True, exist_ok=True)
        schema = pa.schema([pa.field(ner_col, pa.list_(pa.string()))])
        writer = pq.ParquetWriter(str(tmp_out), schema, compression="zstd")
        for chunk in pd.read_csv(input_path, chunksize=chunksize, dtype=str):
            col = chunk[ner_col] if ner_col in chunk.columns else pd.Series([None] * len(chunk))
            lists = [parse_listish(x) for x in col]
            arr = pa.array(lists, type=pa.list_(pa.string()))
            tbl = pa.Table.from_arrays([arr], names=[ner_col])
            writer.write_table(tbl)
        writer.close()
        return tmp_out

    if suffix in (".xlsx", ".xls"):
        df = pd.read_excel(input_path, dtype=str)
        col = df[ner_col] if ner_col in df.columns else pd.Series([None] * len(df))
        lists = [parse_listish(x) for x in col]
        arr = pa.array(lists, type=pa.list_(pa.string()))
        tbl = pa.Table.from_arrays([arr], names=[ner_col])
        tmp_out.parent.mkdir(parents=True, exist_ok=True)
        pq.write_table(tbl, str(tmp_out), compression="zstd")
        return tmp_out

    raise ValueError(f"Unsupported file type: {input_path.name}")

================================================================================
FILE: preprocess_pipeline\pipeline\ingrnorm\parquet_utils.py
================================================================================

from __future__ import annotations
from collections import Counter
from pathlib import Path
from typing import Dict, Union
import numpy as np
import pyarrow.parquet as pq

def vocab_from_parquet_listcol(path: Union[str, Path], col: str = "NER_clean", min_freq: int = 1) -> Dict[str, int]:
    path = Path(path)
    pf = pq.ParquetFile(path)
    cnt = Counter()
    for rg in range(pf.num_row_groups):
        df = pf.read_row_group(rg, columns=[col]).to_pandas()
        if col not in df.columns:
            continue
        for v in df[col]:
            if isinstance(v, (list, tuple, np.ndarray)):
                for t in v:
                    s = str(t).strip()
                    if s:
                        cnt[s] += 1
    if min_freq > 1:
        cnt = Counter({k: c for k, c in cnt.items() if c >= min_freq})
    return dict(cnt)

================================================================================
FILE: preprocess_pipeline\pipeline\ingrnorm\sbert_dedupe.py
================================================================================

from __future__ import annotations
from pathlib import Path
from typing import Dict, List
import json
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import NearestNeighbors

GENERIC = {"salt", "water", "oil", "sugar"}
def _tokset(s: str) -> set:
    return set(s.split())

def sbert_dedupe(
    vocab_counter: Dict[str, int],
    out_path: Path | str,
    model_name: str = "all-MiniLM-L6-v2",
    threshold: float = 0.88,
    topk: int = 25,
    min_len: int = 2,
    require_token_overlap: bool = True,
    block_generic_as_canon: bool = True,
) -> Dict[str, str]:
    phrases = [p.strip().lower() for p, c in vocab_counter.items() if c > 0]
    freqs   = np.array([vocab_counter[p] for p in phrases], dtype=np.int64)
    if not phrases:
        raise ValueError("sbert_dedupe: empty vocab")

    model = SentenceTransformer(model_name)
    X = model.encode(phrases, normalize_embeddings=True, show_progress_bar=True)

    nn = NearestNeighbors(n_neighbors=min(topk + 1, len(phrases)), metric="cosine", algorithm="auto")
    nn.fit(X)
    dists, idxs = nn.kneighbors(X, return_distance=True)

    # Centroid-based clustering: avoid union-find chaining
    # Sort phrases by frequency (descending) to process most common first
    phrase_indices = list(range(len(phrases)))
    phrase_indices.sort(key=lambda i: (-freqs[i], len(phrases[i]), phrases[i]))
    
    clusters: Dict[int, List[int]] = {}  # cluster_id -> list of phrase indices
    cluster_centroids: Dict[int, int] = {}  # cluster_id -> centroid phrase index
    cluster_vectors: Dict[int, np.ndarray] = {}  # cluster_id -> centroid vector
    next_cluster_id = 0
    
    # Verification threshold: members must be within this distance of centroid
    centroid_verification_threshold = threshold * 0.95  # Slightly stricter than merge threshold
    
    for i in phrase_indices:
        assigned = False
        phrase_i = phrases[i]
        vec_i = X[i]
        
        # Check if phrase_i can join an existing cluster
        for cluster_id, centroid_idx in cluster_centroids.items():
            centroid_vec = cluster_vectors[cluster_id]
            centroid_phrase = phrases[centroid_idx]
            
            # Check similarity to centroid
            cos_sim = float(np.dot(vec_i, centroid_vec))
            if cos_sim < threshold:
                continue
            
            # Check lexical requirements
            if len(phrase_i) < min_len or len(centroid_phrase) < min_len:
                continue
            if require_token_overlap and not (_tokset(phrase_i) & _tokset(centroid_phrase)):
                continue
            
            # Verify member is actually close to centroid (not just a neighbor)
            if cos_sim >= centroid_verification_threshold:
                clusters[cluster_id].append(i)
                assigned = True
                break
        
        # If not assigned, create new cluster with this phrase as centroid
        if not assigned:
            cluster_id = next_cluster_id
            next_cluster_id += 1
            clusters[cluster_id] = [i]
            cluster_centroids[cluster_id] = i
            cluster_vectors[cluster_id] = vec_i

    mapping: Dict[str, str] = {}
    for cluster_id, members in clusters.items():
        # Use centroid as canonical form (already selected as most representative)
        centroid_idx = cluster_centroids[cluster_id]
        canon = phrases[centroid_idx]
        
        # Block generic as canon if there are alternatives
        if block_generic_as_canon and canon in GENERIC and len(members) > 1:
            # Find highest frequency non-generic member
            members_sorted = sorted(members, key=lambda m: (-freqs[m], len(phrases[m]), phrases[m]))
            alt = next((phrases[m] for m in members_sorted if phrases[m] not in GENERIC), canon)
            canon = alt
        
        # Map all members to canonical form
        for m in members:
            mapping[phrases[m]] = canon

    out_path = Path(out_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        for src, tgt in mapping.items():
            if src and tgt and src != tgt:
                f.write(json.dumps({"from": src, "to": tgt}) + "\n")
    return mapping

================================================================================
FILE: preprocess_pipeline\pipeline\ingrnorm\spacy_normalizer.py
================================================================================
from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, List, Set, Union, Sequence

import time
import re
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import spacy
from spacy.tokens import Token, Doc
import logging

DROP_ADJ: Set[str] = {
    "fresh","ripe","frozen","thawed","organic","unsalted","salted","sweetened","unsweetened",
    "minced","chopped","diced","sliced","crushed","ground","grated","shredded","powdered",
    "large","small","medium","extra","virgin","golden","dark","light","skinless","boneless",
}
UNITS: Set[str] = {"cup","cups","tbsp","tablespoon","tablespoons","tsp","teaspoon","teaspoons",
                   "g","kg","oz","ounce","ounces","ml","l","lb","lbs","pound","pounds"}
ALNUM = re.compile(r"[a-z0-9]+")
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Regex patterns for bronze layer cleaning
PARENTHESES_PATTERN = re.compile(r'\([^)]*\)')
COMMERCIAL_SYMBOLS_PATTERN = re.compile(r'[®™©]')
BRAND_ARTIFACTS_PATTERN = re.compile(r'\b(Inc\.|LLC|Ltd\.|Corp\.|Corporation|Company|Co\.)\b', re.IGNORECASE)
MULTISPACE_PATTERN = re.compile(r'\s+')


@dataclass
class SpacyIngredientNormalizer:
    model: str = "en_core_web_sm"
    batch_size: int = 128
    n_process: int = 1  # Multiprocessing: 0=auto, 1=single-threaded, >1=multiprocess (may not work on Windows)

    def __post_init__(self):
        # Disable components we don't need for ingredient normalization
        # We only need tokenizer + parser for dependency parsing
        self.nlp = spacy.load(
            self.model,
            disable=["ner", "textcat", "lemmatizer", "senter", "attribute_ruler"],
        )
        # Optimize for speed: disable unnecessary pipeline components
        # We only need tokenizer and parser for dependency analysis
        Token.set_extension("keep", default=True, force=True)
    
    @staticmethod
    def clean_raw_text(text: str) -> str:
        """
        Bronze layer cleaner: Remove artifacts from raw ingredient text.
        
        Removes:
        - Text inside parentheses (e.g., "Spinach (frozen)" -> "Spinach")
        - Commercial symbols: ®, ™, ©
        - Brand artifacts: Inc., LLC, Ltd., Corp., etc.
        - Collapses multiple spaces to one
        
        Args:
            text: Raw ingredient text
            
        Returns:
            Cleaned text
        """
        if not text or not isinstance(text, str):
            return text if text else ""
        
        # Remove text inside parentheses
        cleaned = PARENTHESES_PATTERN.sub('', text)
        
        # Remove commercial symbols
        cleaned = COMMERCIAL_SYMBOLS_PATTERN.sub('', cleaned)
        
        # Remove brand artifacts
        cleaned = BRAND_ARTIFACTS_PATTERN.sub('', cleaned)
        
        # Collapse multiple spaces to one
        cleaned = MULTISPACE_PATTERN.sub(' ', cleaned)
        
        return cleaned.strip()

    def _basic_tokens(self, s: str) -> List[str]:
        return ALNUM.findall(s.lower())

    def _is_unit_or_number(self, t: str) -> bool:
        return t.isdigit() or t in UNITS

    def _normalize_doc(self, doc: Doc, raw: str) -> Optional[str]:
        """Core normalize logic, reusing an existing Doc."""
        # Apply bronze layer cleaning first
        raw = self.clean_raw_text(raw)
        s = raw.strip().lower()
        if not s:
            return None

        head: Optional[Token] = None
        for token in doc:
            if token.dep_ == "ROOT":
                head = token
                break
        if head is None:
            return None

        compounds = [t for t in head.lefts if t.dep_ in ("compound",)]
        parts: List[str] = []
        for t in compounds + [head]:
            tok = t.text.lower()
            if tok in DROP_ADJ:
                continue
            if self._is_unit_or_number(tok):
                continue
            if not ALNUM.fullmatch(tok):
                continue
            parts.append(tok)

        # Special cases like "powder", "sauce", etc.
        if len(parts) == 1 and parts[0] in {"powder","sauce","paste","oil","vinegar","cheese"}:
            for child in head.lefts:
                if child.dep_ in ("amod","compound") and ALNUM.fullmatch(child.text.lower()):
                    mod = child.text.lower()
                    if mod not in DROP_ADJ:
                        parts.insert(0, mod)
                        break

        if not parts:
            toks = [
                t for t in self._basic_tokens(s)
                if t not in DROP_ADJ and not self._is_unit_or_number(t)
            ]
            if not toks:
                return None
            parts = toks[-2:] if len(toks) >= 2 else toks

        return " ".join(parts).strip()

    def _normalize_phrase(self, s: str) -> Optional[str]:
        """Single-phrase wrapper; still used by normalize_list."""
        # Apply bronze layer cleaning first
        s = self.clean_raw_text(s)
        s = s.strip()
        if not s:
            return None
        doc = self.nlp(s.lower())
        return self._normalize_doc(doc, s)

    def normalize_list(self, lst: Union[List[str], np.ndarray, tuple]) -> List[str]:
        """Existing API: normalize a single list of ingredient strings."""
        if not isinstance(lst, (list, tuple, np.ndarray)):
            return []
        out: List[str] = []
        seen: Set[str] = set()
        for x in lst:
            norm = self._normalize_phrase(str(x))
            if norm and norm not in seen:
                out.append(norm)
                seen.add(norm)
        return out

    def normalize_batch(
        self,
        lists: Sequence[Union[List[str], np.ndarray, tuple]],
    ) -> List[List[str]]:
        """
        New batched API: normalize many ingredient lists at once using nlp.pipe.
        Returns a list of normalized lists, same length as `lists`.
        """
        # Flatten all phrases and remember boundaries
        flat_phrases: List[str] = []
        boundaries: List[tuple[int, int]] = []

        for lst in lists:
            if not isinstance(lst, (list, tuple, np.ndarray)):
                # mimic normalize_list: non-list -> empty output
                boundaries.append((len(flat_phrases), len(flat_phrases)))
                continue
            start = len(flat_phrases)
            for x in lst:
                # Apply bronze layer cleaning before processing
                cleaned = SpacyIngredientNormalizer.clean_raw_text(str(x))
                flat_phrases.append(cleaned)
            end = len(flat_phrases)
            boundaries.append((start, end))

        if not flat_phrases:
            # no work to do
            return [[] for _ in lists]

        # Run spaCy once over all phrases
        logger.info(f"[spacy_norm] normalize_batch: {len(flat_phrases)} phrases total (batch_size={self.batch_size}, n_process={self.n_process})")
        # Pre-lowercase all texts for better performance
        texts = [p.lower() for p in flat_phrases]
        
        # Process with spaCy pipe - use list() to materialize all docs at once
        # This is more efficient than iterating one-by-one
        # n_process: 0 or None = auto-detect, 1 = single-threaded, >1 = multiprocess
        n_proc = self.n_process if self.n_process > 0 else 1  # Default to 1 for safety
        docs = list(self.nlp.pipe(
            texts,
            batch_size=self.batch_size,
            n_process=n_proc,
        ))

        # Compute normalized forms for each phrase (parallelized if n_process > 1)
        flat_norms: List[Optional[str]] = []
        for raw, doc in zip(flat_phrases, docs):
            flat_norms.append(self._normalize_doc(doc, raw))

        # Rebuild per-list outputs with deduping
        out_lists: List[List[str]] = []
        for start, end in boundaries:
            seen: Set[str] = set()
            cur: List[str] = []
            for i in range(start, end):
                norm = flat_norms[i]
                if norm and norm not in seen:
                    cur.append(norm)
                    seen.add(norm)
            out_lists.append(cur)

        return out_lists


def apply_spacy_normalizer_to_parquet(
    in_parquet: Union[str, Path],
    out_parquet: Union[str, Path],
    list_col: str = "NER",
    out_col: str = "NER_clean",
    compression: str = "zstd",
    spacy_model: str = "en_core_web_sm",
    batch_size: int = 512,  # Increased batch size for better throughput
    n_process: int = 0,  # 0=auto-detect (uses all CPUs), 1=single-threaded
) -> Path:
    """
    Apply spaCy normalization to parquet file.
    
    Args:
        n_process: Number of processes (0=auto, 1=single-threaded, >1=multiprocess)
                  Note: Multiprocessing may not work on Windows due to spawn method.
    """
    normalizer = SpacyIngredientNormalizer(spacy_model, batch_size=batch_size, n_process=n_process)
    pf = pq.ParquetFile(str(in_parquet))
    out_parquet = Path(out_parquet)
    out_parquet.parent.mkdir(parents=True, exist_ok=True)

    writer = None
    schema = pa.schema([pa.field(out_col, pa.list_(pa.string()))])

    total_row_groups = pf.num_row_groups
    for rg in range(total_row_groups):
        t0 = time.time()
        logger.info(f"[spacy_norm] RG{rg+1}/{total_row_groups} start")

        # Only read the column we need
        tbl = pf.read_row_group(rg, columns=[list_col])
        df = tbl.to_pandas()
        n_rows = len(df)
        logger.info(f"[spacy_norm] RG{rg+1}/{total_row_groups}: {n_rows:,} rows")

        if list_col not in df.columns:
            df[list_col] = [[] for _ in range(n_rows)]

        # Ensure we pass list-like objects (or [] for bad types) into normalize_batch
        # Use list comprehension for better performance
        lists = [
            list(x) if isinstance(x, (list, tuple, np.ndarray)) else []
            for x in df[list_col]
        ]

        t1 = time.time()
        logger.info(f"[spacy_norm] RG{rg+1}/{total_row_groups}: running normalizer.normalize_batch…")
        cleaned_lists = normalizer.normalize_batch(lists)
        elapsed = time.time() - t1
        logger.info(f"[spacy_norm] RG{rg+1}/{total_row_groups}: normalizer step took {elapsed:.1f}s ({n_rows/elapsed:.0f} rows/sec)")

        df[out_col] = cleaned_lists

        table = pa.Table.from_pandas(df[[out_col]], preserve_index=False).replace_schema_metadata(None)
        if table.schema != schema:
            table = table.cast(schema, safe=False)

        if writer is None:
            writer = pq.ParquetWriter(str(out_parquet), schema, compression=compression)
        writer.write_table(table)

        logger.info(f"[spacy_norm] RG{rg}: total row group time {time.time() - t0:.1f}s")

    if writer:
        writer.close()
        
    return out_parquet

================================================================================
FILE: preprocess_pipeline\pipeline\ingrnorm\w2v_dedupe.py
================================================================================


from __future__ import annotations
import json
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
from rapidfuzz.distance import Levenshtein

def _iter_parquet_tokenized_docs(parquet_path: Union[str, Path], list_col: str = "NER_clean") -> Iterable[List[str]]:
    pf = pq.ParquetFile(str(parquet_path))
    for rg in range(pf.num_row_groups):
        tbl = pf.read_row_group(rg, columns=[list_col])
        df = tbl.to_pandas()
        if list_col not in df.columns:
            continue
        for lst in df[list_col]:
            items: List[str] = []
            if isinstance(lst, (list, tuple, np.ndarray)):
                items = [str(x) for x in lst if str(x).strip()]
            elif isinstance(lst, str):
                s = lst.strip()
                if s and (s.startswith("[") and s.endswith("]")):
                    try:
                        parsed = json.loads(s)
                        if isinstance(parsed, list):
                            items = [str(x) for x in parsed if str(x).strip()]
                    except Exception:
                        pass
                if not items and s:
                    items = [s]
            if not items:
                continue
            tokens: List[str] = []
            for phrase in items:
                tokens.extend(simple_preprocess(str(phrase), deacc=True, min_len=2))
            if tokens:
                yield tokens

def train_or_load_w2v(
    corpus_parquet: Union[str, Path],
    list_col: str = "NER_clean",
    model_cache_path: Optional[Union[str, Path]] = None,
    vector_size: int = 100, window: int = 5, min_count: int = 1, workers: int = 4, sg: int = 1, epochs: int = 8,
) -> Word2Vec:
    """Train or load Word2Vec model. Uses streaming iterator to avoid loading all sentences into memory."""
    model_cache_path = Path(model_cache_path) if model_cache_path else None
    if model_cache_path and model_cache_path.exists():
        return Word2Vec.load(str(model_cache_path))
    
    # Use streaming iterator instead of loading all into memory
    sentences_iter = _iter_parquet_tokenized_docs(corpus_parquet, list_col=list_col)
    model = Word2Vec(
        sentences=sentences_iter,  # Word2Vec accepts iterables directly
        vector_size=vector_size, 
        window=window, 
        min_count=min_count, 
        workers=workers, 
        sg=sg, 
        epochs=epochs,
    )
    if model_cache_path:
        model_cache_path.parent.mkdir(parents=True, exist_ok=True)
        model.save(str(model_cache_path))
    return model

def _tokenize_phrase(phrase: str) -> List[str]:
    return simple_preprocess(str(phrase), deacc=True, min_len=2)

# Stop words to exclude from token overlap check
STOP_WORDS = {"salt", "water", "oil", "sugar", "pepper", "flour", "butter", "garlic", "onion"}

def _tokenize_excluding_stops(phrase: str) -> set:
    """Tokenize phrase and return set of tokens excluding stop words."""
    tokens = _tokenize_phrase(phrase)
    return {t for t in tokens if t not in STOP_WORDS}

def _has_token_overlap(phrase_a: str, phrase_b: str) -> bool:
    """Check if two phrases share at least one non-stop-word token."""
    tokens_a = _tokenize_excluding_stops(phrase_a)
    tokens_b = _tokenize_excluding_stops(phrase_b)
    return bool(tokens_a & tokens_b)

def _levenshtein_ratio(phrase_a: str, phrase_b: str) -> float:
    """Compute normalized Levenshtein similarity ratio (0-1, higher = more similar)."""
    if not phrase_a or not phrase_b:
        return 0.0
    # Use rapidfuzz for normalized similarity (0-1 scale)
    distance = Levenshtein.distance(phrase_a.lower(), phrase_b.lower())
    max_len = max(len(phrase_a), len(phrase_b))
    if max_len == 0:
        return 1.0
    return 1.0 - (distance / max_len)

def _should_merge(phrase_a: str, phrase_b: str, levenshtein_threshold: float = 0.7) -> bool:
    """
    Determine if two phrases should be merged based on lexical guardrails.
    
    Merges if:
    - They share at least one non-stop-word token, OR
    - Their Levenshtein similarity is above threshold
    
    Args:
        phrase_a: First phrase
        phrase_b: Second phrase
        levenshtein_threshold: Minimum Levenshtein similarity ratio (0-1)
        
    Returns:
        True if phrases should be merged
    """
    # Check token overlap
    if _has_token_overlap(phrase_a, phrase_b):
        return True
    
    # Check Levenshtein distance
    lev_ratio = _levenshtein_ratio(phrase_a, phrase_b)
    if lev_ratio >= levenshtein_threshold:
        return True
    
    return False

def phrase_vector(model: Word2Vec, phrase: str, oov_policy: str = "skip") -> Optional[np.ndarray]:
    toks = _tokenize_phrase(phrase)
    vecs = [model.wv[t] for t in toks if t in model.wv]
    if not vecs:
        return None if oov_policy != "zeros" else np.zeros(model.vector_size, dtype=np.float32)
    v = np.mean(np.stack(vecs, 0), 0)
    n = np.linalg.norm(v) + 1e-12
    return (v / n).astype(np.float32)

def _topk_similar_indices(M: np.ndarray, k: int = 25, self_exclude: bool = True, chunk_size: int = 2048) -> Tuple[np.ndarray, np.ndarray]:
    n, d = M.shape
    k = min(k, n - 1 if self_exclude else n)
    topk_idx = np.empty((n, k), dtype=np.int32)
    topk_sc  = np.empty((n, k), dtype=np.float32)
    for start in range(0, n, chunk_size):
        end = min(start + chunk_size, n)
        block = M[start:end]
        sims = block @ M.T
        if self_exclude:
            rows = np.arange(start, end)
            sims[np.arange(end - start), rows] = -np.inf
        part_idx = np.argpartition(-sims, kth=k - 1, axis=1)[:, :k]
        part_scores = np.take_along_axis(sims, part_idx, axis=1)
        order = np.argsort(-part_scores, axis=1)
        block_topk_idx = np.take_along_axis(part_idx, order, axis=1).astype(np.int32)
        block_topk_sc  = np.take_along_axis(part_scores, order, axis=1).astype(np.float32)
        topk_idx[start:end] = block_topk_idx
        topk_sc[start:end]  = block_topk_sc
    return topk_idx, topk_sc

def _uf_build(n: int): return list(range(n))
def _uf_find(p, a): 
    while p[a] != a:
        p[a] = p[p[a]]; a = p[a]
    return a
def _uf_union(p, a, b):
    ra, rb = _uf_find(p, a), _uf_find(p, b)
    if ra != rb: p[rb] = ra

def w2v_dedupe(
    vocab_counter: Dict[str, int],
    *, corpus_parquet: Union[str, Path], list_col: str = "NER_clean",
    model_cache_path: Optional[Union[str, Path]] = None,
    vector_size: int = 100, window: int = 5, min_count: int = 1, workers: int = 4, sg: int = 1, epochs: int = 8,
    threshold: float = 0.85, topk: int = 25, out_path: Union[str, Path] = Path("../data/w2v_dedupe_map.jsonl"),
) -> Dict[str, str]:
    phrases = [p for p, c in vocab_counter.items() if c > 0]
    if not phrases:
        raise ValueError("w2v_dedupe: empty vocabulary.")
    try:
        model = train_or_load_w2v(corpus_parquet=corpus_parquet, list_col=list_col, model_cache_path=model_cache_path,
                                  vector_size=vector_size, window=window, min_count=min_count, workers=workers, sg=sg, epochs=epochs)
    except ValueError:
        from gensim.utils import simple_preprocess
        sentences = [simple_preprocess(p, deacc=True, min_len=2) for p in phrases]
        sentences = [s for s in sentences if s]
        model = Word2Vec(sentences=sentences, vector_size=vector_size, window=window, min_count=1, workers=workers, sg=sg, epochs=max(epochs, 5))

    vecs, kept = [], []
    for ph in phrases:
        v = phrase_vector(model, ph, oov_policy="skip")
        if v is None: continue
        kept.append(ph); vecs.append(v)
    if not vecs:
        raise ValueError("w2v_dedupe: no phrases had in-vocab tokens.")

    M = np.stack(vecs, 0).astype(np.float32)
    nbr_idx, nbr_sc = _topk_similar_indices(M, k=topk, self_exclude=True, chunk_size=2048)
    
    # Centroid-based clustering: avoid union-find chaining
    # Sort phrases by frequency (descending) to process most common first
    freqs = vocab_counter
    phrase_indices = list(range(len(kept)))
    phrase_indices.sort(key=lambda i: (-freqs.get(kept[i], 0), len(kept[i]), kept[i]))
    
    clusters: Dict[int, List[int]] = {}  # cluster_id -> list of phrase indices
    cluster_centroids: Dict[int, int] = {}  # cluster_id -> centroid phrase index
    cluster_vectors: Dict[int, np.ndarray] = {}  # cluster_id -> centroid vector
    next_cluster_id = 0
    
    # Levenshtein threshold for lexical guardrails (0.7 = 70% similarity)
    levenshtein_threshold = 0.7
    # Verification threshold: members must be within this distance of centroid
    centroid_verification_threshold = threshold * 0.95  # Slightly stricter than merge threshold
    
    for i in phrase_indices:
        assigned = False
        phrase_i = kept[i]
        vec_i = M[i]
        
        # Check if phrase_i can join an existing cluster
        for cluster_id, centroid_idx in cluster_centroids.items():
            centroid_vec = cluster_vectors[cluster_id]
            centroid_phrase = kept[centroid_idx]
            
            # Check similarity to centroid
            cos_sim = float(np.dot(vec_i, centroid_vec))
            if cos_sim < threshold:
                continue
            
            # Lexical guardrail: check token overlap or Levenshtein distance
            if not _should_merge(phrase_i, centroid_phrase, levenshtein_threshold):
                continue
            
            # Verify member is actually close to centroid (not just a neighbor)
            if cos_sim >= centroid_verification_threshold:
                clusters[cluster_id].append(i)
                assigned = True
                break
        
        # If not assigned, create new cluster with this phrase as centroid
        if not assigned:
            cluster_id = next_cluster_id
            next_cluster_id += 1
            clusters[cluster_id] = [i]
            cluster_centroids[cluster_id] = i
            cluster_vectors[cluster_id] = vec_i
    
    # Build mapping: all members map to their cluster centroid
    mapping = {}
    for cluster_id, members in clusters.items():
        centroid_idx = cluster_centroids[cluster_id]
        canon = kept[centroid_idx]
        for i in members:
            mapping[kept[i]] = canon
    
    # Ensure all phrases have a mapping (even if not clustered)
    for ph in phrases:
        mapping.setdefault(ph, ph)

    out_path = Path(out_path); out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        for frm, to in mapping.items():
            f.write(json.dumps({"from": frm, "to": to}) + "\n")
    return mapping


================================================================================
FILE: preprocess_pipeline\pipeline\scripts\apply_ingredient_ner.py
================================================================================
# pipeline/scripts/apply_ingredient_ner.py
"""
Apply trained ingredient NER model to a new dataset.

This script:
1. Loads a trained spaCy NER model
2. Runs NER on a text column to extract ingredient entities
3. Normalizes and canonicalizes ingredients using dedupe map
4. Optionally maps to ingredient IDs using encoder maps
5. Outputs wide and tall format Parquet files

Example usage:
    python pipeline/scripts/apply_ingredient_ner.py \
        --config pipeline/config/ingredient_ner.yaml \
        --in-path data/new_dataset.parquet \
        --text-col ingredients_raw \
        --out-base data/new_dataset_ner
"""
from __future__ import annotations

from pathlib import Path
import argparse
import sys as _sys
import logging

# Make `pipeline/` importable when running from repo root
_SYS_PATH_ROOT = Path.cwd() / "pipeline"
if str(_SYS_PATH_ROOT) not in _sys.path:
    _sys.path.append(str(_SYS_PATH_ROOT))

import yaml

from common.logging_setup import setup_logging
from ingredient_ner.config import load_inference_configs_from_dict, DATA, OUT
from ingredient_ner.inference import run_full_inference_from_config
from ingredient_ner.utils import configure_device

logger = logging.getLogger(__name__)


def main() -> None:
    ap = argparse.ArgumentParser(
        description="Apply trained ingredient NER model to a new dataset",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage with config defaults
  python pipeline/scripts/apply_ingredient_ner.py --text-col ingredients_raw --out-base data/output_ner

  # Override input path and sample
  python pipeline/scripts/apply_ingredient_ner.py \\
      --in-path data/new_dataset.parquet \\
      --text-col ingredients_raw \\
      --out-base data/new_dataset_ner \\
      --sample-n 1000

  # Process first 100 rows
  python pipeline/scripts/apply_ingredient_ner.py \\
      --text-col ingredients_raw \\
      --out-base data/output_ner \\
      --head-n 100
        """,
    )
    
    ap.add_argument(
        "--config",
        type=str,
        default="pipeline/config/ingredient_ner_inference.yaml",
        help="Path to ingredient NER inference config YAML",
    )
    ap.add_argument(
        "--in-path",
        type=str,
        default=None,
        help="Input dataset path (CSV or Parquet). Overrides config data.input_path.",
    )
    ap.add_argument(
        "--text-col",
        type=str,
        default=None,
        help="Column name containing raw ingredient text to process. Overrides config inference.text_col.",
    )
    ap.add_argument(
        "--out-base",
        type=str,
        default=None,
        help="Base path for output files (will write <base>_wide.parquet and <base>_tall.parquet). Overrides config output.out_base.",
    )
    
    # Sampling options (mutually exclusive)
    sampling_group = ap.add_mutually_exclusive_group()
    sampling_group.add_argument(
        "--sample-n",
        type=int,
        default=None,
        help="Randomly sample N rows",
    )
    sampling_group.add_argument(
        "--sample-frac",
        type=float,
        default=None,
        help="Randomly sample fraction of rows (0.0-1.0)",
    )
    sampling_group.add_argument(
        "--head-n",
        type=int,
        default=None,
        help="Take first N rows",
    )
    
    # Performance options
    ap.add_argument(
        "--batch-size",
        type=int,
        default=256,
        help="Batch size for spaCy processing (default: 256)",
    )
    ap.add_argument(
        "--n-process",
        type=int,
        default=1,
        help="Number of processes for spaCy (default: 1). "
             "Note: >1 may not work on Windows with transformers. Use at your own risk.",
    )
    ap.add_argument(
        "--use-gpu",
        action="store_true",
        help="Attempt to use GPU (default: CPU). May not work on Windows.",
    )
    
    args = ap.parse_args()

    # Load config first to get defaults
    config_path = Path(args.config)
    if not config_path.exists():
        raise FileNotFoundError(f"Config not found: {config_path}")
    
    with open(config_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if not isinstance(cfg, dict):
        raise ValueError(f"YAML at {config_path} did not parse to a dict.")

    # Setup logging
    setup_logging(cfg)
    logger.info("=" * 60)
    logger.info("Applying Ingredient NER Model")
    logger.info("=" * 60)

    # Populate DATA / OUT globals from inference config
    load_inference_configs_from_dict(cfg)
    
    # Get inference settings from config
    inference_cfg = cfg.get("inference") or {}
    
    # Determine text column (CLI overrides config)
    text_col = args.text_col or inference_cfg.get("text_col")
    if not text_col:
        raise ValueError(
            "text_col must be specified either via --text-col argument "
            "or in config as inference.text_col"
        )
    
    # Determine output base (CLI overrides config)
    out_base = args.out_base or str(OUT.PRED_OUT)
    
    # Determine input path (CLI overrides config)
    if args.in_path:
        in_path = Path(args.in_path)
    elif DATA.TRAIN_PATH:
        in_path = DATA.TRAIN_PATH
        logger.info(f"Using input path from config: {in_path}")
    else:
        raise ValueError(
            "Input path must be specified either via --in-path argument "
            "or in config as data.input_path"
        )
    
    if not in_path.exists():
        raise FileNotFoundError(f"Input data not found: {in_path}")
    
    # Get normalization settings (CLI overrides config)
    use_spacy_normalizer = inference_cfg.get("use_spacy_normalizer", True)
    spacy_model = inference_cfg.get("spacy_model", "en_core_web_sm")
    
    # Get performance settings (CLI overrides config)
    # Use CLI args if provided, otherwise use config values
    batch_size = args.batch_size if args.batch_size != 256 else inference_cfg.get("batch_size", 256)
    n_process = args.n_process if args.n_process != 1 else inference_cfg.get("n_process", 1)
    use_gpu = args.use_gpu or inference_cfg.get("use_gpu", False)
    
    # Get sampling settings (CLI overrides config)
    sample_n = args.sample_n or inference_cfg.get("sample_n")
    sample_frac = args.sample_frac or inference_cfg.get("sample_frac")
    head_n = args.head_n or inference_cfg.get("head_n")
    
    # Configure device (CPU by default, GPU if requested)
    if use_gpu:
        configure_device()
    else:
        import spacy
        spacy.prefer_gpu()
        logger.info("Using CPU (set inference.use_gpu: true in config or use --use-gpu to attempt GPU acceleration)")
    
    logger.info(f"Input: {in_path}")
    logger.info(f"Text column: {text_col}")
    logger.info(f"Output base: {out_base}")
    logger.info(f"Model: {OUT.MODEL_DIR}")
    logger.info(f"Batch size: {batch_size}, Processes: {n_process}")
    
    if not OUT.MODEL_DIR.exists():
        raise FileNotFoundError(
            f"Model directory not found: {OUT.MODEL_DIR}. "
            f"Train a model first using: python pipeline/scripts/run_ingredient_ner.py"
        )

    # Validate sampling options
    sampling_count = sum([
        sample_n is not None,
        sample_frac is not None,
        head_n is not None,
    ])
    if sampling_count > 1:
        raise ValueError("Only one sampling option (--sample-n, --sample-frac, --head-n) can be used at a time")

    # Run inference
    logger.info("Starting inference...")
    try:
        df_wide, df_tall = run_full_inference_from_config(
            text_col=text_col,
            out_base=Path(out_base),
            data_path=in_path,
            sample_n=sample_n,
            sample_frac=sample_frac,
            head_n=head_n,
            batch_size=batch_size,
            n_process=n_process,
            use_spacy_normalizer=use_spacy_normalizer,
            spacy_model=spacy_model,
        )
        print(df_wide.head())
        print(df_tall.head())
        logger.info("=" * 60)
        logger.info("Inference Complete")
        logger.info("=" * 60)
        logger.info(f"Processed {len(df_wide)} rows")
        logger.info(f"Extracted {len(df_tall)} ingredient entities")
        logger.info(f"Output files:")
        logger.info(f"  - {Path(out_base).with_name(Path(out_base).stem + '_wide.parquet')}")
        logger.info(f"  - {Path(out_base).with_name(Path(out_base).stem + '_tall.parquet')}")
        
    except Exception as e:
        logger.error(f"Inference failed: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()

================================================================================
FILE: preprocess_pipeline\pipeline\scripts\combine_raw_dataset.py
================================================================================
"""
Script to combine multiple raw datasets into a unified format.

Processes CSV files from data/raw, extracts ingredients column,
and combines them into a single DataFrame with Dataset_ID, index, ingredients, cuisine.
"""

import argparse
import json
import logging
import re
from pathlib import Path
from typing import List, Optional
import sys as _sys

import pandas as pd
# Make `pipeline/` importable when running from repo root
_SYS_PATH_ROOT = Path.cwd() / "pipeline"
if str(_SYS_PATH_ROOT) not in _sys.path:
    _sys.path.append(str(_SYS_PATH_ROOT))

from common.logging_setup import setup_logging

# Import inference functionality
try:
    from ingredient_ner.inference import run_full_inference_from_config
    from ingredient_ner.config import load_inference_configs_from_yaml, DATA, OUT
    _HAS_INFERENCE = True
except ImportError as e:
    _HAS_INFERENCE = False
    print(f"Warning: Could not import inference modules: {e}")


def read_csv_with_fallback(path: Path, logger: logging.Logger) -> pd.DataFrame:
    """
    Tries to read CSV using UTF-8, then cp1252 (windows-1252), then latin-1.
    Returns a DataFrame and the encoding used (string).
    """
    encodings = ["utf-8", "cp1252", "latin-1"]
    for enc in encodings:
        try:
            logger.debug(f"Attempting to read {path.name} with encoding={enc}")
            df = pd.read_csv(path, encoding=enc, dtype=str, low_memory=False)
            logger.info(f"Successfully read {path.name} with encoding={enc} ({len(df):,} rows)")
            return df
        except UnicodeDecodeError as e:
            logger.warning(f"Failed to read {path.name} with encoding={enc}: {e}")
            continue
        except Exception as e:
            logger.exception(f"Unexpected error reading {path.name} with encoding={enc}: {e}")
            raise

    # Last resort: read with errors='replace'
    logger.warning(f"All standard encodings failed for {path.name}. Trying utf-8 with errors='replace'.")
    df = pd.read_csv(path, encoding="utf-8", errors='replace', dtype=str, low_memory=False)
    logger.info(f"Read {path.name} with utf-8 (errors=replace) ({len(df):,} rows)")
    return df


def find_ingredients_column(df: pd.DataFrame, logger: logging.Logger) -> Optional[str]:
    """Find the ingredients column (case-insensitive)."""
    cols_lower = {col.lower(): col for col in df.columns}
    
    # Common variations
    candidates = [
        "ingredients",
        "ingredient",
        "ing",
        "ingr",
        "ingredient_list",
        "ingredients_list",
    ]
    
    for candidate in candidates:
        if candidate in cols_lower:
            actual_col = cols_lower[candidate]
            logger.info(f"Found ingredients column: '{actual_col}' (matched '{candidate}')")
            return actual_col
    
    # If not found, log available columns
    logger.warning(f"Ingredients column not found. Available columns: {list(df.columns)[:10]}")
    return None


def process_dataset(
    csv_path: Path,
    dataset_id: int,
    logger: logging.Logger,
    cuisine_default: str = "unknown",
) -> Optional[pd.DataFrame]:
    """
    Process a single CSV dataset.
    
    Returns:
        DataFrame with columns: Dataset_ID, index, ingredients, cuisine
        or None if processing fails
    """
    try:
        # Read CSV
        df = read_csv_with_fallback(csv_path, logger)
        
        if df.empty:
            logger.warning(f"{csv_path.name} is empty, skipping")
            return None
        
        # Convert column names to lowercase
        df.columns = df.columns.str.lower()
        
        # Find ingredients column
        ing_col = find_ingredients_column(df, logger)
        if ing_col is None:
            logger.error(f"Could not find ingredients column in {csv_path.name}, skipping")
            return None
        
        # Try to find cuisine column (case-insensitive)
        cuisine_col = None
        cols_lower = {col.lower(): col for col in df.columns}
        cuisine_candidates = ["cuisine", "cuisines", "country", "countries", "type", "category", "region"]
        for candidate in cuisine_candidates:
            if candidate in cols_lower:
                cuisine_col = cols_lower[candidate]
                logger.info(f"Found cuisine column: '{cuisine_col}' (matched '{candidate}')")
                break
        
        # Debug: log all columns if no cuisine column found
        if cuisine_col is None:
            logger.warning(f"No cuisine column found in {csv_path.name}. Available columns: {list(df.columns)}")
        
        # Extract cuisine values if column exists, otherwise use default
        if cuisine_col:
            # Handle list-like cuisine values (e.g., "[American]" -> "American")
            cuisine_values = df[cuisine_col].astype(str)
            # Try to parse as list and extract first element
            def extract_cuisine(val):
                s = str(val).strip()
                if not s or s.lower() in ["nan", "none", ""]:
                    return cuisine_default
                # Check for empty list string
                if s == "[]" or s.lower() == "[]":
                    return cuisine_default
                # Try to parse as list
                if s.startswith("[") and s.endswith("]"):
                    try:
                        import ast
                        parsed = ast.literal_eval(s)
                        if isinstance(parsed, list):
                            if len(parsed) == 0:
                                return cuisine_default
                            # Extract all non-empty elements and join them
                            non_empty = [str(x).strip() for x in parsed if str(x).strip() and str(x).strip().lower() not in ["nan", "none", ""]]
                            if non_empty:
                                # Return as list string format for consistency
                                return str(non_empty) if len(non_empty) > 1 else non_empty[0]
                            else:
                                return cuisine_default
                    except Exception as e:
                        logger.debug(f"Could not parse cuisine value '{s}' as list: {e}")
                        # If parsing fails, return the string as-is (might be a single cuisine)
                        pass
                return s if s else cuisine_default
            cuisine_series = cuisine_values.apply(extract_cuisine)
            
            # Debug: log cuisine extraction stats
            non_default = (cuisine_series != cuisine_default).sum()
            logger.info(f"Extracted cuisine values: {non_default:,} non-default out of {len(cuisine_series):,} rows")
            if non_default > 0:
                sample_cuisines = cuisine_series[cuisine_series != cuisine_default].head(5).tolist()
                logger.debug(f"Sample cuisine values: {sample_cuisines}")
        else:
            logger.info(f"No cuisine column found in {csv_path.name}, using default: '{cuisine_default}'")
            cuisine_series = pd.Series([cuisine_default] * len(df), dtype=str)
        
        # If cuisine is empty/default, try to extract from all text columns
        empty_cuisine_mask = (cuisine_series == cuisine_default) | (cuisine_series.astype(str).str.strip() == "[]")
        if empty_cuisine_mask.sum() > 0:
            # Load known cuisine vocabulary for matching
            cuisine_vocab_path = Path("./data/cuisine_encoded/cuisine_token_to_id.json")
            known_cuisines = set()
            if cuisine_vocab_path.exists():
                try:
                    with open(cuisine_vocab_path, "r", encoding="utf-8") as f:
                        cuisine_vocab = json.load(f)
                        # Get all cuisine names (lowercase for matching)
                        known_cuisines = {k.lower().strip() for k in cuisine_vocab.keys() if k != "<UNK>"}
                        logger.info(f"Loaded {len(known_cuisines)} known cuisines from vocabulary for text matching")
                except Exception as e:
                    logger.warning(f"Could not load cuisine vocabulary: {e}")
            
            # Also add common cuisine/country names that might appear in text
            common_cuisines = {
                "american", "mexican", "italian", "chinese", "japanese", "indian", "thai",
                "french", "greek", "spanish", "mediterranean", "asian", "korean", "vietnamese",
                "german", "british", "irish", "cuban", "caribbean", "brazilian", "african",
                "middle eastern", "middleeastern", "arab", "arabic", "persian", "turkish",
                "moroccan", "lebanese", "israeli", "jewish", "scandinavian", "nordic",
                "eastern european", "russian", "polish", "cajun", "creole", "southern",
                "tex-mex", "texmex", "latin", "south american", "peruvian", "argentinian"
            }
            known_cuisines.update(common_cuisines)
            
            # Identify text columns to search (exclude cuisine column and ingredients column)
            text_columns = []
            exclude_cols = {ing_col.lower(), "cuisine", "cuisines", "country", "countries"}
            for col in df.columns:
                col_lower = col.lower()
                if col_lower not in exclude_cols:
                    # Check if column contains text data
                    sample_val = df[col].dropna().head(1)
                    if len(sample_val) > 0:
                        val_str = str(sample_val.iloc[0])
                        # Consider it a text column if it's not purely numeric and has reasonable length
                        if len(val_str) > 3 and not val_str.replace(".", "").replace("-", "").isdigit():
                            text_columns.append(col)
            
            logger.info(f"Searching for cuisine in {len(text_columns)} text columns: {text_columns[:5]}{'...' if len(text_columns) > 5 else ''}")
            
            def extract_cuisine_from_text(row_data, cuisine_val):
                """Extract cuisine from any text column if cuisine is empty/default."""
                # If cuisine already has a value, use it
                if cuisine_val and cuisine_val != cuisine_default and str(cuisine_val).strip() not in ["", "[]", "nan"]:
                    return cuisine_val
                
                found_cuisines = []
                
                # Search through all text columns
                for col in text_columns:
                    if col not in row_data.index:
                        continue
                    
                    text_val = row_data[col]
                    if pd.isna(text_val) or not str(text_val).strip():
                        continue
                    
                    text_str = str(text_val).lower()
                    
                    # Handle list-like strings (e.g., tags, categories)
                    if text_str.startswith("[") and text_str.endswith("]"):
                        try:
                            import ast
                            parsed = ast.literal_eval(str(text_val))
                            if isinstance(parsed, list):
                                text_parts = [str(t).strip().lower() for t in parsed]
                            else:
                                text_parts = [text_str]
                        except:
                            # Split by comma if not a valid list
                            text_parts = [t.strip().lower() for t in text_str.split(",")]
                    else:
                        # For regular text, split by common delimiters and also search as whole
                        text_parts = [text_str]
                        # Also add comma-split parts
                        text_parts.extend([t.strip().lower() for t in text_str.split(",")])
                        # Also add space-split parts (for recipe names, descriptions)
                        text_parts.extend([t.strip().lower() for t in text_str.split()])
                    
                    # Check each text part against known cuisines
                    for text_part in text_parts:
                        text_clean = text_part.strip()
                        if not text_clean or len(text_clean) < 3:
                            continue
                        
                        # Direct exact match
                        if text_clean in known_cuisines:
                            if text_clean not in found_cuisines:
                                found_cuisines.append(text_clean)
                        else:
                            # Word boundary matching (cuisine name as whole word)
                            for known_cuisine in known_cuisines:
                                # Use word boundaries to avoid partial matches (e.g., "indian" in "indiana")
                                pattern = r'\b' + re.escape(known_cuisine) + r'\b'
                                if re.search(pattern, text_clean):
                                    if known_cuisine not in found_cuisines:
                                        found_cuisines.append(known_cuisine)
                                    break
                            
                            # Also check substring match (but prefer word boundary matches)
                            if not found_cuisines:
                                for known_cuisine in known_cuisines:
                                    if known_cuisine in text_clean or text_clean in known_cuisine:
                                        if known_cuisine not in found_cuisines:
                                            found_cuisines.append(known_cuisine)
                                        break
                    
                    # If we found a match, stop searching (take first match)
                    if found_cuisines:
                        break
                
                if found_cuisines:
                    # Return first match, or list if multiple
                    return found_cuisines[0] if len(found_cuisines) == 1 else str(found_cuisines)
                return cuisine_default
            
            # Apply text-based extraction for rows with empty cuisine
            logger.info(f"Attempting to extract cuisine from all text columns for {empty_cuisine_mask.sum():,} rows with empty cuisine...")
            cuisine_from_text = pd.Series([
                extract_cuisine_from_text(df.iloc[i], cuisine_series.iloc[i])
                for i in range(len(df))
            ])
            # Update only rows that had empty cuisine
            cuisine_series[empty_cuisine_mask] = cuisine_from_text[empty_cuisine_mask]
            
            # Log how many were found
            found_in_text = (cuisine_from_text[empty_cuisine_mask] != cuisine_default).sum()
            logger.info(f"Found cuisine in text columns for {found_in_text:,} rows (out of {empty_cuisine_mask.sum():,} empty)")
            if found_in_text > 0:
                sample_found = cuisine_from_text[empty_cuisine_mask & (cuisine_from_text != cuisine_default)].head(5).tolist()
                logger.info(f"Sample cuisines found in text: {sample_found}")
        
        # Create result DataFrame
        result = pd.DataFrame({
            "Dataset_ID": dataset_id,
            "index": df.index,
            "ingredients": df[ing_col].astype(str),
            "cuisine": cuisine_series,
        })
        
        # Drop rows where cuisine is still empty/default (no way to add cuisine label)
        before_drop = len(result)
        # Filter out rows with empty/default cuisine
        empty_cuisine_mask = (
            (result["cuisine"] == cuisine_default) | 
            (result["cuisine"].astype(str).str.strip() == "") |
            (result["cuisine"].astype(str).str.strip() == "[]") |
            (result["cuisine"].astype(str).str.strip().isin(["nan", "None", "null"]))
        )
        result = result[~empty_cuisine_mask].copy()
        dropped_count = before_drop - len(result)
        
        if dropped_count > 0:
            logger.info(f"Dropped {dropped_count:,} rows without cuisine labels (out of {before_drop:,} total)")
        logger.info(f"Processed {csv_path.name}: {len(result):,} rows (after dropping rows without cuisine)")
        return result
        
    except Exception as e:
        logger.exception(f"Error processing {csv_path.name}: {e}")
        return None


def main():
    parser = argparse.ArgumentParser(
        description="Combine raw datasets into unified format"
    )
    parser.add_argument(
        "--data-dir",
        type=str,
        default="./data/raw",
        help="Directory containing raw CSV files (default: ./data/raw)",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="./data/combined_raw_datasets.parquet",
        help="Output path for combined dataset (default: ./data/combined_raw_datasets.parquet)",
    )
    parser.add_argument(
        "--cuisine-default",
        type=str,
        default="unknown",
        help="Default cuisine value (default: unknown)",
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Optional config file for logging",
    )
    parser.add_argument(
        "--inference-config",
        type=str,
        default="./pipeline/config/ingredient_ner_inference.yaml",
        help="Path to inference config YAML (default: ./pipeline/config/ingredient_ner_inference.yaml)",
    )
    parser.add_argument(
        "--skip-inference",
        action="store_true",
        help="Skip ingredient NER inference (default: run inference)",
    )
    args = parser.parse_args()
    
    # Setup logging
    logger = setup_logging(args.config) if args.config else logging.getLogger(__name__)
    if not logger.handlers:
        logging.basicConfig(
            level=logging.INFO,
            format="[%(asctime)s] %(levelname)s %(name)s: %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        logger = logging.getLogger(__name__)
    
    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        logger.error(f"Data directory not found: {data_dir}")
        return 1
    
    # Find all CSV files, excluding specified ones
    excluded_files = {
        "wilmerarltstrmberg_data.csv",
        "sample_data.csv",
        "recipe_api_data.csv",
    }
    
    csv_files = sorted([
        f for f in data_dir.glob("*.csv")
        if f.name not in excluded_files
    ])
    
    logger.info(f"Found {len(csv_files)} CSV files to process (excluding .xlsx and {len(excluded_files)} specified files)")
    
    if not csv_files:
        logger.warning("No CSV files found to process")
        return 1
    
    # Process each dataset
    all_dfs: List[pd.DataFrame] = []
    for dataset_id, csv_path in enumerate(csv_files, start=1):
        logger.info(f"[{dataset_id}/{len(csv_files)}] Processing {csv_path.name}...")
        result_df = process_dataset(csv_path, dataset_id, logger, args.cuisine_default)
        if result_df is not None:
            all_dfs.append(result_df)
    
    if not all_dfs:
        logger.error("No datasets were successfully processed")
        return 1
    
    # Combine all DataFrames
    logger.info("Combining all datasets...")
    combined = pd.concat(all_dfs, ignore_index=True)
    
    # Final safety check: drop any rows that still don't have cuisine labels
    before_final_drop = len(combined)
    empty_cuisine_mask = (
        (combined["cuisine"] == args.cuisine_default) | 
        (combined["cuisine"].astype(str).str.strip() == "") |
        (combined["cuisine"].astype(str).str.strip() == "[]") |
        (combined["cuisine"].astype(str).str.strip().isin(["nan", "None", "null"]))
    )
    combined = combined[~empty_cuisine_mask].copy()
    final_dropped = before_final_drop - len(combined)
    
    if final_dropped > 0:
        logger.warning(f"Final safety check: dropped {final_dropped:,} additional rows without cuisine labels")
    
    logger.info(f"Combined dataset: {len(combined):,} total rows (after dropping rows without cuisine)")
    logger.info(f"Dataset_ID distribution:")
    for dataset_id, count in combined["Dataset_ID"].value_counts().sort_index().items():
        logger.info(f"  Dataset {dataset_id}: {count:,} rows")
    
    # Initialize inferred_ingredients and encoded_ingredients columns
    combined["inferred_ingredients"] = None
    combined["encoded_ingredients"] = None  # For debugging: ingredient IDs
    
    # Run inference unless skipped
    if not args.skip_inference:
        if not _HAS_INFERENCE:
            logger.error("Inference modules not available. Cannot run inference.")
            return 1
        
        logger.info("=" * 60)
        logger.info("Running ingredient NER inference...")
        logger.info("=" * 60)
        
        try:
            # Load inference config
            inference_config_path = Path(args.inference_config)
            if not inference_config_path.exists():
                logger.error(f"Inference config not found: {inference_config_path}")
                return 1
            
            logger.info(f"Loading inference config from {inference_config_path}")
            load_inference_configs_from_yaml(inference_config_path)
            
            # Save combined dataset temporarily for inference (as parquet)
            temp_path = Path(args.output).with_suffix(".temp.parquet")
            temp_path.parent.mkdir(parents=True, exist_ok=True)
            combined.to_parquet(temp_path, index=False, compression="zstd")
            logger.info(f"Saved temporary dataset to {temp_path} for inference")
            
            # Run inference - use predict_normalize_encode_structured directly to ensure parquet is detected
            from ingredient_ner.inference import predict_normalize_encode_structured, load_dedupe_and_maps_from_config
            
            # Load dedupe and maps
            dedupe, tok2id = load_dedupe_and_maps_from_config()
            
            logger.info("Running inference on combined dataset...")
            df_wide, df_tall = predict_normalize_encode_structured(
                nlp_dir=OUT.MODEL_DIR,
                data_path=temp_path,
                is_parquet=True,  # Explicitly set to True since we saved as parquet
                text_col="ingredients",
                dedupe=dedupe,
                tok2id=tok2id,
                out_path=None,  # Don't write separate output files
                batch_size=256,
                n_process=1,
                use_spacy_normalizer=True,
                spacy_model="en_core_web_sm",
            )
            
            # Map inference results back to combined dataset
            # df_wide has the same row order as input, with NER_clean and Ingredients columns
            logger.info("Mapping inference results to combined dataset...")
            if len(df_wide) == len(combined):
                combined["inferred_ingredients"] = df_wide["NER_clean"].tolist()
                # Extract encoded ingredient IDs if available (when tok2id is provided)
                if "Ingredients" in df_wide.columns:
                    combined["encoded_ingredients"] = df_wide["Ingredients"].tolist()
                    logger.info(f"Successfully mapped {len(df_wide):,} inference results (with encoding)")
                else:
                    logger.warning("Ingredients column not found in inference results (encoding not available)")
                    combined["encoded_ingredients"] = None
            else:
                logger.warning(
                    f"Row count mismatch: combined={len(combined)}, inference={len(df_wide)}. "
                    f"Using index-based mapping."
                )
                # Try to match by index if possible
                if "index" in df_wide.columns:
                    # This shouldn't happen, but handle it
                    logger.warning("Index-based mapping not implemented. Inference results may not align.")
                else:
                    # Assume same order
                    combined["inferred_ingredients"] = df_wide["NER_clean"].tolist()[:len(combined)]
                    if "Ingredients" in df_wide.columns:
                        combined["encoded_ingredients"] = df_wide["Ingredients"].tolist()[:len(combined)]
                    else:
                        combined["encoded_ingredients"] = None
            
            # Clean up temp file
            if temp_path.exists():
                temp_path.unlink()
                logger.debug(f"Removed temporary file: {temp_path}")
            
            logger.info("Inference complete")
            
        except Exception as e:
            logger.exception(f"Error during inference: {e}")
            logger.warning("Continuing without inference results...")
    
    # Write output
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    if output_path.suffix == ".parquet":
        combined.to_parquet(output_path, index=False, compression="zstd")
        logger.info(f"Saved combined dataset to {output_path} (Parquet format)")
    else:
        combined.to_csv(output_path, index=False)
        logger.info(f"Saved combined dataset to {output_path} (CSV format)")
    
    logger.info("=" * 60)
    logger.info("Dataset combination complete")
    logger.info("=" * 60)
    logger.info("\nFirst few rows:")
    logger.info("\n" + str(combined.head()))
    return 0


if __name__ == "__main__":
    exit(main())

================================================================================
FILE: preprocess_pipeline\pipeline\scripts\encode_ingredients.py
================================================================================
"""
Script to encode already-inferred ingredients to IDs.

This script takes a dataset with inferred_ingredients (normalized/canonical forms)
and encodes them to ingredient IDs using the token-to-ID mapping.
This is separate from inference to allow faster iteration when only encoding changes.
"""

import pyarrow.parquet as pq
import argparse
import json
import logging
from pathlib import Path
from typing import List, Optional

import pandas as pd
import pyarrow.parquet as pq

import sys as _sys
# Make `pipeline/` importable when running from repo root
_SYS_PATH_ROOT = Path.cwd() / "pipeline"
if str(_SYS_PATH_ROOT) not in _sys.path:
    _sys.path.append(str(_SYS_PATH_ROOT))

from common.logging_setup import setup_logging


def load_encoder_maps(
    id2tok_path: Optional[Path],
    tok2id_path: Optional[Path],
) -> tuple[Optional[dict], Optional[dict]]:
    """Load IngredientEncoder id2tok / tok2id maps."""
    if not id2tok_path or not tok2id_path:
        return None, None
    if (not Path(id2tok_path).exists()) or (not Path(tok2id_path).exists()):
        return None, None
    with open(id2tok_path, "r", encoding="utf-8") as f:
        id2tok_raw = json.load(f)
    with open(tok2id_path, "r", encoding="utf-8") as f:
        tok2id_raw = json.load(f)
    id2tok = {int(k): str(v) for k, v in id2tok_raw.items()}
    tok2id = {str(k): int(v) for k, v in tok2id_raw.items()}
    return id2tok, tok2id


def encode_ingredient_list(ingredient_list, tok2id: dict) -> List[int]:
    """
    Encode a list of normalized ingredients to IDs.
    Handles various input formats: list, numpy array, tuple, string representation of list, None, etc.
    """
    import numpy as np
    
    # Handle None or NaN
    if ingredient_list is None or (isinstance(ingredient_list, float) and pd.isna(ingredient_list)):
        return []
    
    # Convert numpy array to list
    if isinstance(ingredient_list, np.ndarray):
        ingredient_list = ingredient_list.tolist()
    
    # If it's a string, try to parse it as a list
    if isinstance(ingredient_list, str):
        s = ingredient_list.strip()
        if not s or s.lower() in ["none", "nan", "null", "[]"]:
            return []
        # Try to parse as JSON or Python list
        if (s.startswith("[") and s.endswith("]")) or (s.startswith("(") and s.endswith(")")):
            try:
                import ast
                ingredient_list = ast.literal_eval(s)
            except:
                try:
                    ingredient_list = json.loads(s)
                except:
                    # If parsing fails, treat as single string
                    ingredient_list = [s]
        else:
            # Single string, not a list
            ingredient_list = [s]
    
    # Now ingredient_list should be a list-like object
    if not isinstance(ingredient_list, (list, tuple, np.ndarray)):
        return []
    
    encoded = []
    for ing in ingredient_list:
        if ing is None:
            continue
        ing_str = str(ing).strip()
        if not ing_str:
            continue
        # Try exact match first
        ing_id = tok2id.get(ing_str, None)
        if ing_id is None:
            # Try with different whitespace normalization
            ing_normalized = " ".join(ing_str.split())
            ing_id = tok2id.get(ing_normalized, 0)
        encoded.append(ing_id)
    
    return encoded if encoded else []


def main():
    parser = argparse.ArgumentParser(
        description="Encode inferred ingredients to IDs"
    )
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="Input parquet file with inferred_ingredients column",
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="Output parquet file (will add/update encoded_ingredients column)",
    )
    parser.add_argument(
        "--ingredient-token-to-id",
        type=str,
        default="./data/encoded/ingredient_token_to_id.json",
        help="Path to token-to-ID mapping JSON (default: ./data/encoded/ingredient_token_to_id.json)",
    )
    parser.add_argument(
        "--ingredient-id-to-token",
        type=str,
        default="./data/encoded/ingredient_id_to_token.json",
        help="Path to ID-to-token mapping JSON (default: ./data/encoded/ingredient_id_to_token.json)",
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Optional config file for logging",
    )
    args = parser.parse_args()
    
    # Setup logging
    logger = setup_logging(args.config) if args.config else logging.getLogger(__name__)
    if not logger.handlers:
        logging.basicConfig(
            level=logging.INFO,
            format="[%(asctime)s] %(levelname)s %(name)s: %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        logger = logging.getLogger(__name__)
    
    # Load encoder maps
    tok2id_path = Path(args.ingredient_token_to_id)
    id2tok_path = Path(args.ingredient_id_to_token)
    
    if not tok2id_path.exists():
        logger.error(f"Token-to-ID map not found: {tok2id_path}")
        return 1
    
    logger.info(f"Loading encoder maps...")
    id2tok, tok2id = load_encoder_maps(id2tok_path, tok2id_path)
    
    if not tok2id:
        logger.error("Failed to load token-to-ID mapping")
        return 1
    
    logger.info(f"Loaded token→ID map: {len(tok2id):,} tokens")
    
    # Load input data
    input_path = Path(args.input)
    if not input_path.exists():
        logger.error(f"Input file not found: {input_path}")
        return 1
    
    logger.info(f"Loading input data from {input_path}...")
    
    # Read parquet file
    pf = pq.ParquetFile(str(input_path))
    logger.info(f"Input file has {pf.num_row_groups} row groups")
    
    # Check if inferred_ingredients column exists
    schema = pf.schema_arrow
    col_names = [field.name for field in schema]
    
    if "inferred_ingredients" not in col_names:
        logger.error("Column 'inferred_ingredients' not found in input file")
        logger.error(f"Available columns: {col_names}")
        return 1
    
    # Process row groups and encode
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    logger.info("Encoding ingredients...")
    
    # Read all data (or process in chunks if too large)
    # Use pyarrow to preserve list types correctly
    pf = pq.ParquetFile(str(input_path))
    
    # Read in chunks to handle large files
    chunks = []
    for rg in range(pf.num_row_groups):
        df_chunk = pf.read_row_group(rg).to_pandas()
        chunks.append(df_chunk)
    df = pd.concat(chunks, ignore_index=True)
    logger.info(f"Loaded {len(df):,} rows")
    
    # Debug: Check data types and sample values
    logger.info("Debugging data format...")
    logger.info(f"  Column dtypes: {df.dtypes.to_dict()}")
    
    # Check sample of inferred_ingredients
    import numpy as np
    sample_ing = df["inferred_ingredients"].iloc[0] if len(df) > 0 else None
    logger.info(f"  Sample inferred_ingredients (row 0):")
    logger.info(f"    Type: {type(sample_ing)}")
    logger.info(f"    Value: {sample_ing}")
    
    # Convert to list if numpy array
    if isinstance(sample_ing, np.ndarray):
        sample_ing_list = sample_ing.tolist()
    elif isinstance(sample_ing, (list, tuple)):
        sample_ing_list = list(sample_ing)
    else:
        sample_ing_list = []
    
    if len(sample_ing_list) > 0:
        logger.info(f"    First item: '{sample_ing_list[0]}' (type: {type(sample_ing_list[0])})")
        # Check if it's in tok2id
        first_key = str(sample_ing_list[0]).strip()
        in_vocab = first_key in tok2id
        logger.info(f"    In tok2id: {in_vocab}")
        if in_vocab:
            logger.info(f"    ID: {tok2id[first_key]}")
        else:
            # Show some sample keys from tok2id
            sample_keys = list(tok2id.keys())[:10]
            logger.info(f"    Sample tok2id keys: {sample_keys}")
            # Try to find similar keys
            first_lower = first_key.lower()
            similar = [k for k in tok2id.keys() if first_lower in k.lower() or k.lower() in first_lower][:5]
            if similar:
                logger.info(f"    Similar keys in vocab: {similar}")
    elif sample_ing is not None:
        logger.warning(f"    Sample is not a list/array! Type: {type(sample_ing)}, Value: {sample_ing}")
    
    # Encode ingredients
    logger.info("Encoding inferred_ingredients to IDs...")
    encoded_lists = []
    zero_count = 0
    total_ingredients = 0
    sample_issues = []  # Track sample encoding issues for debugging
    
    for idx, ing_list in enumerate(df["inferred_ingredients"]):
        if idx % 10000 == 0 and idx > 0:
            logger.info(f"Processed {idx:,} / {len(df):,} rows...")
        
        # Debug first few entries
        if idx < 5:
            logger.debug(f"Row {idx}: type={type(ing_list)}, value={ing_list}")
            if isinstance(ing_list, (list, tuple)) and len(ing_list) > 0:
                logger.debug(f"  First ingredient: '{ing_list[0]}' (type={type(ing_list[0])})")
                # Check if first ingredient is in tok2id
                first_ing = str(ing_list[0]).strip()
                if first_ing in tok2id:
                    logger.debug(f"  ✓ Found in tok2id: {tok2id[first_ing]}")
                else:
                    logger.debug(f"  ✗ Not found in tok2id. Sample keys: {list(tok2id.keys())[:5]}")
        
        encoded = encode_ingredient_list(ing_list, tok2id)
        encoded_lists.append(encoded)
        
        # Count zeros for diagnostics
        zeros = sum(1 for x in encoded if x == 0)
        zero_count += zeros
        total_ingredients += len(encoded)
        
        # Track issues for first few rows
        if idx < 5 and len(encoded) == 0 and ing_list is not None:
            if isinstance(ing_list, (list, tuple)) and len(ing_list) > 0:
                sample_issues.append((idx, ing_list[:3], encoded))
    
    # Add encoded column
    df["encoded_ingredients"] = encoded_lists
    
    # Log statistics
    logger.info(f"Encoding complete:")
    logger.info(f"  Total rows: {len(df):,}")
    logger.info(f"  Total ingredients: {total_ingredients:,}")
    if total_ingredients > 0:
        logger.info(f"  Ingredients with ID=0 (unknown): {zero_count:,} ({zero_count/total_ingredients*100:.1f}%)")
    else:
        logger.warning("  No ingredients were encoded! This suggests a data format issue.")
        logger.warning("  Checking sample data...")
        
        # Show sample of what we're trying to encode
        sample_df = df[["inferred_ingredients"]].head(10)
        for idx, row in sample_df.iterrows():
            ing_list = row["inferred_ingredients"]
            logger.warning(f"  Row {idx}: type={type(ing_list)}, value={ing_list}")
            if isinstance(ing_list, (list, tuple)) and len(ing_list) > 0:
                first = str(ing_list[0]).strip()
                logger.warning(f"    First item: '{first}' (in tok2id: {first in tok2id})")
                # Show similar keys
                if first not in tok2id:
                    # Find similar keys
                    similar = [k for k in tok2id.keys() if first.lower() in k.lower() or k.lower() in first.lower()][:3]
                    if similar:
                        logger.warning(f"    Similar keys in tok2id: {similar}")
    
    logger.info(f"  First few encoded ingredients:")
    logger.info(f"\n{df[['inferred_ingredients', 'encoded_ingredients']].head()}")
    logger.info(f"Saving to {output_path}...")
    df.to_parquet(output_path, index=False, compression="zstd")
    logger.info(f"Saved encoded dataset to {output_path}")
    
    logger.info("=" * 60)
    logger.info("Encoding complete")
    logger.info("=" * 60)
    
    return 0


if __name__ == "__main__":
    exit(main())

================================================================================
FILE: preprocess_pipeline\pipeline\scripts\run_cuisine_norm.py
================================================================================
"""
Script to normalize and encode cuisines using the ingrnorm pipeline.

Reuses the same normalization, deduplication, and encoding infrastructure
as run_ingrnorm.py, but applied to the cuisine column.
Handles splitting entries that contain multiple cuisines per row.
"""

from __future__ import annotations
import argparse
import logging
import ast
import json
import os
import glob
from pathlib import Path
import sys as _sys
import pathlib as _pathlib
import yaml
import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
import numpy as np

# Make `pipeline/` importable when running from repo root
_SYS_PATH_ROOT = _pathlib.Path.cwd() / "pipeline"
if str(_SYS_PATH_ROOT) not in _sys.path:
    _sys.path.append(str(_SYS_PATH_ROOT))

from common.logging_setup import setup_logging
from ingrnorm.io import materialize_parquet_source
from ingrnorm.parquet_utils import vocab_from_parquet_listcol
from ingrnorm.spacy_normalizer import apply_spacy_normalizer_to_parquet
from ingrnorm.sbert_dedupe import sbert_dedupe
from ingrnorm.w2v_dedupe import w2v_dedupe
from ingrnorm.dedupe_map import apply_map_to_parquet_streaming, write_jsonl_map
from ingrnorm.encoder import IngredientEncoder

logger = logging.getLogger("cuisine_norm")


def _cleanup_paths(cfg: dict, logger: logging.Logger, preserve_files: list[str] = None):
    """
    Clean up old artifacts if cleanup is enabled in config.
    By default, deletes parquet files but keeps JSON/JSONL files.
    
    Args:
        cfg: Configuration dict
        logger: Logger instance
        preserve_files: List of file paths/patterns to preserve (e.g., main datasets)
    """
    cleanup_cfg = cfg.get("cleanup", {})
    if not cleanup_cfg.get("enabled", False):
        logger.info("Cleanup disabled – skipping file deletions.")
        return
    
    preserve_files = preserve_files or []
    preserve_patterns = []
    for p in preserve_files:
        try:
            preserve_patterns.append(Path(p).resolve())
        except (OSError, ValueError):
            # If path doesn't exist or can't be resolved, store as string for pattern matching
            preserve_patterns.append(p)
    
    paths = cleanup_cfg.get("paths", [])
    if not paths:
        logger.info("Cleanup enabled but no paths specified.")
        return
    
    deleted_count = 0
    for pattern in paths:
        for f in glob.glob(pattern):
            f_path = Path(f).resolve()
            
            # Skip if file should be preserved
            should_preserve = False
            for preserve in preserve_patterns:
                if isinstance(preserve, Path):
                    if preserve.exists() and f_path.samefile(preserve):
                        should_preserve = True
                        break
                    # Also check if preserve is a pattern that matches
                    if str(f_path).endswith(str(preserve)) or str(preserve) in str(f_path):
                        should_preserve = True
                        break
                else:
                    # String pattern matching
                    if str(preserve) in str(f_path) or str(f_path).endswith(str(preserve)):
                        should_preserve = True
                        break
            
            if should_preserve:
                logger.debug(f"[cleanup] Preserving {f}")
                continue
            
            # Only delete parquet files, keep JSON/JSONL files
            if f.endswith('.parquet'):
                try:
                    os.remove(f)
                    logger.info(f"[cleanup] Deleted {f}")
                    deleted_count += 1
                except FileNotFoundError:
                    pass
                except Exception as e:
                    logger.warning(f"[cleanup] Failed to delete {f}: {e}")
            else:
                logger.debug(f"[cleanup] Keeping non-parquet file: {f}")
    
    if deleted_count > 0:
        logger.info(f"[cleanup] Cleaned up {deleted_count} parquet file(s)")


def _needs_reapply_map(dedupe_map_path: Path, deduped_parquet: Path) -> bool:
    """Check if dedupe map needs to be reapplied (map is newer than deduped parquet)."""
    if not dedupe_map_path.exists():
        return False
    if not deduped_parquet.exists():
        return True
    try:
        map_mtime = dedupe_map_path.stat().st_mtime
        parquet_mtime = deduped_parquet.stat().st_mtime
        return map_mtime > parquet_mtime
    except (OSError, AttributeError):
        return False


def split_cuisine_entries(cuisine_value: str) -> list[str]:
    """
    Split cuisine entries that may contain multiple cuisines.
    Handles formats like:
    - "[American, Italian]" -> ["American", "Italian"]
    - "American, Italian" -> ["American", "Italian"]
    - "American & Italian" -> ["American", "Italian"]
    - "American" -> ["American"]
    - "Goan recipes" -> ["Goan"] (removes "recipes" suffix)
    """
    if pd.isna(cuisine_value) or not str(cuisine_value).strip():
        return []
    
    s = str(cuisine_value).strip()
    
    # Try parsing as list/JSON first
    if s.startswith("[") and s.endswith("]"):
        try:
            parsed = ast.literal_eval(s)
            if isinstance(parsed, list):
                cuisines = [str(x).strip() for x in parsed if str(x).strip()]
            else:
                cuisines = [s]
        except:
            try:
                parsed = json.loads(s)
                if isinstance(parsed, list):
                    cuisines = [str(x).strip() for x in parsed if str(x).strip()]
                else:
                    cuisines = [s]
            except:
                cuisines = [s]
    else:
        # Try splitting on comma
        if "," in s:
            parts = [x.strip() for x in s.split(",") if x.strip()]
            if len(parts) > 1:
                cuisines = parts
            else:
                cuisines = [s]
        else:
            # Try splitting on " & " or " and "
            found_split = False
            for sep in [" & ", " and ", " AND "]:
                if sep in s:
                    parts = [x.strip() for x in s.split(sep) if x.strip()]
                    if len(parts) > 1:
                        cuisines = parts
                        found_split = True
                        break
            if not found_split:
                # Single value
                cuisines = [s] if s else []
    
    # Clean up each cuisine: remove "recipes" suffix (case-insensitive)
    cleaned = []
    for cuisine in cuisines:
        if not cuisine:
            continue
        # Remove "recipes" from the end (case-insensitive, with optional whitespace)
        cuisine_clean = cuisine.strip()
        # Remove trailing "recipes" or " recipe" (singular or plural)
        for suffix in [" recipes", " recipe", "Recipes", "Recipe"]:
            if cuisine_clean.lower().endswith(suffix.lower()):
                cuisine_clean = cuisine_clean[:-len(suffix)].strip()
                break
        if cuisine_clean:
            cleaned.append(cuisine_clean)
    
    return cleaned


def prepare_cuisine_list_column(
    input_path: Path,
    cuisine_col: str,
    output_path: Path,
    logger: logging.Logger,
) -> Path:
    """
    Prepare cuisine column as a list column for normalization pipeline.
    Splits entries that contain multiple cuisines.
    """
    logger.info(f"Preparing cuisine list column from {input_path}...")
    
    # Read parquet file
    pf = pq.ParquetFile(str(input_path))
    logger.info(f"Input file has {pf.num_row_groups} row groups")
    
    # Process row groups and split cuisines
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    writer = None
    schema = None
    
    for rg_idx in range(pf.num_row_groups):
        logger.info(f"Processing row group {rg_idx + 1}/{pf.num_row_groups}...")
        
        # Read row group
        df = pf.read_row_group(rg_idx).to_pandas()
        
        if cuisine_col not in df.columns:
            logger.error(f"Column '{cuisine_col}' not found in input file")
            logger.error(f"Available columns: {list(df.columns)}")
            raise KeyError(f"Column '{cuisine_col}' not found")
        
        # Split cuisine entries into lists
        logger.info(f"Splitting cuisine entries in row group {rg_idx + 1}...")
        cuisine_lists = df[cuisine_col].apply(split_cuisine_entries)
        
        # Count splits for statistics
        splits = sum(1 for lst in cuisine_lists if len(lst) > 1)
        if splits > 0:
            logger.info(f"  Found {splits:,} entries with multiple cuisines (out of {len(cuisine_lists):,})")
        
        # Create new DataFrame with list column
        df_output = pd.DataFrame({
            cuisine_col: cuisine_lists,
        })
        
        # Convert to PyArrow table
        import pyarrow as pa
        table = pa.Table.from_pandas(df_output, preserve_index=False)
        
        # Set schema (list of strings)
        if schema is None:
            schema = pa.schema([
                pa.field(cuisine_col, pa.list_(pa.string())),
            ])
            writer = pq.ParquetWriter(str(output_path), schema, compression="zstd")
        
        writer.write_table(table)
    
    if writer:
        writer.close()
    
    logger.info(f"Saved prepared cuisine list column to {output_path}")
    return output_path


def main():
    ap = argparse.ArgumentParser(
        description="Normalize and encode cuisines using ingrnorm pipeline"
    )
    ap.add_argument(
        "--config",
        type=str,
        default="pipeline/config/cuisnorm.yaml",
        help="Path to cuisine normalization config YAML (default: pipeline/config/cuisnorm.yaml)",
    )
    ap.add_argument(
        "--input",
        type=str,
        default=None,
        help="Input parquet file with cuisine column (default: from config data.input_path)",
    )
    ap.add_argument(
        "--cuisine-col",
        type=str,
        default="cuisine",
        help="Name of cuisine column (default: cuisine)",
    )
    ap.add_argument(
        "--output-dir",
        type=str,
        default="./data/cuisine_normalized",
        help="Output directory for cuisine normalization artifacts (default: derived from config)",
    )
    ap.add_argument(
        "--force",
        action="store_true",
        help="Rebuild artifacts even if they exist",
    )
    args = ap.parse_args()
    
    # Load config
    cfg_path = Path(args.config)
    if not cfg_path.exists():
        raise FileNotFoundError(f"Config not found: {cfg_path}")
    with open(cfg_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    
    setup_logging(cfg)
    logger = logging.getLogger("cuisine_norm")
    
    # Get paths from config (with CLI overrides)
    data_cfg = cfg.get("data", {})
    out_cfg = cfg.get("output", {})
    
    # Validate config and cleanup old artifacts (preserve main dataset)
    combined_dataset_path = Path(data_cfg.get("input_path", "./data/encoded_combined_datasets.parquet"))
    _cleanup_paths(cfg, logger, preserve_files=[str(combined_dataset_path)])
    
    
    # Input path: use CLI arg if provided, otherwise from config
    if args.input:
        input_path = Path(args.input)
    else:
        input_path = Path(data_cfg.get("input_path", "./data/encoded_combined_datasets.parquet"))
    
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")
    
    # Output directory: use CLI arg if provided, otherwise derive from config
    if args.output_dir != "./data/cuisine_normalized":  # User provided custom path
        output_dir = Path(args.output_dir)
    else:
        # Derive from config output paths
        baseline_path = Path(out_cfg.get("baseline_parquet", "./data/cuisine_normalized/cuisine_baseline.parquet"))
        output_dir = baseline_path.parent
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Cuisine column: use CLI arg if provided, otherwise from config
    cuisine_col = args.cuisine_col if args.cuisine_col != "cuisine" else data_cfg.get("cuisine_col", "cuisine")
    
    # Get output paths from config
    baseline_parquet = Path(out_cfg.get("baseline_parquet", "./data/cuisine_normalized/cuisine_baseline.parquet"))
    final_dedup_parquet = Path(out_cfg.get("dedup_parquet", "./data/cuisine_normalized/cuisine_deduped.parquet"))
    dedupe_map_path = Path(out_cfg.get("cosine_map_path", "./data/cuisine_normalized/cuisine_dedupe_map.jsonl"))
    list_col_for_vocab = out_cfg.get("list_col_for_vocab", "cuisine_clean")
    unified_parquet = Path(out_cfg.get("unified_parquet", "./data/cuisine_encoded/cuisine_unified.parquet"))
    cuisine_id_to_token = Path(out_cfg.get("cuisine_id_to_token", "./data/cuisine_encoded/cuisine_id_to_token.json"))
    cuisine_token_to_id = Path(out_cfg.get("cuisine_token_to_id", "./data/cuisine_encoded/cuisine_token_to_id.json"))
    
    # Step 1: Prepare cuisine list column (split multi-cuisine entries)
    prepared_parquet = baseline_parquet.parent / "_cuisine_prepared.parquet"
    if args.force or not prepared_parquet.exists():
        logger.info("=" * 60)
        logger.info("Step 1: Preparing cuisine list column (splitting multi-cuisine entries)")
        logger.info("=" * 60)
        prepare_cuisine_list_column(input_path, cuisine_col, prepared_parquet, logger)
    else:
        logger.info("Step 1: Skipped (prepared file exists)")
    
    # Step 2: Apply spaCy normalization (reuse ingrnorm pipeline)
    if args.force or not baseline_parquet.exists():
        logger.info("=" * 60)
        logger.info("Step 2: spaCy normalization")
        logger.info("=" * 60)
        sbert_cfg = cfg.get("sbert", {})
        apply_spacy_normalizer_to_parquet(
            in_parquet=str(prepared_parquet),
            out_parquet=str(baseline_parquet),
            list_col=cuisine_col,
            out_col=list_col_for_vocab,
            spacy_model=sbert_cfg.get("spacy_model", "en_core_web_sm"),
            batch_size=int(sbert_cfg.get("spacy_batch_size", 512)),
            n_process=int(sbert_cfg.get("spacy_n_process", 1)),
        )
        logger.info(f"Saved baseline cuisine Parquet → {baseline_parquet}")
    else:
        logger.info("Step 2: Skipped (baseline file exists)")
    
    # Step 3: Build vocabulary
    min_freq_for_vocab = int(cfg.get("sbert", {}).get("min_freq_for_vocab", 1))
    vocab = vocab_from_parquet_listcol(
        str(baseline_parquet),
        col=list_col_for_vocab,
        min_freq=min_freq_for_vocab,
    )
    logger.info(f"Vocab size: {len(vocab):,} (min_freq={min_freq_for_vocab})")
    
    if len(vocab) == 0:
        logger.warning("Vocabulary is empty. Check cuisine_clean column.")
        return 1
    
    # Step 4: Deduplication (SBERT)
    stages_cfg = cfg.get("stages", {})
    use_sbert = bool(stages_cfg.get("sbert_dedupe", True))
    
    if use_sbert and (args.force or not dedupe_map_path.exists()):
        logger.info("=" * 60)
        logger.info("Step 3: SBERT deduplication")
        logger.info("=" * 60)
        sbert_cfg = cfg.get("sbert", {})
        mapping = sbert_dedupe(
            vocab_counter=vocab,
            out_path=str(dedupe_map_path),
            model_name=sbert_cfg.get("model", "all-MiniLM-L6-v2"),
            threshold=float(sbert_cfg.get("threshold", 0.88)),
            topk=int(sbert_cfg.get("topk", 25)),
            min_len=int(sbert_cfg.get("min_len", 2)),
            require_token_overlap=bool(sbert_cfg.get("require_token_overlap", True)),
            block_generic_as_canon=bool(sbert_cfg.get("block_generic_as_canon", True)),
        )
        write_jsonl_map(mapping, dedupe_map_path)
        logger.info(f"Saved dedupe map → {dedupe_map_path}")
    else:
        logger.info("Step 3: Skipped (dedupe map exists or disabled)")
    
    # Step 5: Apply dedupe map
    # The JSON map is the source of truth - always apply it to the parquet if it exists
    do_apply_map = bool(stages_cfg.get("apply_cosine_map", True))
    if do_apply_map and dedupe_map_path.exists():
        needs_reapply = _needs_reapply_map(dedupe_map_path, final_dedup_parquet)
        logger.info(f"Step 4: Checking dedupe map application...")
        logger.info(f"  - Map exists: {dedupe_map_path.exists()}")
        logger.info(f"  - Deduped parquet exists: {final_dedup_parquet.exists()}")
        logger.info(f"  - Needs reapply: {needs_reapply}")
        logger.info(f"  - Force flag: {args.force}")
        
        if args.force or not final_dedup_parquet.exists() or needs_reapply:
            logger.info("=" * 60)
            logger.info("Step 4: Applying dedupe map to parquet")
            logger.info("=" * 60)
            if needs_reapply and final_dedup_parquet.exists():
                logger.info(f"Dedupe map is newer than deduped parquet; re-applying map...")
            
            # Load and log a sample of the mapping to verify it's being read
            from ingrnorm.dedupe_map import load_jsonl_map
            mapping_dict = load_jsonl_map(dedupe_map_path)
            logger.info(f"Loaded {len(mapping_dict):,} mappings from {dedupe_map_path}")
            if mapping_dict:
                sample_items = list(mapping_dict.items())[:5]
                logger.info(f"Sample mappings: {sample_items}")
            
            # Check baseline parquet before applying map
            try:
                pf_baseline = pq.ParquetFile(str(baseline_parquet))
                if pf_baseline.num_row_groups > 0:
                    df_baseline_sample = pf_baseline.read_row_group(0).to_pandas()
                    logger.info(f"Baseline parquet: shape={df_baseline_sample.shape}, columns={list(df_baseline_sample.columns)}")
                    if list_col_for_vocab in df_baseline_sample.columns:
                        baseline_sample = df_baseline_sample[list_col_for_vocab].head(5)
                        logger.info(f"Baseline sample (before mapping):")
                        for idx, lst in enumerate(baseline_sample):
                            if isinstance(lst, (list, tuple, np.ndarray)) and len(lst) > 0:
                                logger.info(f"  Row {idx}: {lst[:3]}")
            except Exception as e:
                logger.warning(f"Could not inspect baseline parquet: {e}")
            
            apply_map_to_parquet_streaming(
                in_path=str(baseline_parquet),
                out_path=str(final_dedup_parquet),
                mapping=str(dedupe_map_path),
                list_col=list_col_for_vocab,
            )
            logger.info(f"Saved deduped cuisine Parquet → {final_dedup_parquet}")
            
            # Verify the mapping was applied by checking a sample
            try:
                pf_check = pq.ParquetFile(str(final_dedup_parquet))
                logger.info(f"Verification: Parquet has {pf_check.num_row_groups} row group(s)")
                if pf_check.num_row_groups > 0:
                    df_sample = pf_check.read_row_group(0).to_pandas()
                    logger.info(f"Verification: DataFrame shape: {df_sample.shape}")
                    logger.info(f"Verification: Columns: {list(df_sample.columns)}")
                    logger.info(f"Verification: Looking for column '{list_col_for_vocab}'")
                    
                    if list_col_for_vocab in df_sample.columns:
                        sample_lists = df_sample[list_col_for_vocab].head(10)
                        logger.info(f"Sample of deduped data (first 10 rows):")
                        non_empty_count = 0
                        for idx, lst in enumerate(sample_lists):
                            if isinstance(lst, (list, tuple, np.ndarray)):
                                if len(lst) > 0:
                                    logger.info(f"  Row {idx}: {lst[:5]}")  # Show first 5 items
                                    non_empty_count += 1
                                else:
                                    logger.info(f"  Row {idx}: [] (empty)")
                            else:
                                logger.info(f"  Row {idx}: {type(lst).__name__} = {lst}")
                        logger.info(f"Verification: {non_empty_count}/10 rows have non-empty lists")
                    else:
                        logger.warning(f"Verification: Column '{list_col_for_vocab}' not found in deduped parquet!")
            except Exception as e:
                logger.warning(f"Could not verify deduped data: {e}", exc_info=True)
        else:
            logger.info("Step 4: Skipped (deduped file exists and is up-to-date with map)")
    elif do_apply_map and not dedupe_map_path.exists():
        logger.warning(f"Step 4: Dedupe map not found at {dedupe_map_path} - skipping map application")
    else:
        logger.info("Step 4: Skipped (map application disabled)")
    
    # Step 6: Encode to IDs
    # Always re-encode if the deduped parquet is newer than the encoded parquet
    # (This ensures manual edits to the JSON map are reflected in the encoded output)
    do_encode_ids = bool(stages_cfg.get("encode_ids", True))
    
    # CRITICAL: If dedupe map exists and we're encoding, ensure it's been applied first
    if do_encode_ids and do_apply_map and dedupe_map_path.exists():
        if not final_dedup_parquet.exists() or _needs_reapply_map(dedupe_map_path, final_dedup_parquet):
            logger.info("=" * 60)
            logger.info("Step 4 (re-check): Ensuring dedupe map is applied before encoding")
            logger.info("=" * 60)
            apply_map_to_parquet_streaming(
                in_path=str(baseline_parquet),
                out_path=str(final_dedup_parquet),
                mapping=str(dedupe_map_path),
                list_col=list_col_for_vocab,
            )
            logger.info(f"Re-applied dedupe map → {final_dedup_parquet}")
    
    needs_reencode = False
    if unified_parquet.exists() and final_dedup_parquet.exists():
        try:
            unified_mtime = unified_parquet.stat().st_mtime
            deduped_mtime = final_dedup_parquet.stat().st_mtime
            if deduped_mtime > unified_mtime:
                needs_reencode = True
                logger.info(f"Deduped parquet is newer than encoded parquet; re-encoding needed...")
        except (OSError, AttributeError):
            pass
    # Also check if the dedupe map is newer than the encoded parquet (map was manually edited)
    if not needs_reencode and unified_parquet.exists() and dedupe_map_path.exists():
        try:
            unified_mtime = unified_parquet.stat().st_mtime
            map_mtime = dedupe_map_path.stat().st_mtime
            if map_mtime > unified_mtime:
                needs_reencode = True
                logger.info(f"Dedupe map is newer than encoded parquet; will re-apply map and re-encode...")
        except (OSError, AttributeError):
            pass
    
    if do_encode_ids and (args.force or not unified_parquet.exists() or needs_reencode):
        logger.info("=" * 60)
        logger.info("Step 5: Encoding cuisines to IDs")
        logger.info("=" * 60)
        # Always prefer deduped parquet if it exists (it has the applied dedupe map)
        # CRITICAL: If dedupe map exists, we MUST use the deduped parquet, not the baseline
        if dedupe_map_path.exists() and do_apply_map:
            if not final_dedup_parquet.exists():
                logger.warning(f"Dedupe map exists but deduped parquet not found! Applying map now...")
                apply_map_to_parquet_streaming(
                    in_path=str(baseline_parquet),
                    out_path=str(final_dedup_parquet),
                    mapping=str(dedupe_map_path),
                    list_col=list_col_for_vocab,
                )
            source_parquet = final_dedup_parquet
            logger.info(f"Using DEDUPED parquet (with applied map) → {source_parquet}")
        else:
            source_parquet = final_dedup_parquet if final_dedup_parquet.exists() else baseline_parquet
            logger.info(f"Using source parquet → {source_parquet}")
        
        enc_cfg = cfg.get("encoder", {})
        enc_min_freq = int(enc_cfg.get("min_freq", 1))
        dataset_id = int(enc_cfg.get("dataset_id", 1))
        ingredients_col = enc_cfg.get("ingredients_col", list_col_for_vocab)
        
        enc = IngredientEncoder(min_freq=enc_min_freq)
        logger.info(f"Fitting encoder from {source_parquet} (col={ingredients_col}, min_freq={enc_min_freq})...")
        enc.fit_from_parquet_streaming(source_parquet, col=ingredients_col, min_freq=enc_min_freq).freeze()
        
        cuisine_id_to_token.parent.mkdir(parents=True, exist_ok=True)
        enc.save_maps(id_to_token_path=cuisine_id_to_token, token_to_id_path=cuisine_token_to_id)
        logger.info(f"Saved encoder maps → {cuisine_id_to_token}, {cuisine_token_to_id}")
        
        logger.info(f"Writing encoded cuisines → {unified_parquet}")
        enc.encode_parquet_streaming(
            parquet_path=source_parquet,
            out_parquet_path=unified_parquet,
            dataset_id=dataset_id,
            col=ingredients_col,
            compression="zstd",
        )
        logger.info(f"Saved encoded cuisine Parquet → {unified_parquet}")
    else:
        logger.info("Step 5: Skipped (encoded file exists and is up-to-date, or disabled)")
    
    logger.info("=" * 60)
    logger.info("Cuisine normalization and encoding complete")
    
    # Step 7: Apply dedupe and encoding to the original combined dataset
    logger.info("=" * 60)
    logger.info("Step 6: Applying dedupe and encoding to combined dataset")
    logger.info("=" * 60)
    
    combined_dataset_path = Path(data_cfg.get("input_path", "./data/encoded_combined_datasets.parquet"))
    if combined_dataset_path.exists():
        logger.info(f"Loading combined dataset from {combined_dataset_path}")
        
        # Load the combined dataset
        pf_combined = pq.ParquetFile(str(combined_dataset_path))
        logger.info(f"Combined dataset has {pf_combined.num_row_groups} row group(s)")
        
        # Load dedupe map and encoder
        from ingrnorm.dedupe_map import load_jsonl_map
        dedupe_map = load_jsonl_map(dedupe_map_path)
        logger.info(f"Loaded {len(dedupe_map):,} dedupe mappings")
        
        # Load encoder maps
        if cuisine_token_to_id.exists():
            with open(cuisine_token_to_id, "r", encoding="utf-8") as f:
                token_to_id = json.load(f)
            logger.info(f"Loaded encoder with {len(token_to_id):,} tokens")
        else:
            logger.warning(f"Encoder map not found at {cuisine_token_to_id}")
            token_to_id = {}
        
        # Process each row group and apply dedupe + encoding
        output_path = combined_dataset_path.parent / f"{combined_dataset_path.stem}_with_cuisine_encoded.parquet"
        writer = None
        
        for rg_idx in range(pf_combined.num_row_groups):
            logger.info(f"Processing row group {rg_idx + 1}/{pf_combined.num_row_groups}...")
            df_rg = pf_combined.read_row_group(rg_idx).to_pandas()
            
            # Apply dedupe map to cuisine column
            if cuisine_col in df_rg.columns:
                # Convert cuisine to list format (handle both string and list inputs)
                def normalize_cuisine_to_list(x):
                    """Convert cuisine value to a list of strings, handling various input formats."""
                    if pd.isna(x):
                        return []
                    # If already a list/array, extract strings directly
                    if isinstance(x, (list, tuple, np.ndarray)):
                        return [str(t).strip() for t in x if str(t).strip()]
                    # If it's a string, parse it (could be "[American]" or "American" or "[American, Italian]")
                    s = str(x).strip()
                    if not s or s.lower() in ["nan", "none", "[]", ""]:
                        return []
                    # Use split_cuisine_entries to handle all string formats
                    return split_cuisine_entries(s)
                
                cuisine_lists = df_rg[cuisine_col].apply(normalize_cuisine_to_list)
                
                # Normalize and apply dedupe map (lowercase for lookup)
                cuisine_deduped = cuisine_lists.apply(
                    lambda lst: [
                        dedupe_map.get(str(tok).lower().strip(), str(tok).lower().strip())
                        for tok in lst
                        if str(tok).strip()  # Skip empty strings
                    ]
                )
                df_rg[f"{cuisine_col}_deduped"] = cuisine_deduped
                
                # Encode to IDs
                cuisine_encoded = cuisine_deduped.apply(
                    lambda lst: [
                        token_to_id.get(tok.lower().strip(), 0)
                        for tok in lst
                    ]
                )
                df_rg[f"{cuisine_col}_encoded"] = cuisine_encoded
                
                logger.info(f"  Applied dedupe and encoding to {len(df_rg):,} rows")
                logger.info(f"  Sample: {cuisine_col} -> {cuisine_col}_deduped -> {cuisine_col}_encoded")
            else:
                logger.warning(f"  Column '{cuisine_col}' not found in row group {rg_idx + 1}")
                logger.info(f"  Available columns: {list(df_rg.columns)}")
            
            # Write to output parquet
            table = pa.Table.from_pandas(df_rg, preserve_index=False)
            if writer is None:
                writer = pq.ParquetWriter(str(output_path), table.schema, compression="zstd")
            writer.write_table(table)
        
        if writer:
            writer.close()
        
        logger.info(f"Saved updated dataset → {output_path}")
        
        # Debug: Show head and columns
        logger.info("=" * 60)
        logger.info("Debug: Inspecting updated dataset")
        logger.info("=" * 60)
        df_debug = pq.read_table(output_path).to_pandas()
        logger.info(f"Dataset shape: {df_debug.shape}")
        logger.info(f"Columns: {list(df_debug.columns)}")
        logger.info("\nFirst 10 rows (showing key columns):")
        # Show relevant columns
        cols_to_show = [cuisine_col]
        if f"{cuisine_col}_deduped" in df_debug.columns:
            cols_to_show.append(f"{cuisine_col}_deduped")
        if f"{cuisine_col}_encoded" in df_debug.columns:
            cols_to_show.append(f"{cuisine_col}_encoded")
        # Also show a few other columns if they exist
        for col in ["Dataset_ID", "index", "ingredients"]:
            if col in df_debug.columns and col not in cols_to_show:
                cols_to_show.append(col)
        
        logger.info(f"\n{df_debug[cols_to_show].head(10).to_string()}")
        
        # Show detailed sample of cuisine transformations
        if f"{cuisine_col}_deduped" in df_debug.columns:
            logger.info(f"\nDetailed sample of cuisine transformations (first 5 rows):")
            for idx in range(min(5, len(df_debug))):
                orig = df_debug[cuisine_col].iloc[idx]
                deduped = df_debug[f"{cuisine_col}_deduped"].iloc[idx]
                encoded = df_debug[f"{cuisine_col}_encoded"].iloc[idx]
                logger.info(f"  Row {idx}:")
                logger.info(f"    Original: {orig}")
                logger.info(f"    Deduped:  {deduped}")
                logger.info(f"    Encoded:  {encoded}")
    else:
        logger.warning(f"Combined dataset not found at {combined_dataset_path} - skipping Step 6")

    # Final cleanup: Remove intermediate parquet files, keep JSON files and main dataset
    logger.info("=" * 60)
    logger.info("Final cleanup: Removing intermediate parquet files")
    logger.info("=" * 60)
    preserve_files = [
        str(combined_dataset_path),
        str(output_path) if 'output_path' in locals() and output_path.exists() else None,
    ]
    preserve_files = [f for f in preserve_files if f]  # Remove None values
    
    # Also clean up temporary prepared file if it exists
    prepared_parquet = baseline_parquet.parent / "_cuisine_prepared.parquet"
    if prepared_parquet.exists():
        try:
            prepared_parquet.unlink()
            logger.info(f"[cleanup] Deleted temporary file: {prepared_parquet}")
        except Exception as e:
            logger.warning(f"[cleanup] Failed to delete {prepared_parquet}: {e}")
    
    _cleanup_paths(cfg, logger, preserve_files=preserve_files)
    
    logger.info("=" * 60)
    logger.info(f"Output directory: {output_dir}")
    logger.info(f"  - Dedupe map (JSONL): {dedupe_map_path}")
    logger.info(f"  - Token→ID map (JSON): {cuisine_token_to_id}")
    logger.info(f"  - ID→Token map (JSON): {cuisine_id_to_token}")
    if 'output_path' in locals() and output_path.exists():
        logger.info(f"  - Final dataset: {output_path}")


if __name__ == "__main__":
    main()

================================================================================
FILE: preprocess_pipeline\pipeline\scripts\run_ingredient_ner.py
================================================================================
# pipeline/scripts/ingredient_ner_train.py
from __future__ import annotations

from pathlib import Path
import argparse
import sys as _sys

# Make `pipeline/` importable when running from repo root
_SYS_PATH_ROOT = Path.cwd() / "pipeline"
if str(_SYS_PATH_ROOT) not in _sys.path:
    _sys.path.append(str(_SYS_PATH_ROOT))

import yaml

from common.logging_setup import setup_logging
from ingredient_ner.config import load_configs_from_dict, DATA, TRAIN, OUT
from ingredient_ner.utils import set_global_seed
from ingredient_ner.data_prep import prepare_docbins_from_config
from ingredient_ner.training import train_ner_from_docbins


def main() -> None:
    ap = argparse.ArgumentParser(description="Train ingredient NER model")
    ap.add_argument(
        "--config",
        type=str,
        default="pipeline/config/ingredient_ner.yaml",
        help="Path to ingredient NER config YAML",
    )
    args = ap.parse_args()

    config_path = Path(args.config)
    if not config_path.exists():
        raise FileNotFoundError(f"Config not found: {config_path}")

    # Load full pipeline config once
    with open(config_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if not isinstance(cfg, dict):
        raise ValueError(f"YAML at {config_path} did not parse to a dict.")

    # Logging first
    setup_logging(cfg)

    # Populate DATA / TRAIN / OUT globals for ingredient_ner modules
    load_configs_from_dict(cfg)

    # Seed + run training pipeline
    set_global_seed(TRAIN.RANDOM_SEED)
    prepare_docbins_from_config()
    train_ner_from_docbins()
    
    # Cleanup: Remove intermediate docbins after training (keep only the final model)
    import logging
    import shutil
    logger = logging.getLogger(__name__)
    
    cleanup_docbins = cfg.get("ner", {}).get("cleanup_docbins", True)
    if cleanup_docbins:
        logger.info("=" * 60)
        logger.info("Cleaning up intermediate training artifacts (docbins)")
        logger.info("=" * 60)
        
        docbin_dirs = [OUT.TRAIN_DIR, OUT.VALID_DIR]
        for docbin_dir in docbin_dirs:
            if docbin_dir.exists():
                try:
                    # Count files before deletion
                    spacy_files = list(docbin_dir.glob("*.spacy"))
                    if spacy_files:
                        logger.info(f"Removing {len(spacy_files)} docbin shard(s) from {docbin_dir}")
                        shutil.rmtree(docbin_dir)
                        logger.info(f"Deleted {docbin_dir}")
                    else:
                        logger.debug(f"No docbin files found in {docbin_dir}")
                except Exception as e:
                    logger.warning(f"Failed to delete {docbin_dir}: {e}")
        
        logger.info("Training complete. Final model saved at:")
        logger.info(f"  - {OUT.MODEL_DIR}")
        logger.info("Intermediate docbins have been cleaned up.")
    else:
        logger.info("Training complete. Final model saved at:")
        logger.info(f"  - {OUT.MODEL_DIR}")
        logger.info("Docbins preserved (cleanup_docbins=false in config)")


if __name__ == "__main__":
    main()

================================================================================
FILE: preprocess_pipeline\pipeline\scripts\run_ingrnorm.py
================================================================================

from __future__ import annotations
import argparse, logging, os, glob
from pathlib import Path
import sys as _sys
import pathlib as _pathlib
import yaml
import pyarrow.parquet as pq

# Make `pipeline/` importable when running from repo root
_SYS_PATH_ROOT = _pathlib.Path.cwd() / "pipeline"
if str(_SYS_PATH_ROOT) not in _sys.path:
    _sys.path.append(str(_SYS_PATH_ROOT))

from common.logging_setup import setup_logging
from ingrnorm.io import materialize_parquet_source
from ingrnorm.parquet_utils import vocab_from_parquet_listcol
from ingrnorm.spacy_normalizer import apply_spacy_normalizer_to_parquet
from ingrnorm.sbert_dedupe import sbert_dedupe
from ingrnorm.w2v_dedupe import w2v_dedupe
from ingrnorm.dedupe_map import apply_map_to_parquet_streaming, write_jsonl_map
from ingrnorm.encoder import IngredientEncoder

logger = logging.getLogger("ingrnorm")

def _cleanup_paths(cfg: dict, logger: logging.Logger, preserve_files: list[str] = None):
    """
    Clean up old artifacts if cleanup is enabled in config.
    By default, deletes parquet files but keeps JSON/JSONL files.
    
    Args:
        cfg: Configuration dict
        logger: Logger instance
        preserve_files: List of file paths/patterns to preserve (e.g., main datasets)
    """
    cleanup_cfg = cfg.get("cleanup", {})
    if not cleanup_cfg.get("enabled", False):
        logger.info("Cleanup disabled – skipping file deletions.")
        return
    
    preserve_files = preserve_files or []
    preserve_patterns = []
    for p in preserve_files:
        try:
            if Path(p).exists():
                preserve_patterns.append(Path(p).resolve())
            else:
                # If path doesn't exist, store as string for pattern matching
                preserve_patterns.append(p)
        except (OSError, ValueError):
            preserve_patterns.append(p)
    
    paths = cleanup_cfg.get("paths", [])
    if not paths:
        logger.info("Cleanup enabled but no paths specified.")
        return
    
    deleted_count = 0
    for pattern in paths:
        for f in glob.glob(pattern):
            f_path = Path(f).resolve()
            
            # Skip if file should be preserved
            should_preserve = False
            for preserve in preserve_patterns:
                if isinstance(preserve, Path):
                    if preserve.exists() and f_path.samefile(preserve):
                        should_preserve = True
                        break
                    # Also check if preserve is a pattern that matches
                    if str(f_path).endswith(str(preserve)) or str(preserve) in str(f_path):
                        should_preserve = True
                        break
                else:
                    # String pattern matching
                    if str(preserve) in str(f_path) or str(f_path).endswith(str(preserve)):
                        should_preserve = True
                        break
            
            if should_preserve:
                logger.debug(f"[cleanup] Preserving {f}")
                continue
            
            # Only delete parquet files, keep JSON/JSONL files
            if f.endswith('.parquet'):
                try:
                    os.remove(f)
                    logger.info(f"[cleanup] Deleted {f}")
                    deleted_count += 1
                except FileNotFoundError:
                    pass
                except Exception as e:
                    logger.warning(f"[cleanup] Failed to delete {f}: {e}")
            else:
                logger.debug(f"[cleanup] Keeping non-parquet file: {f}")
    
    if deleted_count > 0:
        logger.info(f"[cleanup] Cleaned up {deleted_count} parquet file(s)")

def _as_path(d: dict, key: str) -> Path:
    v = d.get(key)
    if v is None:
        raise ValueError(f"Missing required path key: '{key}'")
    return Path(v)

def _exists(p: Path) -> bool:
    """Check if path exists, handling permission errors gracefully."""
    try:
        return p.exists()
    except (OSError, PermissionError):
        return False

def _validate_config(cfg: dict, logger: logging.Logger) -> None:
    """Validate required config sections and values."""
    required_sections = ["data", "output"]
    for section in required_sections:
        if section not in cfg:
            raise ValueError(f"Missing required config section: '{section}'")
    
    data_cfg = cfg.get("data", {})
    if "input_path" not in data_cfg:
        raise ValueError("Missing required config: data.input_path")
    
    out_cfg = cfg.get("output", {})
    required_outputs = ["baseline_parquet", "dedup_parquet", "cosine_map_path", "unified_parquet"]
    for key in required_outputs:
        if key not in out_cfg:
            raise ValueError(f"Missing required config: output.{key}")
    
    input_path = Path(data_cfg["input_path"])
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")
    
    logger.debug("Config validation passed")

def _run_stage1_spacy_normalization(
    input_path: Path,
    ner_col: str,
    chunksize: int,
    baseline_parquet: Path,
    list_col_for_vocab: str,
    sbert_cfg: dict,
    do_write_parquet: bool,
    args,
    logger: logging.Logger,
) -> None:
    """Stage 1: spaCy normalization."""
    tmp_raw_parquet = baseline_parquet.with_name("_raw_source_for_spacy.parquet")
    if do_write_parquet and (args.force or not _exists(baseline_parquet)):
        logger.info("Stage 1: spaCy normalization → baseline_parquet (NER_clean)")

        logger.info("[stage1] materialize_parquet_source starting…")
        src_parquet = materialize_parquet_source(input_path, ner_col, chunksize, tmp_raw_parquet)
        logger.info(f"[stage1] materialize_parquet_source done → {src_parquet}")

        baseline_parquet.parent.mkdir(parents=True, exist_ok=True)
        logger.info("[stage1] apply_spacy_normalizer_to_parquet starting…")
        # Use configurable batch size and n_process for better performance
        spacy_batch_size = int(sbert_cfg.get("spacy_batch_size", 512))
        spacy_n_process = int(sbert_cfg.get("spacy_n_process", 0))  # 0=auto, 1=single-threaded
        apply_spacy_normalizer_to_parquet(
            in_parquet=str(src_parquet),
            out_parquet=str(baseline_parquet),
            list_col=ner_col,
            out_col=list_col_for_vocab,
            spacy_model=sbert_cfg.get("spacy_model", "en_core_web_sm"),
            batch_size=spacy_batch_size,
            n_process=spacy_n_process,
        )
        logger.info("[stage1] apply_spacy_normalizer_to_parquet done")

        if src_parquet == tmp_raw_parquet and tmp_raw_parquet.exists():
            try:
                tmp_raw_parquet.unlink()
                logger.info(f"[stage1] Removed temp parquet {tmp_raw_parquet}")
            except (OSError, PermissionError) as e:
                logger.warning(f"[stage1] Could not remove temp file {tmp_raw_parquet}: {e}")
        logger.info(f"Saved baseline Parquet → {baseline_parquet}")
    else:
        logger.info("Stage 1: spaCy normalization – skipped (exists or disabled)")

def _run_stage2_dedupe(
    baseline_parquet: Path,
    dedupe_map_path: Path,
    list_col_for_vocab: str,
    vocab: dict,
    use_sbert: bool,
    use_w2v: bool,
    sbert_cfg: dict,
    w2v_cfg: dict,
    args,
    logger: logging.Logger,
) -> None:
    """Stage 2: Build dedupe map."""
    if (use_sbert or use_w2v) and (args.force or not _exists(dedupe_map_path)):
        if len(vocab) == 0:
            logger.warning("[dedupe] Vocab empty; skipping dedupe + map application.")
            return
        
        if use_sbert:
            logger.info("Stage 2: SBERT de-dupe – building phrase map")
            mapping = sbert_dedupe(
                vocab_counter=vocab,
                out_path=str(dedupe_map_path),
                model_name=sbert_cfg.get("model", "all-MiniLM-L6-v2"),
                threshold=float(sbert_cfg.get("threshold", 0.88)),
                topk=int(sbert_cfg.get("topk", 25)),
                min_len=int(sbert_cfg.get("min_len", 2)),
                require_token_overlap=bool(sbert_cfg.get("require_token_overlap", True)),
                block_generic_as_canon=bool(sbert_cfg.get("block_generic_as_canon", True)),
            )
            write_jsonl_map(mapping, dedupe_map_path)
        elif use_w2v:
            logger.info("Stage 2: W2V de-dupe – building phrase map")
            mapping = w2v_dedupe(
                vocab_counter=vocab,
                corpus_parquet=str(baseline_parquet),
                list_col=list_col_for_vocab,
                model_cache_path=Path(dedupe_map_path).with_suffix(".w2v"),
                vector_size=int(w2v_cfg.get("vector_size", 100)),
                window=int(w2v_cfg.get("window", 5)),
                min_count=int(w2v_cfg.get("min_count", 1)),
                workers=int(w2v_cfg.get("workers", 4)),
                sg=int(w2v_cfg.get("sg", 1)),
                epochs=int(w2v_cfg.get("epochs", 8)),
                threshold=float(w2v_cfg.get("threshold", 0.85)),
                topk=int(w2v_cfg.get("topk", 25)),
                out_path=str(dedupe_map_path),
            )
            write_jsonl_map(mapping, dedupe_map_path)
        logger.info(f"Saved dedupe map → {dedupe_map_path}")
    else:
        logger.info("Stage 2: Dedupe – skipped (exists or disabled)")

def _run_stage3_apply_map(
    baseline_parquet: Path,
    final_dedup_parquet: Path,
    dedupe_map_path: Path,
    list_col_for_vocab: str,
    do_apply_map: bool,
    args,
    logger: logging.Logger,
) -> None:
    """Stage 3: Apply dedupe map."""
    if do_apply_map and (args.force or not _exists(final_dedup_parquet)):
        if not _exists(dedupe_map_path):
            logger.warning(f"[apply] Map not found at {dedupe_map_path}; skipping Stage 3.")
        else:
            logger.info("Stage 3: Apply dedupe map → final deduped Parquet")
            apply_map_to_parquet_streaming(
                in_path=str(baseline_parquet),
                out_path=str(final_dedup_parquet),
                mapping=str(dedupe_map_path),
                list_col=list_col_for_vocab,
            )
            logger.info(f"Saved final deduped Parquet → {final_dedup_parquet}")
    else:
        logger.info("Stage 3: Apply map – skipped (exists or disabled)")

def _run_stage4_encode(
    baseline_parquet: Path,
    final_dedup_parquet: Path,
    unified_parquet: Path,
    ing_id_to_token: Path,
    ing_token_to_id: Path,
    list_col_for_vocab: str,
    enc_cfg: dict,
    do_encode_ids: bool,
    args,
    logger: logging.Logger,
) -> None:
    """Stage 4: Encode to IDs."""
    if do_encode_ids and (args.force or not _exists(unified_parquet)):
        logger.info("Stage 4: Encode to IDs – starting")
        source_parquet = final_dedup_parquet if _exists(final_dedup_parquet) else baseline_parquet
        logger.info(f"[encode] Source selected → {source_parquet}")
        enc_min_freq    = int(enc_cfg.get("min_freq", 1))
        dataset_id      = int(enc_cfg.get("dataset_id", 1))
        ingredients_col = enc_cfg.get("ingredients_col", list_col_for_vocab)

        enc = IngredientEncoder(min_freq=enc_min_freq)
        logger.info(f"[encode] Fitting encoder from {source_parquet} (col={ingredients_col}, min_freq={enc_min_freq}) …")
        enc.fit_from_parquet_streaming(source_parquet, col=ingredients_col, min_freq=enc_min_freq).freeze()

        ing_id_to_token.parent.mkdir(parents=True, exist_ok=True)
        enc.save_maps(id_to_token_path=ing_id_to_token, token_to_id_path=ing_token_to_id)
        logger.info(f"Saved encoder maps → {ing_id_to_token}, {ing_token_to_id}")

        logger.info(f"[encode] Writing → {unified_parquet}")
        unified_parquet.parent.mkdir(parents=True, exist_ok=True)
        enc.encode_parquet_streaming(
            parquet_path=source_parquet,
            out_parquet_path=unified_parquet,
            dataset_id=dataset_id,
            col=ingredients_col,
            compression="zstd",
        )
        logger.info(f"Saved unified encoded Parquet → {unified_parquet}")
    else:
        logger.info("Stage 4: Encode to IDs – skipped (exists or disabled)")

def main():
    ap = argparse.ArgumentParser(description="Run ingrnorm workflow (spaCy → dedupe → apply → encode)")
    ap.add_argument("--config", type=str, default="pipeline/config/ingrnorm.yaml", help="Path to ingrnorm config YAML")
    ap.add_argument("--force", action="store_true", help="Rebuild artifacts even if they exist")
    args = ap.parse_args()

    cfg_path = Path(args.config)
    if not cfg_path.exists():
        raise FileNotFoundError(f"Config not found: {cfg_path}")
    with open(cfg_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    setup_logging(cfg)
    logger = logging.getLogger("ingrnorm")
    _validate_config(cfg, logger)
    
    # Cleanup old artifacts (preserve main input dataset)
    data_cfg = cfg.get("data", {})
    input_path = Path(data_cfg.get("input_path", ""))
    preserve_files = [str(input_path)] if input_path.exists() else []
    _cleanup_paths(cfg, logger, preserve_files=preserve_files)

    data_cfg   = cfg.get("data", {})
    out_cfg    = cfg.get("output", {})
    stages_cfg = cfg.get("stages", {})
    w2v_cfg    = cfg.get("w2v", {})
    sbert_cfg  = cfg.get("sbert", {})
    enc_cfg    = cfg.get("encoder", {})

    input_path          = _as_path(data_cfg, "input_path")
    ner_col             = data_cfg.get("ner_col", "NER")
    chunksize           = int(data_cfg.get("chunksize", 200_000))

    baseline_parquet    = _as_path(out_cfg, "baseline_parquet")
    final_dedup_parquet = _as_path(out_cfg, "dedup_parquet")
    dedupe_map_path     = _as_path(out_cfg, "cosine_map_path")
    list_col_for_vocab  = out_cfg.get("list_col_for_vocab", "NER_clean")

    unified_parquet     = _as_path(out_cfg, "unified_parquet")
    ing_id_to_token     = _as_path(out_cfg, "ingredient_id_to_token")
    ing_token_to_id     = _as_path(out_cfg, "ingredient_token_to_id")

    do_write_parquet = bool(stages_cfg.get("write_parquet", True))
    use_sbert        = bool(stages_cfg.get("sbert_dedupe", True))
    use_w2v          = bool(stages_cfg.get("w2v_dedupe", False))
    do_apply_map     = bool(stages_cfg.get("apply_cosine_map", True))
    do_encode_ids    = bool(stages_cfg.get("encode_ids", True))

    if use_sbert and use_w2v:
        logger.warning("[stages] Both sbert_dedupe and w2v_dedupe are True; proceeding with SBERT only.")
        use_w2v = False

    # Stage 1: spaCy normalization
    _run_stage1_spacy_normalization(
        input_path, ner_col, chunksize, baseline_parquet, list_col_for_vocab,
        sbert_cfg, do_write_parquet, args, logger
    )

    # Probe
    try:
        if _exists(baseline_parquet):
            pf = pq.ParquetFile(str(baseline_parquet))
            if pf.num_row_groups > 0:
                tbl = pf.read_row_group(0, columns=[list_col_for_vocab])
                s = tbl.to_pandas()[list_col_for_vocab]
                non_empty = s.explode().dropna().astype(str).str.strip()
                logger.info(f"[probe] RG0 tokens: non-empty={ (non_empty != '').sum() }, unique={ non_empty.nunique() }")
    except Exception as e:
        logger.info(f"[probe] Skipped due to error: {e}")

    if not _exists(baseline_parquet):
        raise FileNotFoundError(
            f"baseline_parquet not found at {baseline_parquet}. "
            f"Run Stage 1 (spaCy normalization) first or check input_path: {input_path}"
        )

    # Build vocab
    min_freq_for_vocab = int(sbert_cfg.get("min_freq_for_vocab", w2v_cfg.get("min_freq_for_vocab", 1)))
    vocab = vocab_from_parquet_listcol(str(baseline_parquet), col=list_col_for_vocab, min_freq=min_freq_for_vocab)
    logger.info(f"[dedupe] Vocab size: {len(vocab)} (min_freq={min_freq_for_vocab})")
    if len(vocab) == 0:
        logger.warning(f"[dedupe] Vocabulary is empty. Check column '{list_col_for_vocab}' in {baseline_parquet}")

    # Stage 2: Dedupe (build map)
    _run_stage2_dedupe(
        baseline_parquet, dedupe_map_path, list_col_for_vocab, vocab,
        use_sbert, use_w2v, sbert_cfg, w2v_cfg, args, logger
    )

    # Stage 3: Apply map
    _run_stage3_apply_map(
        baseline_parquet, final_dedup_parquet, dedupe_map_path,
        list_col_for_vocab, do_apply_map, args, logger
    )

    # Stage 4: Encode
    _run_stage4_encode(
        baseline_parquet, final_dedup_parquet, unified_parquet,
        ing_id_to_token, ing_token_to_id, list_col_for_vocab,
        enc_cfg, do_encode_ids, args, logger
    )

    # Final cleanup: Remove intermediate parquet files, keep JSON files
    logger.info("=" * 60)
    logger.info("Final cleanup: Removing intermediate parquet files")
    logger.info("=" * 60)
    preserve_files = [str(input_path)] if input_path.exists() else []
    
    # Also clean up temporary raw source file if it exists
    tmp_raw_parquet = baseline_parquet.with_name("_raw_source_for_spacy.parquet")
    if tmp_raw_parquet.exists():
        try:
            tmp_raw_parquet.unlink()
            logger.info(f"[cleanup] Deleted temporary file: {tmp_raw_parquet}")
        except Exception as e:
            logger.warning(f"[cleanup] Failed to delete {tmp_raw_parquet}: {e}")
    
    _cleanup_paths(cfg, logger, preserve_files=preserve_files)
    
    logger.info("=" * 60)
    logger.info("Workflow complete.")
    logger.info(f"Kept JSON artifacts: {dedupe_map_path}, {ing_token_to_id}, {ing_id_to_token}")

if __name__ == "__main__":
    main()

================================================================================
FILE: preprocess_pipeline\scripts\1_run_ingrnorm.py
================================================================================

from __future__ import annotations
import argparse, logging, os, glob
from pathlib import Path
import sys as _sys
import pathlib as _pathlib
import yaml
import pyarrow.parquet as pq

# Make `preprocess_pipeline/` importable when running from repo root
_SYS_PATH_ROOT = _pathlib.Path.cwd() / "preprocess_pipeline/"
if str(_SYS_PATH_ROOT) not in _sys.path:
    _sys.path.append(str(_SYS_PATH_ROOT))

from common.logging_setup import setup_logging
from ingrnorm.io import materialize_parquet_source
from ingrnorm.parquet_utils import vocab_from_parquet_listcol
from ingrnorm.spacy_normalizer import apply_spacy_normalizer_to_parquet
from ingrnorm.sbert_dedupe import sbert_dedupe
from ingrnorm.w2v_dedupe import w2v_dedupe
from ingrnorm.dedupe_map import apply_map_to_parquet_streaming, write_jsonl_map
from ingrnorm.encoder import IngredientEncoder

logger = logging.getLogger("ingrnorm")

def _cleanup_paths(cfg: dict, logger: logging.Logger, preserve_files: list[str] = None):
    """
    Clean up old artifacts if cleanup is enabled in config.
    By default, deletes parquet files but keeps JSON/JSONL files.
    
    Args:
        cfg: Configuration dict
        logger: Logger instance
        preserve_files: List of file paths/patterns to preserve (e.g., main datasets)
    """
    cleanup_cfg = cfg.get("cleanup", {})
    if not cleanup_cfg.get("enabled", False):
        logger.info("Cleanup disabled – skipping file deletions.")
        return
    
    preserve_files = preserve_files or []
    preserve_patterns = []
    for p in preserve_files:
        try:
            if Path(p).exists():
                preserve_patterns.append(Path(p).resolve())
            else:
                # If path doesn't exist, store as string for pattern matching
                preserve_patterns.append(p)
        except (OSError, ValueError):
            preserve_patterns.append(p)
    
    paths = cleanup_cfg.get("paths", [])
    if not paths:
        logger.info("Cleanup enabled but no paths specified.")
        return
    
    deleted_count = 0
    for pattern in paths:
        for f in glob.glob(pattern):
            f_path = Path(f).resolve()
            
            # Skip if file should be preserved
            should_preserve = False
            for preserve in preserve_patterns:
                if isinstance(preserve, Path):
                    if preserve.exists() and f_path.samefile(preserve):
                        should_preserve = True
                        break
                    # Also check if preserve is a pattern that matches
                    if str(f_path).endswith(str(preserve)) or str(preserve) in str(f_path):
                        should_preserve = True
                        break
                else:
                    # String pattern matching
                    if str(preserve) in str(f_path) or str(f_path).endswith(str(preserve)):
                        should_preserve = True
                        break
            
            if should_preserve:
                logger.debug(f"[cleanup] Preserving {f}")
                continue
            
            # Only delete parquet files, keep JSON/JSONL files
            if f.endswith('.parquet'):
                try:
                    os.remove(f)
                    logger.info(f"[cleanup] Deleted {f}")
                    deleted_count += 1
                except FileNotFoundError:
                    pass
                except Exception as e:
                    logger.warning(f"[cleanup] Failed to delete {f}: {e}")
            else:
                logger.debug(f"[cleanup] Keeping non-parquet file: {f}")
    
    if deleted_count > 0:
        logger.info(f"[cleanup] Cleaned up {deleted_count} parquet file(s)")

def _as_path(d: dict, key: str) -> Path:
    v = d.get(key)
    if v is None:
        raise ValueError(f"Missing required path key: '{key}'")
    return Path(v)

def _exists(p: Path) -> bool:
    """Check if path exists, handling permission errors gracefully."""
    try:
        return p.exists()
    except (OSError, PermissionError):
        return False

def _validate_config(cfg: dict, logger: logging.Logger) -> None:
    """Validate required config sections and values."""
    required_sections = ["data", "output"]
    for section in required_sections:
        if section not in cfg:
            raise ValueError(f"Missing required config section: '{section}'")
    
    data_cfg = cfg.get("data", {})
    if "input_path" not in data_cfg:
        raise ValueError("Missing required config: data.input_path")
    
    out_cfg = cfg.get("output", {})
    required_outputs = ["baseline_parquet", "dedup_parquet", "cosine_map_path", "unified_parquet"]
    for key in required_outputs:
        if key not in out_cfg:
            raise ValueError(f"Missing required config: output.{key}")
    
    input_path = Path(data_cfg["input_path"])
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")
    
    logger.debug("Config validation passed")

def _run_stage1_spacy_normalization(
    input_path: Path,
    ner_col: str,
    chunksize: int,
    baseline_parquet: Path,
    list_col_for_vocab: str,
    sbert_cfg: dict,
    do_write_parquet: bool,
    args,
    logger: logging.Logger,
) -> None:
    """Stage 1: spaCy normalization."""
    tmp_raw_parquet = baseline_parquet.with_name("_raw_source_for_spacy.parquet")
    if do_write_parquet and (args.force or not _exists(baseline_parquet)):
        logger.info("Stage 1: spaCy normalization → baseline_parquet (NER_clean)")

        logger.info("[stage1] materialize_parquet_source starting…")
        src_parquet = materialize_parquet_source(input_path, ner_col, chunksize, tmp_raw_parquet)
        logger.info(f"[stage1] materialize_parquet_source done → {src_parquet}")

        baseline_parquet.parent.mkdir(parents=True, exist_ok=True)
        logger.info("[stage1] apply_spacy_normalizer_to_parquet starting…")
        # Use configurable batch size and n_process for better performance
        spacy_batch_size = int(sbert_cfg.get("spacy_batch_size", 512))
        spacy_n_process = int(sbert_cfg.get("spacy_n_process", 0))  # 0=auto, 1=single-threaded
        apply_spacy_normalizer_to_parquet(
            in_parquet=str(src_parquet),
            out_parquet=str(baseline_parquet),
            list_col=ner_col,
            out_col=list_col_for_vocab,
            spacy_model=sbert_cfg.get("spacy_model", "en_core_web_sm"),
            batch_size=spacy_batch_size,
            n_process=spacy_n_process,
        )
        logger.info("[stage1] apply_spacy_normalizer_to_parquet done")

        if src_parquet == tmp_raw_parquet and tmp_raw_parquet.exists():
            try:
                tmp_raw_parquet.unlink()
                logger.info(f"[stage1] Removed temp parquet {tmp_raw_parquet}")
            except (OSError, PermissionError) as e:
                logger.warning(f"[stage1] Could not remove temp file {tmp_raw_parquet}: {e}")
        logger.info(f"Saved baseline Parquet → {baseline_parquet}")
    else:
        logger.info("Stage 1: spaCy normalization – skipped (exists or disabled)")

def _run_stage2_dedupe(
    baseline_parquet: Path,
    dedupe_map_path: Path,
    list_col_for_vocab: str,
    vocab: dict,
    use_sbert: bool,
    use_w2v: bool,
    sbert_cfg: dict,
    w2v_cfg: dict,
    args,
    logger: logging.Logger,
) -> None:
    """Stage 2: Build dedupe map."""
    if (use_sbert or use_w2v) and (args.force or not _exists(dedupe_map_path)):
        if len(vocab) == 0:
            logger.warning("[dedupe] Vocab empty; skipping dedupe + map application.")
            return
        
        if use_sbert:
            logger.info("Stage 2: SBERT de-dupe – building phrase map")
            mapping = sbert_dedupe(
                vocab_counter=vocab,
                out_path=str(dedupe_map_path),
                model_name=sbert_cfg.get("model", "all-MiniLM-L6-v2"),
                threshold=float(sbert_cfg.get("threshold", 0.88)),
                topk=int(sbert_cfg.get("topk", 25)),
                min_len=int(sbert_cfg.get("min_len", 2)),
                require_token_overlap=bool(sbert_cfg.get("require_token_overlap", True)),
                block_generic_as_canon=bool(sbert_cfg.get("block_generic_as_canon", True)),
            )
            write_jsonl_map(mapping, dedupe_map_path)
        elif use_w2v:
            logger.info("Stage 2: W2V de-dupe – building phrase map")
            mapping = w2v_dedupe(
                vocab_counter=vocab,
                corpus_parquet=str(baseline_parquet),
                list_col=list_col_for_vocab,
                model_cache_path=Path(dedupe_map_path).with_suffix(".w2v"),
                vector_size=int(w2v_cfg.get("vector_size", 100)),
                window=int(w2v_cfg.get("window", 5)),
                min_count=int(w2v_cfg.get("min_count", 1)),
                workers=int(w2v_cfg.get("workers", 4)),
                sg=int(w2v_cfg.get("sg", 1)),
                epochs=int(w2v_cfg.get("epochs", 8)),
                threshold=float(w2v_cfg.get("threshold", 0.85)),
                topk=int(w2v_cfg.get("topk", 25)),
                out_path=str(dedupe_map_path),
            )
            write_jsonl_map(mapping, dedupe_map_path)
        logger.info(f"Saved dedupe map → {dedupe_map_path}")
    else:
        logger.info("Stage 2: Dedupe – skipped (exists or disabled)")

def _run_stage3_apply_map(
    baseline_parquet: Path,
    final_dedup_parquet: Path,
    dedupe_map_path: Path,
    list_col_for_vocab: str,
    do_apply_map: bool,
    args,
    logger: logging.Logger,
) -> None:
    """Stage 3: Apply dedupe map."""
    if do_apply_map and (args.force or not _exists(final_dedup_parquet)):
        if not _exists(dedupe_map_path):
            logger.warning(f"[apply] Map not found at {dedupe_map_path}; skipping Stage 3.")
        else:
            logger.info("Stage 3: Apply dedupe map → final deduped Parquet")
            apply_map_to_parquet_streaming(
                in_path=str(baseline_parquet),
                out_path=str(final_dedup_parquet),
                mapping=str(dedupe_map_path),
                list_col=list_col_for_vocab,
            )
            logger.info(f"Saved final deduped Parquet → {final_dedup_parquet}")
    else:
        logger.info("Stage 3: Apply map – skipped (exists or disabled)")

def _run_stage4_encode(
    baseline_parquet: Path,
    final_dedup_parquet: Path,
    unified_parquet: Path,
    ing_id_to_token: Path,
    ing_token_to_id: Path,
    list_col_for_vocab: str,
    enc_cfg: dict,
    do_encode_ids: bool,
    args,
    logger: logging.Logger,
) -> None:
    """Stage 4: Encode to IDs."""
    if do_encode_ids and (args.force or not _exists(unified_parquet)):
        logger.info("Stage 4: Encode to IDs – starting")
        source_parquet = final_dedup_parquet if _exists(final_dedup_parquet) else baseline_parquet
        logger.info(f"[encode] Source selected → {source_parquet}")
        enc_min_freq    = int(enc_cfg.get("min_freq", 1))
        dataset_id      = int(enc_cfg.get("dataset_id", 1))
        ingredients_col = enc_cfg.get("ingredients_col", list_col_for_vocab)

        enc = IngredientEncoder(min_freq=enc_min_freq)
        logger.info(f"[encode] Fitting encoder from {source_parquet} (col={ingredients_col}, min_freq={enc_min_freq}) …")
        enc.fit_from_parquet_streaming(source_parquet, col=ingredients_col, min_freq=enc_min_freq).freeze()

        ing_id_to_token.parent.mkdir(parents=True, exist_ok=True)
        enc.save_maps(id_to_token_path=ing_id_to_token, token_to_id_path=ing_token_to_id)
        logger.info(f"Saved encoder maps → {ing_id_to_token}, {ing_token_to_id}")

        logger.info(f"[encode] Writing → {unified_parquet}")
        unified_parquet.parent.mkdir(parents=True, exist_ok=True)
        enc.encode_parquet_streaming(
            parquet_path=source_parquet,
            out_parquet_path=unified_parquet,
            dataset_id=dataset_id,
            col=ingredients_col,
            compression="zstd",
        )
        logger.info(f"Saved unified encoded Parquet → {unified_parquet}")
    else:
        logger.info("Stage 4: Encode to IDs – skipped (exists or disabled)")

def main():
    ap = argparse.ArgumentParser(description="Run ingrnorm workflow (spaCy → dedupe → apply → encode)")
    ap.add_argument("--config", type=str, default="preprocess_pipeline/config/ingrnorm.yaml", help="Path to ingrnorm config YAML")
    ap.add_argument("--force", action="store_true", help="Rebuild artifacts even if they exist")
    args = ap.parse_args()

    cfg_path = Path(args.config)
    if not cfg_path.exists():
        raise FileNotFoundError(f"Config not found: {cfg_path}")
    with open(cfg_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    setup_logging(cfg)
    logger = logging.getLogger("ingrnorm")
    _validate_config(cfg, logger)
    
    # Cleanup old artifacts (preserve main input dataset)
    data_cfg = cfg.get("data", {})
    input_path = Path(data_cfg.get("input_path", ""))
    preserve_files = [str(input_path)] if input_path.exists() else []
    _cleanup_paths(cfg, logger, preserve_files=preserve_files)

    data_cfg   = cfg.get("data", {})
    out_cfg    = cfg.get("output", {})
    stages_cfg = cfg.get("stages", {})
    w2v_cfg    = cfg.get("w2v", {})
    sbert_cfg  = cfg.get("sbert", {})
    enc_cfg    = cfg.get("encoder", {})

    input_path          = _as_path(data_cfg, "input_path")
    ner_col             = data_cfg.get("ner_col", "NER")
    chunksize           = int(data_cfg.get("chunksize", 200_000))

    baseline_parquet    = _as_path(out_cfg, "baseline_parquet")
    final_dedup_parquet = _as_path(out_cfg, "dedup_parquet")
    dedupe_map_path     = _as_path(out_cfg, "cosine_map_path")
    list_col_for_vocab  = out_cfg.get("list_col_for_vocab", "NER_clean")

    unified_parquet     = _as_path(out_cfg, "unified_parquet")
    ing_id_to_token     = _as_path(out_cfg, "ingredient_id_to_token")
    ing_token_to_id     = _as_path(out_cfg, "ingredient_token_to_id")

    do_write_parquet = bool(stages_cfg.get("write_parquet", True))
    use_sbert        = bool(stages_cfg.get("sbert_dedupe", True))
    use_w2v          = bool(stages_cfg.get("w2v_dedupe", False))
    do_apply_map     = bool(stages_cfg.get("apply_cosine_map", True))
    do_encode_ids    = bool(stages_cfg.get("encode_ids", True))

    if use_sbert and use_w2v:
        logger.warning("[stages] Both sbert_dedupe and w2v_dedupe are True; proceeding with SBERT only.")
        use_w2v = False

    # Stage 1: spaCy normalization
    _run_stage1_spacy_normalization(
        input_path, ner_col, chunksize, baseline_parquet, list_col_for_vocab,
        sbert_cfg, do_write_parquet, args, logger
    )

    # Probe
    try:
        if _exists(baseline_parquet):
            pf = pq.ParquetFile(str(baseline_parquet))
            if pf.num_row_groups > 0:
                tbl = pf.read_row_group(0, columns=[list_col_for_vocab])
                s = tbl.to_pandas()[list_col_for_vocab]
                non_empty = s.explode().dropna().astype(str).str.strip()
                logger.info(f"[probe] RG0 tokens: non-empty={ (non_empty != '').sum() }, unique={ non_empty.nunique() }")
    except Exception as e:
        logger.info(f"[probe] Skipped due to error: {e}")

    if not _exists(baseline_parquet):
        raise FileNotFoundError(
            f"baseline_parquet not found at {baseline_parquet}. "
            f"Run Stage 1 (spaCy normalization) first or check input_path: {input_path}"
        )

    # Build vocab
    min_freq_for_vocab = int(sbert_cfg.get("min_freq_for_vocab", w2v_cfg.get("min_freq_for_vocab", 1)))
    vocab = vocab_from_parquet_listcol(str(baseline_parquet), col=list_col_for_vocab, min_freq=min_freq_for_vocab)
    logger.info(f"[dedupe] Vocab size: {len(vocab)} (min_freq={min_freq_for_vocab})")
    if len(vocab) == 0:
        logger.warning(f"[dedupe] Vocabulary is empty. Check column '{list_col_for_vocab}' in {baseline_parquet}")

    # Stage 2: Dedupe (build map)
    _run_stage2_dedupe(
        baseline_parquet, dedupe_map_path, list_col_for_vocab, vocab,
        use_sbert, use_w2v, sbert_cfg, w2v_cfg, args, logger
    )

    # Stage 3: Apply map
    _run_stage3_apply_map(
        baseline_parquet, final_dedup_parquet, dedupe_map_path,
        list_col_for_vocab, do_apply_map, args, logger
    )

    # Stage 4: Encode
    _run_stage4_encode(
        baseline_parquet, final_dedup_parquet, unified_parquet,
        ing_id_to_token, ing_token_to_id, list_col_for_vocab,
        enc_cfg, do_encode_ids, args, logger
    )

    # Final cleanup: Remove intermediate parquet files, keep JSON files
    logger.info("=" * 60)
    logger.info("Final cleanup: Removing intermediate parquet files")
    logger.info("=" * 60)
    preserve_files = [str(input_path)] if input_path.exists() else []
    
    # Also clean up temporary raw source file if it exists
    tmp_raw_parquet = baseline_parquet.with_name("_raw_source_for_spacy.parquet")
    if tmp_raw_parquet.exists():
        try:
            tmp_raw_parquet.unlink()
            logger.info(f"[cleanup] Deleted temporary file: {tmp_raw_parquet}")
        except Exception as e:
            logger.warning(f"[cleanup] Failed to delete {tmp_raw_parquet}: {e}")
    
    _cleanup_paths(cfg, logger, preserve_files=preserve_files)
    
    logger.info("=" * 60)
    logger.info("Workflow complete.")
    logger.info(f"Kept JSON artifacts: {dedupe_map_path}, {ing_token_to_id}, {ing_id_to_token}")

if __name__ == "__main__":
    main()

================================================================================
FILE: preprocess_pipeline\scripts\2_run_ingredient_ner.py
================================================================================
# preprocess_pipeline/scripts/ingredient_ner_train.py
from __future__ import annotations

from pathlib import Path
import argparse
import sys as _sys

# Make `preprocess_pipeline/` importable when running from repo root
_SYS_PATH_ROOT = Path.cwd() / "preprocess_pipeline"
if str(_SYS_PATH_ROOT) not in _sys.path:
    _sys.path.append(str(_SYS_PATH_ROOT))

import yaml

from common.logging_setup import setup_logging
from ingredient_ner.config import load_configs_from_dict, DATA, TRAIN, OUT
from ingredient_ner.utils import set_global_seed
from ingredient_ner.data_prep import prepare_docbins_from_config
from ingredient_ner.training import train_ner_from_docbins


def main() -> None:
    ap = argparse.ArgumentParser(description="Train ingredient NER model")
    ap.add_argument(
        "--config",
        type=str,
        default="preprocess_pipeline/config/ingredient_ner.yaml",
        help="Path to ingredient NER config YAML",
    )
    args = ap.parse_args()

    config_path = Path(args.config)
    if not config_path.exists():
        raise FileNotFoundError(f"Config not found: {config_path}")

    # Load full pipeline config once
    with open(config_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if not isinstance(cfg, dict):
        raise ValueError(f"YAML at {config_path} did not parse to a dict.")

    # Logging first
    setup_logging(cfg)

    # Populate DATA / TRAIN / OUT globals for ingredient_ner modules
    load_configs_from_dict(cfg)

    # Seed + run training pipeline
    set_global_seed(TRAIN.RANDOM_SEED)
    prepare_docbins_from_config()
    train_ner_from_docbins()
    
    # Cleanup: Remove intermediate docbins after training (keep only the final model)
    import logging
    import shutil
    logger = logging.getLogger(__name__)
    
    cleanup_docbins = cfg.get("ner", {}).get("cleanup_docbins", True)
    if cleanup_docbins:
        logger.info("=" * 60)
        logger.info("Cleaning up intermediate training artifacts (docbins)")
        logger.info("=" * 60)
        
        docbin_dirs = [OUT.TRAIN_DIR, OUT.VALID_DIR]
        for docbin_dir in docbin_dirs:
            if docbin_dir.exists():
                try:
                    # Count files before deletion
                    spacy_files = list(docbin_dir.glob("*.spacy"))
                    if spacy_files:
                        logger.info(f"Removing {len(spacy_files)} docbin shard(s) from {docbin_dir}")
                        shutil.rmtree(docbin_dir)
                        logger.info(f"Deleted {docbin_dir}")
                    else:
                        logger.debug(f"No docbin files found in {docbin_dir}")
                except Exception as e:
                    logger.warning(f"Failed to delete {docbin_dir}: {e}")
        
        logger.info("Training complete. Final model saved at:")
        logger.info(f"  - {OUT.MODEL_DIR}")
        logger.info("Intermediate docbins have been cleaned up.")
    else:
        logger.info("Training complete. Final model saved at:")
        logger.info(f"  - {OUT.MODEL_DIR}")
        logger.info("Docbins preserved (cleanup_docbins=false in config)")


if __name__ == "__main__":
    main()

================================================================================
FILE: preprocess_pipeline\scripts\3_combine_raw_dataset.py
================================================================================
"""
Script to combine multiple raw datasets into a unified format.

Processes CSV files from data/raw, extracts ingredients column,
and combines them into a single DataFrame with Dataset_ID, index, ingredients, cuisine.
"""

import argparse
import json
import logging
import re
from pathlib import Path
from typing import List, Optional
import sys as _sys

import pandas as pd
# Make `preprocess_pipeline/` importable when running from repo root
_SYS_PATH_ROOT = Path.cwd() / "preprocess_pipeline"
if str(_SYS_PATH_ROOT) not in _sys.path:
    _sys.path.append(str(_SYS_PATH_ROOT))

from common.logging_setup import setup_logging

# Import inference functionality
try:
    from ingredient_ner.inference import run_full_inference_from_config
    from ingredient_ner.config import load_inference_configs_from_yaml, DATA, OUT
    _HAS_INFERENCE = True
except ImportError as e:
    _HAS_INFERENCE = False
    print(f"Warning: Could not import inference modules: {e}")


def read_csv_with_fallback(path: Path, logger: logging.Logger) -> pd.DataFrame:
    encodings = ["utf-8", "cp1252", "latin-1"]
    for enc in encodings:
        try:
            logger.debug(f"Attempting to read {path.name} with encoding={enc}")
            df = pd.read_csv(path, encoding=enc, dtype=str, low_memory=False)
            logger.info(f"Successfully read {path.name} with encoding={enc} ({len(df):,} rows)")
            return df
        except UnicodeDecodeError as e:
            logger.warning(f"Failed to read {path.name} with encoding={enc}: {e}")
            continue
        except Exception as e:
            logger.exception(f"Unexpected error reading {path.name} with encoding={enc}: {e}")
            raise
    logger.warning(f"All standard encodings failed for {path.name}. Trying utf-8 with errors='replace'.")
    df = pd.read_csv(path, encoding="utf-8", errors='replace', dtype=str, low_memory=False)
    logger.info(f"Read {path.name} with utf-8 (errors=replace) ({len(df):,} rows)")
    return df


def find_ingredients_column(df: pd.DataFrame, logger: logging.Logger) -> Optional[str]:
    cols_lower = {col.lower(): col for col in df.columns}
    candidates = ["ingredients", "ingredient", "ing", "ingr", "ingredient_list", "ingredients_list"]
    for candidate in candidates:
        if candidate in cols_lower:
            actual_col = cols_lower[candidate]
            logger.info(f"Found ingredients column: '{actual_col}' (matched '{candidate}')")
            return actual_col
    logger.warning(f"Ingredients column not found. Available columns: {list(df.columns)[:10]}")
    return None


def process_dataset(csv_path: Path, dataset_id: int, logger: logging.Logger, cuisine_default: str = "unknown") -> Optional[pd.DataFrame]:
    try:
        df = read_csv_with_fallback(csv_path, logger)
        if df.empty:
            logger.warning(f"{csv_path.name} is empty, skipping")
            return None
        df.columns = df.columns.str.lower()
        ing_col = find_ingredients_column(df, logger)
        if ing_col is None:
            logger.error(f"Could not find ingredients column in {csv_path.name}, skipping")
            return None

        # Try to find cuisine column
        cuisine_col = None
        cols_lower = {col.lower(): col for col in df.columns}
        cuisine_candidates = ["cuisine", "cuisines", "country", "countries", "type", "category", "region"]
        for candidate in cuisine_candidates:
            if candidate in cols_lower:
                cuisine_col = cols_lower[candidate]
                logger.info(f"Found cuisine column: '{cuisine_col}' (matched '{candidate}')")
                break

        # Extract cuisine
        if cuisine_col:
            cuisine_values = df[cuisine_col].astype(str)

            def extract_cuisine(val):
                s = str(val).strip()
                if not s or s.lower() in ["nan", "none", ""]:
                    return cuisine_default
                if s == "[]" or s.lower() == "[]":
                    return cuisine_default
                if s.startswith("[") and s.endswith("]"):
                    try:
                        import ast
                        parsed = ast.literal_eval(s)
                        if isinstance(parsed, list):
                            non_empty = [str(x).strip() for x in parsed if str(x).strip() and str(x).strip().lower() not in ["nan", "none", ""]]
                            if non_empty:
                                return str(non_empty) if len(non_empty) > 1 else non_empty[0]
                            else:
                                return cuisine_default
                    except:
                        pass
                return s if s else cuisine_default

            cuisine_series = cuisine_values.apply(extract_cuisine)
            non_default = (cuisine_series != cuisine_default).sum()
            logger.info(f"Extracted cuisine values: {non_default:,} non-default out of {len(cuisine_series):,} rows")
        else:
            logger.info(f"No cuisine column found in {csv_path.name}, using default: '{cuisine_default}'")
            cuisine_series = pd.Series([cuisine_default] * len(df), dtype=str)

        # Vectorized cuisine extraction for missing values
        empty_mask = (cuisine_series == cuisine_default) | (cuisine_series.astype(str).str.strip() == "[]")
        if empty_mask.sum() > 0:
            # Load known cuisines
            cuisine_vocab_path = Path("./data/cuisine_encoded/cuisine_token_to_id.json")
            known_cuisines = set()
            if cuisine_vocab_path.exists():
                try:
                    with open(cuisine_vocab_path, "r", encoding="utf-8") as f:
                        cuisine_vocab = json.load(f)
                        known_cuisines = {k.lower().strip() for k in cuisine_vocab.keys() if k != "<UNK>"}
                        logger.info(f"Loaded {len(known_cuisines)} known cuisines from vocabulary")
                except:
                    logger.warning("Failed to load cuisine vocabulary")

            # Add common cuisines
            known_cuisines.update({
                "american", "mexican", "italian", "chinese", "japanese", "indian", "thai",
                "french", "greek", "spanish", "mediterranean", "asian", "korean", "vietnamese",
                "german", "british", "irish", "cuban", "caribbean", "brazilian", "african",
                "middle eastern", "middleeastern", "arab", "arabic", "persian", "turkish",
                "moroccan", "lebanese", "israeli", "jewish", "scandinavian", "nordic",
                "eastern european", "russian", "polish", "cajun", "creole", "southern",
                "tex-mex", "texmex", "latin", "south american", "peruvian", "argentinian"
            })

            # Select text columns
            exclude_cols = {ing_col.lower(), "cuisine", "cuisines", "country", "countries"}
            text_columns = [c for c in df.columns if c.lower() not in exclude_cols]
            for col in text_columns:
                df[col + "_lc"] = df[col].astype(str).str.lower()

            # Compile regex patterns
            pattern_dict = {c: re.compile(rf'\b{re.escape(c)}\b') for c in known_cuisines}

            def vectorized_cuisine_search(row):
                for col in text_columns:
                    val = row[col + "_lc"]
                    for c, pat in pattern_dict.items():
                        if pat.search(val):
                            return c
                return cuisine_default

            cuisine_series.loc[empty_mask] = df.loc[empty_mask].apply(vectorized_cuisine_search, axis=1)
            df.drop(columns=[col + "_lc" for col in text_columns], inplace=True)

        # Build result DataFrame
        result = pd.DataFrame({
            "Dataset_ID": dataset_id,
            "index": df.index,
            "ingredients": df[ing_col].astype(str),
            "cuisine": cuisine_series,
        })

        before_drop = len(result)
        empty_mask_final = (
            (result["cuisine"] == cuisine_default) |
            (result["cuisine"].astype(str).str.strip() == "") |
            (result["cuisine"].astype(str).str.strip() == "[]") |
            (result["cuisine"].astype(str).str.strip().isin(["nan", "None", "null"]))
        )
        result = result[~empty_mask_final].copy()
        dropped_count = before_drop - len(result)
        if dropped_count > 0:
            logger.info(f"Dropped {dropped_count:,} rows without cuisine labels")
        logger.info(f"Processed {csv_path.name}: {len(result):,} rows after drop")
        return result

    except Exception as e:
        logger.exception(f"Error processing {csv_path.name}: {e}")
        return None


def main():
    parser = argparse.ArgumentParser(description="Combine raw datasets into unified format")
    parser.add_argument("--data-dir", type=str, default="./data/raw")
    parser.add_argument("--output", type=str, default="./data/combined_raw_datasets.parquet")
    parser.add_argument("--cuisine-default", type=str, default="unknown")
    parser.add_argument("--config", type=str, default=None)
    parser.add_argument("--inference-config", type=str, default="./preprocess_pipeline/config/ingredient_ner_inference.yaml")
    parser.add_argument("--skip-inference", action="store_true")
    args = parser.parse_args()

    logger = setup_logging(args.config) if args.config else logging.getLogger(__name__)
    if not logger.handlers:
        logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s %(name)s: %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
        logger = logging.getLogger(__name__)

    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        logger.error(f"Data directory not found: {data_dir}")
        return 1

    excluded_files = {"wilmerarltstrmberg_data.csv", "sample_data.csv", "recipe_api_data.csv"}
    csv_files = sorted([f for f in data_dir.glob("*.csv") if f.name not in excluded_files])
    logger.info(f"Found {len(csv_files)} CSV files to process")

    all_dfs: List[pd.DataFrame] = []
    for dataset_id, csv_path in enumerate(csv_files, start=1):
        logger.info(f"[{dataset_id}/{len(csv_files)}] Processing {csv_path.name}...")
        df_processed = process_dataset(csv_path, dataset_id, logger, args.cuisine_default)
        if df_processed is not None:
            all_dfs.append(df_processed)

    if not all_dfs:
        logger.error("No datasets successfully processed")
        return 1

    combined = pd.concat(all_dfs, ignore_index=True)
    before_final_drop = len(combined)
    empty_mask_final = (
        (combined["cuisine"] == args.cuisine_default) |
        (combined["cuisine"].astype(str).str.strip().isin(["", "[]", "nan", "None", "null"]))
    )
    combined = combined[~empty_mask_final].copy()
    if before_final_drop - len(combined) > 0:
        logger.warning(f"Dropped {before_final_drop - len(combined)} rows without cuisine labels")

    combined["inferred_ingredients"] = None
    combined["encoded_ingredients"] = None

    # Inference
    if not args.skip_inference:
        if not _HAS_INFERENCE:
            logger.error("Inference modules not available")
            return 1
        try:
            from ingredient_ner.inference import predict_normalize_encode_structured, load_dedupe_and_maps_from_config
            load_inference_configs_from_yaml(Path(args.inference_config))
            temp_path = Path(args.output).with_suffix(".temp.parquet")
            temp_path.parent.mkdir(parents=True, exist_ok=True)
            combined.to_parquet(temp_path, index=False, compression="zstd")
            dedupe, tok2id = load_dedupe_and_maps_from_config()
            df_wide, df_tall = predict_normalize_encode_structured(
                nlp_dir=OUT.MODEL_DIR,
                data_path=temp_path,
                is_parquet=True,
                text_col="ingredients",
                dedupe=dedupe,
                tok2id=tok2id,
                out_path=None,
                batch_size=256,
                n_process=1,
                use_spacy_normalizer=True,
                spacy_model="en_core_web_sm",
            )
            if len(df_wide) == len(combined):
                combined["inferred_ingredients"] = df_wide["NER_clean"].tolist()
                combined["encoded_ingredients"] = df_wide["Ingredients"].tolist() if "Ingredients" in df_wide else None
            if temp_path.exists():
                temp_path.unlink()
        except Exception as e:
            logger.exception(f"Inference failed: {e}")

    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    if output_path.suffix == ".parquet":
        combined.to_parquet(output_path, index=False, compression="zstd")
    else:
        combined.to_csv(output_path, index=False)
    logger.info(f"Saved combined dataset to {output_path}")

    return 0


if __name__ == "__main__":
    exit(main())


================================================================================
FILE: preprocess_pipeline\scripts\4_apply_ingredient_ner.py
================================================================================
# preprocess_pipeline/scripts/apply_ingredient_ner.py
"""
Apply trained ingredient NER model to a new dataset.

This script:
1. Loads a trained spaCy NER model
2. Runs NER on a text column to extract ingredient entities
3. Normalizes and canonicalizes ingredients using dedupe map
4. Optionally maps to ingredient IDs using encoder maps
5. Outputs wide and tall format Parquet files

Example usage:
    python preprocess_pipeline/scripts/apply_ingredient_ner.py \
        --config preprocess_pipeline/config/ingredient_ner.yaml \
        --in-path data/new_dataset.parquet \
        --text-col ingredients_raw \
        --out-base data/new_dataset_ner
"""
from __future__ import annotations

from pathlib import Path
import argparse
import sys as _sys
import logging

# Make `preprocess_pipeline/` importable when running from repo root
_SYS_PATH_ROOT = Path.cwd() / "preprocess_pipeline"
if str(_SYS_PATH_ROOT) not in _sys.path:
    _sys.path.append(str(_SYS_PATH_ROOT))

import yaml

from common.logging_setup import setup_logging
from ingredient_ner.config import load_inference_configs_from_dict, DATA, OUT
from ingredient_ner.inference import run_full_inference_from_config
from ingredient_ner.utils import configure_device

logger = logging.getLogger(__name__)


def main() -> None:
    ap = argparse.ArgumentParser(
        description="Apply trained ingredient NER model to a new dataset",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage with config defaults
  python preprocess_pipeline/scripts/apply_ingredient_ner.py --text-col ingredients_raw --out-base data/output_ner

  # Override input path and sample
  python preprocess_pipeline/scripts/apply_ingredient_ner.py \\
      --in-path data/new_dataset.parquet \\
      --text-col ingredients_raw \\
      --out-base data/new_dataset_ner \\
      --sample-n 1000

  # Process first 100 rows
  python preprocess_pipeline/scripts/apply_ingredient_ner.py \\
      --text-col ingredients_raw \\
      --out-base data/output_ner \\
      --head-n 100
        """,
    )
    
    ap.add_argument(
        "--config",
        type=str,
        default="preprocess_pipeline/config/ingredient_ner_inference.yaml",
        help="Path to ingredient NER inference config YAML",
    )
    ap.add_argument(
        "--in-path",
        type=str,
        default=None,
        help="Input dataset path (CSV or Parquet). Overrides config data.input_path.",
    )
    ap.add_argument(
        "--text-col",
        type=str,
        default=None,
        help="Column name containing raw ingredient text to process. Overrides config inference.text_col.",
    )
    ap.add_argument(
        "--out-base",
        type=str,
        default=None,
        help="Base path for output files (will write <base>_wide.parquet and <base>_tall.parquet). Overrides config output.out_base.",
    )
    
    # Sampling options (mutually exclusive)
    sampling_group = ap.add_mutually_exclusive_group()
    sampling_group.add_argument(
        "--sample-n",
        type=int,
        default=None,
        help="Randomly sample N rows",
    )
    sampling_group.add_argument(
        "--sample-frac",
        type=float,
        default=None,
        help="Randomly sample fraction of rows (0.0-1.0)",
    )
    sampling_group.add_argument(
        "--head-n",
        type=int,
        default=None,
        help="Take first N rows",
    )
    
    # Performance options
    ap.add_argument(
        "--batch-size",
        type=int,
        default=256,
        help="Batch size for spaCy processing (default: 256)",
    )
    ap.add_argument(
        "--n-process",
        type=int,
        default=1,
        help="Number of processes for spaCy (default: 1). "
             "Note: >1 may not work on Windows with transformers. Use at your own risk.",
    )
    ap.add_argument(
        "--use-gpu",
        action="store_true",
        help="Attempt to use GPU (default: CPU). May not work on Windows.",
    )
    
    args = ap.parse_args()

    # Load config first to get defaults
    config_path = Path(args.config)
    if not config_path.exists():
        raise FileNotFoundError(f"Config not found: {config_path}")
    
    with open(config_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if not isinstance(cfg, dict):
        raise ValueError(f"YAML at {config_path} did not parse to a dict.")

    # Setup logging
    setup_logging(cfg)
    logger.info("=" * 60)
    logger.info("Applying Ingredient NER Model")
    logger.info("=" * 60)

    # Populate DATA / OUT globals from inference config
    load_inference_configs_from_dict(cfg)
    
    # Get inference settings from config
    inference_cfg = cfg.get("inference") or {}
    
    # Determine text column (CLI overrides config)
    text_col = args.text_col or inference_cfg.get("text_col")
    if not text_col:
        raise ValueError(
            "text_col must be specified either via --text-col argument "
            "or in config as inference.text_col"
        )
    
    # Determine output base (CLI overrides config)
    out_base = args.out_base or str(OUT.PRED_OUT)
    
    # Determine input path (CLI overrides config)
    if args.in_path:
        in_path = Path(args.in_path)
    elif DATA.TRAIN_PATH:
        in_path = DATA.TRAIN_PATH
        logger.info(f"Using input path from config: {in_path}")
    else:
        raise ValueError(
            "Input path must be specified either via --in-path argument "
            "or in config as data.input_path"
        )
    
    if not in_path.exists():
        raise FileNotFoundError(f"Input data not found: {in_path}")
    
    # Get normalization settings (CLI overrides config)
    use_spacy_normalizer = inference_cfg.get("use_spacy_normalizer", True)
    spacy_model = inference_cfg.get("spacy_model", "en_core_web_sm")
    
    # Get performance settings (CLI overrides config)
    # Use CLI args if provided, otherwise use config values
    batch_size = args.batch_size if args.batch_size != 256 else inference_cfg.get("batch_size", 256)
    n_process = args.n_process if args.n_process != 1 else inference_cfg.get("n_process", 1)
    use_gpu = args.use_gpu or inference_cfg.get("use_gpu", False)
    
    # Get sampling settings (CLI overrides config)
    sample_n = args.sample_n or inference_cfg.get("sample_n")
    sample_frac = args.sample_frac or inference_cfg.get("sample_frac")
    head_n = args.head_n or inference_cfg.get("head_n")
    
    # Configure device (CPU by default, GPU if requested)
    if use_gpu:
        configure_device()
    else:
        import spacy
        spacy.prefer_gpu()
        logger.info("Using CPU (set inference.use_gpu: true in config or use --use-gpu to attempt GPU acceleration)")
    
    logger.info(f"Input: {in_path}")
    logger.info(f"Text column: {text_col}")
    logger.info(f"Output base: {out_base}")
    logger.info(f"Model: {OUT.MODEL_DIR}")
    logger.info(f"Batch size: {batch_size}, Processes: {n_process}")
    
    if not OUT.MODEL_DIR.exists():
        raise FileNotFoundError(
            f"Model directory not found: {OUT.MODEL_DIR}. "
            f"Train a model first using: python preprocess_pipeline/scripts/run_ingredient_ner.py"
        )

    # Validate sampling options
    sampling_count = sum([
        sample_n is not None,
        sample_frac is not None,
        head_n is not None,
    ])
    if sampling_count > 1:
        raise ValueError("Only one sampling option (--sample-n, --sample-frac, --head-n) can be used at a time")

    # Run inference
    logger.info("Starting inference...")
    try:
        df_wide, df_tall = run_full_inference_from_config(
            text_col=text_col,
            out_base=Path(out_base),
            data_path=in_path,
            sample_n=sample_n,
            sample_frac=sample_frac,
            head_n=head_n,
            batch_size=batch_size,
            n_process=n_process,
            use_spacy_normalizer=use_spacy_normalizer,
            spacy_model=spacy_model,
        )
        print(df_wide.head())
        print(df_tall.head())
        logger.info("=" * 60)
        logger.info("Inference Complete")
        logger.info("=" * 60)
        logger.info(f"Processed {len(df_wide)} rows")
        logger.info(f"Extracted {len(df_tall)} ingredient entities")
        logger.info(f"Output files:")
        logger.info(f"  - {Path(out_base).with_name(Path(out_base).stem + '_wide.parquet')}")
        logger.info(f"  - {Path(out_base).with_name(Path(out_base).stem + '_tall.parquet')}")
        
    except Exception as e:
        logger.error(f"Inference failed: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()

================================================================================
FILE: preprocess_pipeline\scripts\5_run_cuisine_norm.py
================================================================================
"""
Script to normalize and encode cuisines using the ingrnorm pipeline.

Reuses the same normalization, deduplication, and encoding infrastructure
as run_ingrnorm.py, but applied to the cuisine column.
Handles splitting entries that contain multiple cuisines per row.
"""

from __future__ import annotations
import argparse
import logging
import ast
import json
import os
import glob
from pathlib import Path
import sys as _sys
import pathlib as _pathlib
import yaml
import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
import numpy as np

# Make `preprocess_pipeline/` importable when running from repo root
_SYS_PATH_ROOT = _pathlib.Path.cwd() / "preprocess_pipeline"
if str(_SYS_PATH_ROOT) not in _sys.path:
    _sys.path.append(str(_SYS_PATH_ROOT))

from common.logging_setup import setup_logging
from ingrnorm.io import materialize_parquet_source
from ingrnorm.parquet_utils import vocab_from_parquet_listcol
from ingrnorm.spacy_normalizer import apply_spacy_normalizer_to_parquet
from ingrnorm.sbert_dedupe import sbert_dedupe
from ingrnorm.w2v_dedupe import w2v_dedupe
from ingrnorm.dedupe_map import apply_map_to_parquet_streaming, write_jsonl_map
from ingrnorm.encoder import IngredientEncoder

logger = logging.getLogger("cuisine_norm")


def _filter_and_normalize_cuisine_mapping(mapping: dict[str, str]) -> dict[str, str]:
    """
    Post-process cuisine deduplication mapping to:
    1. Filter out non-cuisine entries (e.g., "friendly", "kid friendly")
    2. Normalize entries (e.g., "gujarati recipes" -> "gujarati")
    
    Args:
        mapping: Raw mapping from sbert_dedupe
        
    Returns:
        Filtered and normalized mapping
    """
    # Non-cuisine terms to exclude
    NON_CUISINES = {"friendly", "kid friendly"}
    
    # Normalization rules: patterns to clean up
    def normalize_cuisine(cuisine: str) -> str:
        """Normalize cuisine name by removing common suffixes."""
        cuisine_lower = cuisine.lower().strip()
        
        # Remove "recipe(s)" suffix
        if cuisine_lower.endswith(" recipes"):
            cuisine_lower = cuisine_lower[:-8]  # Remove " recipes"
        elif cuisine_lower.endswith(" recipe"):
            cuisine_lower = cuisine_lower[:-7]  # Remove " recipe"
        
        return cuisine_lower.strip()
    
    filtered_mapping = {}
    for src, tgt in mapping.items():
        src_lower = src.lower().strip()
        tgt_lower = tgt.lower().strip()
        
        # Skip non-cuisines
        if src_lower in NON_CUISINES or tgt_lower in NON_CUISINES:
            continue
        
        # Normalize target (canonical form)
        tgt_norm = normalize_cuisine(tgt)
        
        # Normalize source, but keep original if normalization doesn't change it
        src_norm = normalize_cuisine(src)
        
        # Only add mapping if source and target are different (after normalization)
        if src_norm != tgt_norm:
            filtered_mapping[src_norm] = tgt_norm
    
    return filtered_mapping


def _cleanup_paths(cfg: dict, logger: logging.Logger, preserve_files: list[str] = None):
    """
    Clean up old artifacts if cleanup is enabled in config.
    By default, deletes parquet files but keeps JSON/JSONL files.
    
    Args:
        cfg: Configuration dict
        logger: Logger instance
        preserve_files: List of file paths/patterns to preserve (e.g., main datasets)
    """
    cleanup_cfg = cfg.get("cleanup", {})
    if not cleanup_cfg.get("enabled", False):
        logger.info("Cleanup disabled – skipping file deletions.")
        return
    
    preserve_files = preserve_files or []
    preserve_patterns = []
    for p in preserve_files:
        try:
            preserve_patterns.append(Path(p).resolve())
        except (OSError, ValueError):
            # If path doesn't exist or can't be resolved, store as string for pattern matching
            preserve_patterns.append(p)
    
    paths = cleanup_cfg.get("paths", [])
    if not paths:
        logger.info("Cleanup enabled but no paths specified.")
        return
    
    deleted_count = 0
    for pattern in paths:
        for f in glob.glob(pattern):
            f_path = Path(f).resolve()
            
            # Skip if file should be preserved
            should_preserve = False
            for preserve in preserve_patterns:
                if isinstance(preserve, Path):
                    if preserve.exists() and f_path.samefile(preserve):
                        should_preserve = True
                        break
                    # Also check if preserve is a pattern that matches
                    if str(f_path).endswith(str(preserve)) or str(preserve) in str(f_path):
                        should_preserve = True
                        break
                else:
                    # String pattern matching
                    if str(preserve) in str(f_path) or str(f_path).endswith(str(preserve)):
                        should_preserve = True
                        break
            
            if should_preserve:
                logger.debug(f"[cleanup] Preserving {f}")
                continue
            
            # Only delete parquet files, keep JSON/JSONL files
            if f.endswith('.parquet'):
                try:
                    os.remove(f)
                    logger.info(f"[cleanup] Deleted {f}")
                    deleted_count += 1
                except FileNotFoundError:
                    pass
                except Exception as e:
                    logger.warning(f"[cleanup] Failed to delete {f}: {e}")
            else:
                logger.debug(f"[cleanup] Keeping non-parquet file: {f}")
    
    if deleted_count > 0:
        logger.info(f"[cleanup] Cleaned up {deleted_count} parquet file(s)")


def _needs_reapply_map(dedupe_map_path: Path, deduped_parquet: Path) -> bool:
    """Check if dedupe map needs to be reapplied (map is newer than deduped parquet)."""
    if not dedupe_map_path.exists():
        return False
    if not deduped_parquet.exists():
        return True
    try:
        map_mtime = dedupe_map_path.stat().st_mtime
        parquet_mtime = deduped_parquet.stat().st_mtime
        return map_mtime > parquet_mtime
    except (OSError, AttributeError):
        return False


def split_cuisine_entries(cuisine_value: str) -> list[str]:
    """
    Split cuisine entries that may contain multiple cuisines.
    Handles formats like:
    - "[American, Italian]" -> ["American", "Italian"]
    - "American, Italian" -> ["American", "Italian"]
    - "American & Italian" -> ["American", "Italian"]
    - "American" -> ["American"]
    - "Goan recipes" -> ["Goan"] (removes "recipes" suffix)
    """
    if pd.isna(cuisine_value) or not str(cuisine_value).strip():
        return []
    
    s = str(cuisine_value).strip()
    
    # Try parsing as list/JSON first
    if s.startswith("[") and s.endswith("]"):
        try:
            parsed = ast.literal_eval(s)
            if isinstance(parsed, list):
                cuisines = [str(x).strip() for x in parsed if str(x).strip()]
            else:
                cuisines = [s]
        except:
            try:
                parsed = json.loads(s)
                if isinstance(parsed, list):
                    cuisines = [str(x).strip() for x in parsed if str(x).strip()]
                else:
                    cuisines = [s]
            except:
                cuisines = [s]
    else:
        # Try splitting on comma
        if "," in s:
            parts = [x.strip() for x in s.split(",") if x.strip()]
            if len(parts) > 1:
                cuisines = parts
            else:
                cuisines = [s]
        else:
            # Try splitting on " & " or " and "
            found_split = False
            for sep in [" & ", " and ", " AND "]:
                if sep in s:
                    parts = [x.strip() for x in s.split(sep) if x.strip()]
                    if len(parts) > 1:
                        cuisines = parts
                        found_split = True
                        break
            if not found_split:
                # Single value
                cuisines = [s] if s else []
    
    # Clean up each cuisine: remove "recipes" suffix (case-insensitive)
    cleaned = []
    for cuisine in cuisines:
        if not cuisine:
            continue
        # Remove "recipes" from the end (case-insensitive, with optional whitespace)
        cuisine_clean = cuisine.strip()
        # Remove trailing "recipes" or " recipe" (singular or plural)
        for suffix in [" recipes", " recipe", "Recipes", "Recipe"]:
            if cuisine_clean.lower().endswith(suffix.lower()):
                cuisine_clean = cuisine_clean[:-len(suffix)].strip()
                break
        if cuisine_clean:
            cleaned.append(cuisine_clean)
    
    return cleaned


def prepare_cuisine_list_column(
    input_path: Path,
    cuisine_col: str,
    output_path: Path,
    logger: logging.Logger,
) -> Path:
    """
    Prepare cuisine column as a list column for normalization pipeline.
    Splits entries that contain multiple cuisines.
    """
    logger.info(f"Preparing cuisine list column from {input_path}...")
    
    # Read parquet file
    pf = pq.ParquetFile(str(input_path))
    logger.info(f"Input file has {pf.num_row_groups} row groups")
    
    # Process row groups and split cuisines
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    writer = None
    schema = None
    
    for rg_idx in range(pf.num_row_groups):
        logger.info(f"Processing row group {rg_idx + 1}/{pf.num_row_groups}...")
        
        # Read row group
        df = pf.read_row_group(rg_idx).to_pandas()
        
        if cuisine_col not in df.columns:
            logger.error(f"Column '{cuisine_col}' not found in input file")
            logger.error(f"Available columns: {list(df.columns)}")
            raise KeyError(f"Column '{cuisine_col}' not found")
        
        # Split cuisine entries into lists
        logger.info(f"Splitting cuisine entries in row group {rg_idx + 1}...")
        cuisine_lists = df[cuisine_col].apply(split_cuisine_entries)
        
        # Count splits for statistics
        splits = sum(1 for lst in cuisine_lists if len(lst) > 1)
        if splits > 0:
            logger.info(f"  Found {splits:,} entries with multiple cuisines (out of {len(cuisine_lists):,})")
        
        # Create new DataFrame with list column
        df_output = pd.DataFrame({
            cuisine_col: cuisine_lists,
        })
        
        # Convert to PyArrow table
        import pyarrow as pa
        table = pa.Table.from_pandas(df_output, preserve_index=False)
        
        # Set schema (list of strings)
        if schema is None:
            schema = pa.schema([
                pa.field(cuisine_col, pa.list_(pa.string())),
            ])
            writer = pq.ParquetWriter(str(output_path), schema, compression="zstd")
        
        writer.write_table(table)
    
    if writer:
        writer.close()
    
    logger.info(f"Saved prepared cuisine list column to {output_path}")
    return output_path


def main():
    ap = argparse.ArgumentParser(
        description="Normalize and encode cuisines using ingrnorm pipeline"
    )
    ap.add_argument(
        "--config",
        type=str,
        default="preprocess_pipeline/config/cuisnorm.yaml",
        help="Path to cuisine normalization config YAML (default: preprocess_pipeline/config/cuisnorm.yaml)",
    )
    ap.add_argument(
        "--input",
        type=str,
        default=None,
        help="Input parquet file with cuisine column (default: from config data.input_path)",
    )
    ap.add_argument(
        "--cuisine-col",
        type=str,
        default="cuisine",
        help="Name of cuisine column (default: cuisine)",
    )
    ap.add_argument(
        "--output-dir",
        type=str,
        default="./data/cuisine_normalized",
        help="Output directory for cuisine normalization artifacts (default: derived from config)",
    )
    ap.add_argument(
        "--force",
        action="store_true",
        help="Rebuild artifacts even if they exist",
    )
    args = ap.parse_args()
    
    # Load config
    cfg_path = Path(args.config)
    if not cfg_path.exists():
        raise FileNotFoundError(f"Config not found: {cfg_path}")
    with open(cfg_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    
    setup_logging(cfg)
    logger = logging.getLogger("cuisine_norm")
    
    # Get paths from config (with CLI overrides)
    data_cfg = cfg.get("data", {})
    out_cfg = cfg.get("output", {})
    
    # Validate config and cleanup old artifacts (preserve main dataset)
    combined_dataset_path = Path(data_cfg.get("input_path", "./data/encoded_combined_datasets.parquet"))
    _cleanup_paths(cfg, logger, preserve_files=[str(combined_dataset_path)])
    
    
    # Input path: use CLI arg if provided, otherwise from config
    if args.input:
        input_path = Path(args.input)
    else:
        input_path = Path(data_cfg.get("input_path", "./data/encoded_combined_datasets.parquet"))
    
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")
    
    # Output directory: use CLI arg if provided, otherwise derive from config
    if args.output_dir != "./data/cuisine_normalized":  # User provided custom path
        output_dir = Path(args.output_dir)
    else:
        # Derive from config output paths
        baseline_path = Path(out_cfg.get("baseline_parquet", "./data/cuisine_normalized/cuisine_baseline.parquet"))
        output_dir = baseline_path.parent
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Cuisine column: use CLI arg if provided, otherwise from config
    cuisine_col = args.cuisine_col if args.cuisine_col != "cuisine" else data_cfg.get("cuisine_col", "cuisine")
    
    # Get output paths from config
    baseline_parquet = Path(out_cfg.get("baseline_parquet", "./data/cuisine_normalized/cuisine_baseline.parquet"))
    final_dedup_parquet = Path(out_cfg.get("dedup_parquet", "./data/cuisine_normalized/cuisine_deduped.parquet"))
    dedupe_map_path = Path(out_cfg.get("cosine_map_path", "./data/cuisine_normalized/cuisine_dedupe_map.jsonl"))
    list_col_for_vocab = out_cfg.get("list_col_for_vocab", "cuisine_clean")
    unified_parquet = Path(out_cfg.get("unified_parquet", "./data/cuisine_encoded/cuisine_unified.parquet"))
    cuisine_id_to_token = Path(out_cfg.get("cuisine_id_to_token", "./data/cuisine_encoded/cuisine_id_to_token.json"))
    cuisine_token_to_id = Path(out_cfg.get("cuisine_token_to_id", "./data/cuisine_encoded/cuisine_token_to_id.json"))
    
    # Step 1: Prepare cuisine list column (split multi-cuisine entries)
    prepared_parquet = baseline_parquet.parent / "_cuisine_prepared.parquet"
    if args.force or not prepared_parquet.exists():
        logger.info("=" * 60)
        logger.info("Step 1: Preparing cuisine list column (splitting multi-cuisine entries)")
        logger.info("=" * 60)
        prepare_cuisine_list_column(input_path, cuisine_col, prepared_parquet, logger)
    else:
        logger.info("Step 1: Skipped (prepared file exists)")
    
    # Step 2: Apply spaCy normalization (reuse ingrnorm pipeline)
    if args.force or not baseline_parquet.exists():
        logger.info("=" * 60)
        logger.info("Step 2: spaCy normalization")
        logger.info("=" * 60)
        sbert_cfg = cfg.get("sbert", {})
        apply_spacy_normalizer_to_parquet(
            in_parquet=str(prepared_parquet),
            out_parquet=str(baseline_parquet),
            list_col=cuisine_col,
            out_col=list_col_for_vocab,
            spacy_model=sbert_cfg.get("spacy_model", "en_core_web_sm"),
            batch_size=int(sbert_cfg.get("spacy_batch_size", 512)),
            n_process=int(sbert_cfg.get("spacy_n_process", 1)),
        )
        logger.info(f"Saved baseline cuisine Parquet → {baseline_parquet}")
    else:
        logger.info("Step 2: Skipped (baseline file exists)")
    
    # Step 3: Build vocabulary
    min_freq_for_vocab = int(cfg.get("sbert", {}).get("min_freq_for_vocab", 1))
    vocab = vocab_from_parquet_listcol(
        str(baseline_parquet),
        col=list_col_for_vocab,
        min_freq=min_freq_for_vocab,
    )
    
    # Filter out non-cuisines from vocab before deduplication
    NON_CUISINES = {"friendly", "kid friendly"}
    vocab_filtered = {k: v for k, v in vocab.items() if k.lower().strip() not in NON_CUISINES}
    if len(vocab) != len(vocab_filtered):
        logger.info(f"Filtered {len(vocab) - len(vocab_filtered)} non-cuisine entries from vocab")
    vocab = vocab_filtered
    
    logger.info(f"Vocab size: {len(vocab):,} (min_freq={min_freq_for_vocab})")
    
    if len(vocab) == 0:
        logger.warning("Vocabulary is empty. Check cuisine_clean column.")
        return 1
    
    # Step 4: Deduplication (SBERT)
    stages_cfg = cfg.get("stages", {})
    use_sbert = bool(stages_cfg.get("sbert_dedupe", True))
    
    if use_sbert and (args.force or not dedupe_map_path.exists()):
        logger.info("=" * 60)
        logger.info("Step 3: SBERT deduplication")
        logger.info("=" * 60)
        sbert_cfg = cfg.get("sbert", {})
        mapping = sbert_dedupe(
            vocab_counter=vocab,
            out_path=str(dedupe_map_path),
            model_name=sbert_cfg.get("model", "all-MiniLM-L6-v2"),
            threshold=float(sbert_cfg.get("threshold", 0.88)),
            topk=int(sbert_cfg.get("topk", 25)),
            min_len=int(sbert_cfg.get("min_len", 2)),
            require_token_overlap=bool(sbert_cfg.get("require_token_overlap", True)),
            block_generic_as_canon=bool(sbert_cfg.get("block_generic_as_canon", True)),
        )
        
        # Post-process mapping: filter non-cuisines and normalize entries
        mapping = _filter_and_normalize_cuisine_mapping(mapping)
        logger.info(f"Post-processed mapping: {len(mapping):,} entries (after filtering/normalization)")
        
        write_jsonl_map(mapping, dedupe_map_path)
        logger.info(f"Saved dedupe map → {dedupe_map_path}")
    else:
        logger.info("Step 3: Skipped (dedupe map exists or disabled)")
    
    # Step 5: Apply dedupe map
    # The JSON map is the source of truth - always apply it to the parquet if it exists
    do_apply_map = bool(stages_cfg.get("apply_cosine_map", True))
    if do_apply_map and dedupe_map_path.exists():
        needs_reapply = _needs_reapply_map(dedupe_map_path, final_dedup_parquet)
        logger.info(f"Step 4: Checking dedupe map application...")
        logger.info(f"  - Map exists: {dedupe_map_path.exists()}")
        logger.info(f"  - Deduped parquet exists: {final_dedup_parquet.exists()}")
        logger.info(f"  - Needs reapply: {needs_reapply}")
        logger.info(f"  - Force flag: {args.force}")
        
        if args.force or not final_dedup_parquet.exists() or needs_reapply:
            logger.info("=" * 60)
            logger.info("Step 4: Applying dedupe map to parquet")
            logger.info("=" * 60)
            if needs_reapply and final_dedup_parquet.exists():
                logger.info(f"Dedupe map is newer than deduped parquet; re-applying map...")
            
            # Load and log a sample of the mapping to verify it's being read
            from ingrnorm.dedupe_map import load_jsonl_map
            mapping_dict = load_jsonl_map(dedupe_map_path)
            logger.info(f"Loaded {len(mapping_dict):,} mappings from {dedupe_map_path}")
            if mapping_dict:
                sample_items = list(mapping_dict.items())[:5]
                logger.info(f"Sample mappings: {sample_items}")
            
            # Check baseline parquet before applying map
            try:
                pf_baseline = pq.ParquetFile(str(baseline_parquet))
                if pf_baseline.num_row_groups > 0:
                    df_baseline_sample = pf_baseline.read_row_group(0).to_pandas()
                    logger.info(f"Baseline parquet: shape={df_baseline_sample.shape}, columns={list(df_baseline_sample.columns)}")
                    if list_col_for_vocab in df_baseline_sample.columns:
                        baseline_sample = df_baseline_sample[list_col_for_vocab].head(5)
                        logger.info(f"Baseline sample (before mapping):")
                        for idx, lst in enumerate(baseline_sample):
                            if isinstance(lst, (list, tuple, np.ndarray)) and len(lst) > 0:
                                logger.info(f"  Row {idx}: {lst[:3]}")
            except Exception as e:
                logger.warning(f"Could not inspect baseline parquet: {e}")
            
            apply_map_to_parquet_streaming(
                in_path=str(baseline_parquet),
                out_path=str(final_dedup_parquet),
                mapping=str(dedupe_map_path),
                list_col=list_col_for_vocab,
            )
            logger.info(f"Saved deduped cuisine Parquet → {final_dedup_parquet}")
            
            # Verify the mapping was applied by checking a sample
            try:
                pf_check = pq.ParquetFile(str(final_dedup_parquet))
                logger.info(f"Verification: Parquet has {pf_check.num_row_groups} row group(s)")
                if pf_check.num_row_groups > 0:
                    df_sample = pf_check.read_row_group(0).to_pandas()
                    logger.info(f"Verification: DataFrame shape: {df_sample.shape}")
                    logger.info(f"Verification: Columns: {list(df_sample.columns)}")
                    logger.info(f"Verification: Looking for column '{list_col_for_vocab}'")
                    
                    if list_col_for_vocab in df_sample.columns:
                        sample_lists = df_sample[list_col_for_vocab].head(10)
                        logger.info(f"Sample of deduped data (first 10 rows):")
                        non_empty_count = 0
                        for idx, lst in enumerate(sample_lists):
                            if isinstance(lst, (list, tuple, np.ndarray)):
                                if len(lst) > 0:
                                    logger.info(f"  Row {idx}: {lst[:5]}")  # Show first 5 items
                                    non_empty_count += 1
                                else:
                                    logger.info(f"  Row {idx}: [] (empty)")
                            else:
                                logger.info(f"  Row {idx}: {type(lst).__name__} = {lst}")
                        logger.info(f"Verification: {non_empty_count}/10 rows have non-empty lists")
                    else:
                        logger.warning(f"Verification: Column '{list_col_for_vocab}' not found in deduped parquet!")
            except Exception as e:
                logger.warning(f"Could not verify deduped data: {e}", exc_info=True)
        else:
            logger.info("Step 4: Skipped (deduped file exists and is up-to-date with map)")
    elif do_apply_map and not dedupe_map_path.exists():
        logger.warning(f"Step 4: Dedupe map not found at {dedupe_map_path} - skipping map application")
    else:
        logger.info("Step 4: Skipped (map application disabled)")
    
    # Step 6: Encode to IDs
    # Always re-encode if the deduped parquet is newer than the encoded parquet
    # (This ensures manual edits to the JSON map are reflected in the encoded output)
    do_encode_ids = bool(stages_cfg.get("encode_ids", True))
    
    # CRITICAL: If dedupe map exists and we're encoding, ensure it's been applied first
    if do_encode_ids and do_apply_map and dedupe_map_path.exists():
        if not final_dedup_parquet.exists() or _needs_reapply_map(dedupe_map_path, final_dedup_parquet):
            logger.info("=" * 60)
            logger.info("Step 4 (re-check): Ensuring dedupe map is applied before encoding")
            logger.info("=" * 60)
            apply_map_to_parquet_streaming(
                in_path=str(baseline_parquet),
                out_path=str(final_dedup_parquet),
                mapping=str(dedupe_map_path),
                list_col=list_col_for_vocab,
            )
            logger.info(f"Re-applied dedupe map → {final_dedup_parquet}")
    
    needs_reencode = False
    if unified_parquet.exists() and final_dedup_parquet.exists():
        try:
            unified_mtime = unified_parquet.stat().st_mtime
            deduped_mtime = final_dedup_parquet.stat().st_mtime
            if deduped_mtime > unified_mtime:
                needs_reencode = True
                logger.info(f"Deduped parquet is newer than encoded parquet; re-encoding needed...")
        except (OSError, AttributeError):
            pass
    # Also check if the dedupe map is newer than the encoded parquet (map was manually edited)
    if not needs_reencode and unified_parquet.exists() and dedupe_map_path.exists():
        try:
            unified_mtime = unified_parquet.stat().st_mtime
            map_mtime = dedupe_map_path.stat().st_mtime
            if map_mtime > unified_mtime:
                needs_reencode = True
                logger.info(f"Dedupe map is newer than encoded parquet; will re-apply map and re-encode...")
        except (OSError, AttributeError):
            pass
    
    if do_encode_ids and (args.force or not unified_parquet.exists() or needs_reencode):
        logger.info("=" * 60)
        logger.info("Step 5: Encoding cuisines to IDs")
        logger.info("=" * 60)
        # Always prefer deduped parquet if it exists (it has the applied dedupe map)
        # CRITICAL: If dedupe map exists, we MUST use the deduped parquet, not the baseline
        if dedupe_map_path.exists() and do_apply_map:
            if not final_dedup_parquet.exists():
                logger.warning(f"Dedupe map exists but deduped parquet not found! Applying map now...")
                apply_map_to_parquet_streaming(
                    in_path=str(baseline_parquet),
                    out_path=str(final_dedup_parquet),
                    mapping=str(dedupe_map_path),
                    list_col=list_col_for_vocab,
                )
            source_parquet = final_dedup_parquet
            logger.info(f"Using DEDUPED parquet (with applied map) → {source_parquet}")
        else:
            source_parquet = final_dedup_parquet if final_dedup_parquet.exists() else baseline_parquet
            logger.info(f"Using source parquet → {source_parquet}")
        
        enc_cfg = cfg.get("encoder", {})
        enc_min_freq = int(enc_cfg.get("min_freq", 1))
        dataset_id = int(enc_cfg.get("dataset_id", 1))
        ingredients_col = enc_cfg.get("ingredients_col", list_col_for_vocab)
        
        enc = IngredientEncoder(min_freq=enc_min_freq)
        logger.info(f"Fitting encoder from {source_parquet} (col={ingredients_col}, min_freq={enc_min_freq})...")
        enc.fit_from_parquet_streaming(source_parquet, col=ingredients_col, min_freq=enc_min_freq).freeze()
        
        cuisine_id_to_token.parent.mkdir(parents=True, exist_ok=True)
        enc.save_maps(id_to_token_path=cuisine_id_to_token, token_to_id_path=cuisine_token_to_id)
        logger.info(f"Saved encoder maps → {cuisine_id_to_token}, {cuisine_token_to_id}")
        
        logger.info(f"Writing encoded cuisines → {unified_parquet}")
        enc.encode_parquet_streaming(
            parquet_path=source_parquet,
            out_parquet_path=unified_parquet,
            dataset_id=dataset_id,
            col=ingredients_col,
            compression="zstd",
        )
        logger.info(f"Saved encoded cuisine Parquet → {unified_parquet}")
    else:
        logger.info("Step 5: Skipped (encoded file exists and is up-to-date, or disabled)")
    
    logger.info("=" * 60)
    logger.info("Cuisine normalization and encoding complete")
    
    # Step 7: Apply dedupe and encoding to the original combined dataset
    logger.info("=" * 60)
    logger.info("Step 6: Applying dedupe and encoding to combined dataset")
    logger.info("=" * 60)
    
    combined_dataset_path = Path(data_cfg.get("input_path", "./data/encoded_combined_datasets.parquet"))
    if combined_dataset_path.exists():
        logger.info(f"Loading combined dataset from {combined_dataset_path}")
        
        # Load the combined dataset
        pf_combined = pq.ParquetFile(str(combined_dataset_path))
        logger.info(f"Combined dataset has {pf_combined.num_row_groups} row group(s)")
        
        # Load dedupe map and encoder
        from ingrnorm.dedupe_map import load_jsonl_map
        dedupe_map = load_jsonl_map(dedupe_map_path)
        logger.info(f"Loaded {len(dedupe_map):,} dedupe mappings")
        
        # Load encoder maps
        if cuisine_token_to_id.exists():
            with open(cuisine_token_to_id, "r", encoding="utf-8") as f:
                token_to_id = json.load(f)
            logger.info(f"Loaded encoder with {len(token_to_id):,} tokens")
        else:
            logger.warning(f"Encoder map not found at {cuisine_token_to_id}")
            token_to_id = {}
        
        # Process each row group and apply dedupe + encoding
        output_path = combined_dataset_path.parent / f"{combined_dataset_path.stem}_with_cuisine_encoded.parquet"
        writer = None
        
        for rg_idx in range(pf_combined.num_row_groups):
            logger.info(f"Processing row group {rg_idx + 1}/{pf_combined.num_row_groups}...")
            df_rg = pf_combined.read_row_group(rg_idx).to_pandas()
            
            # Apply dedupe map to cuisine column
            if cuisine_col in df_rg.columns:
                # Convert cuisine to list format (handle both string and list inputs)
                def normalize_cuisine_to_list(x):
                    """Convert cuisine value to a list of strings, handling various input formats."""
                    if pd.isna(x):
                        return []
                    # If already a list/array, extract strings directly
                    if isinstance(x, (list, tuple, np.ndarray)):
                        return [str(t).strip() for t in x if str(t).strip()]
                    # If it's a string, parse it (could be "[American]" or "American" or "[American, Italian]")
                    s = str(x).strip()
                    if not s or s.lower() in ["nan", "none", "[]", ""]:
                        return []
                    # Use split_cuisine_entries to handle all string formats
                    return split_cuisine_entries(s)
                
                cuisine_lists = df_rg[cuisine_col].apply(normalize_cuisine_to_list)
                
                # Normalize and apply dedupe map (lowercase for lookup)
                cuisine_deduped = cuisine_lists.apply(
                    lambda lst: [
                        dedupe_map.get(str(tok).lower().strip(), str(tok).lower().strip())
                        for tok in lst
                        if str(tok).strip()  # Skip empty strings
                    ]
                )
                df_rg[f"{cuisine_col}_deduped"] = cuisine_deduped
                
                # Encode to IDs
                cuisine_encoded = cuisine_deduped.apply(
                    lambda lst: [
                        token_to_id.get(tok.lower().strip(), 0)
                        for tok in lst
                    ]
                )
                df_rg[f"{cuisine_col}_encoded"] = cuisine_encoded
                
                logger.info(f"  Applied dedupe and encoding to {len(df_rg):,} rows")
                logger.info(f"  Sample: {cuisine_col} -> {cuisine_col}_deduped -> {cuisine_col}_encoded")
            else:
                logger.warning(f"  Column '{cuisine_col}' not found in row group {rg_idx + 1}")
                logger.info(f"  Available columns: {list(df_rg.columns)}")
            
            # Write to output parquet
            table = pa.Table.from_pandas(df_rg, preserve_index=False)
            if writer is None:
                writer = pq.ParquetWriter(str(output_path), table.schema, compression="zstd")
            writer.write_table(table)
        
        if writer:
            writer.close()
        
        logger.info(f"Saved updated dataset → {output_path}")
        
        # Debug: Show head and columns
        logger.info("=" * 60)
        logger.info("Debug: Inspecting updated dataset")
        logger.info("=" * 60)
        df_debug = pq.read_table(output_path).to_pandas()
        logger.info(f"Dataset shape: {df_debug.shape}")
        logger.info(f"Columns: {list(df_debug.columns)}")
        logger.info("\nFirst 10 rows (showing key columns):")
        # Show relevant columns
        cols_to_show = [cuisine_col]
        if f"{cuisine_col}_deduped" in df_debug.columns:
            cols_to_show.append(f"{cuisine_col}_deduped")
        if f"{cuisine_col}_encoded" in df_debug.columns:
            cols_to_show.append(f"{cuisine_col}_encoded")
        # Also show a few other columns if they exist
        for col in ["Dataset_ID", "index", "ingredients"]:
            if col in df_debug.columns and col not in cols_to_show:
                cols_to_show.append(col)
        
        logger.info(f"\n{df_debug[cols_to_show].head(10).to_string()}")
        
        # Show detailed sample of cuisine transformations
        if f"{cuisine_col}_deduped" in df_debug.columns:
            logger.info(f"\nDetailed sample of cuisine transformations (first 5 rows):")
            for idx in range(min(5, len(df_debug))):
                orig = df_debug[cuisine_col].iloc[idx]
                deduped = df_debug[f"{cuisine_col}_deduped"].iloc[idx]
                encoded = df_debug[f"{cuisine_col}_encoded"].iloc[idx]
                logger.info(f"  Row {idx}:")
                logger.info(f"    Original: {orig}")
                logger.info(f"    Deduped:  {deduped}")
                logger.info(f"    Encoded:  {encoded}")
    else:
        logger.warning(f"Combined dataset not found at {combined_dataset_path} - skipping Step 6")

    # Final cleanup: Remove intermediate parquet files, keep JSON files and main dataset
    logger.info("=" * 60)
    logger.info("Final cleanup: Removing intermediate parquet files")
    logger.info("=" * 60)
    preserve_files = [
        str(combined_dataset_path),
        str(output_path) if 'output_path' in locals() and output_path.exists() else None,
    ]
    preserve_files = [f for f in preserve_files if f]  # Remove None values
    
    # Also clean up temporary prepared file if it exists
    prepared_parquet = baseline_parquet.parent / "_cuisine_prepared.parquet"
    if prepared_parquet.exists():
        try:
            prepared_parquet.unlink()
            logger.info(f"[cleanup] Deleted temporary file: {prepared_parquet}")
        except Exception as e:
            logger.warning(f"[cleanup] Failed to delete {prepared_parquet}: {e}")
    
    _cleanup_paths(cfg, logger, preserve_files=preserve_files)
    
    logger.info("=" * 60)
    logger.info(f"Output directory: {output_dir}")
    logger.info(f"  - Dedupe map (JSONL): {dedupe_map_path}")
    logger.info(f"  - Token→ID map (JSON): {cuisine_token_to_id}")
    logger.info(f"  - ID→Token map (JSON): {cuisine_id_to_token}")
    if 'output_path' in locals() and output_path.exists():
        logger.info(f"  - Final dataset: {output_path}")


if __name__ == "__main__":
    main()

================================================================================
FILE: preprocess_pipeline\scripts\6_encode_ingredients.py
================================================================================
"""
Script to encode already-inferred ingredients to IDs.

This script takes a dataset with inferred_ingredients (normalized/canonical forms)
and encodes them to ingredient IDs using the token-to-ID mapping.
This is separate from inference to allow faster iteration when only encoding changes.
"""

import pyarrow.parquet as pq
import argparse
import json
import logging
from pathlib import Path
from typing import List, Optional

import pandas as pd
import pyarrow.parquet as pq

import sys as _sys
# Make `preprocess_pipeline/` importable when running from repo root
_SYS_PATH_ROOT = Path.cwd() / "preprocess_pipeline"
if str(_SYS_PATH_ROOT) not in _sys.path:
    _sys.path.append(str(_SYS_PATH_ROOT))

from common.logging_setup import setup_logging


def load_encoder_maps(
    id2tok_path: Optional[Path],
    tok2id_path: Optional[Path],
) -> tuple[Optional[dict], Optional[dict]]:
    """Load IngredientEncoder id2tok / tok2id maps."""
    if not id2tok_path or not tok2id_path:
        return None, None
    if (not Path(id2tok_path).exists()) or (not Path(tok2id_path).exists()):
        return None, None
    with open(id2tok_path, "r", encoding="utf-8") as f:
        id2tok_raw = json.load(f)
    with open(tok2id_path, "r", encoding="utf-8") as f:
        tok2id_raw = json.load(f)
    id2tok = {int(k): str(v) for k, v in id2tok_raw.items()}
    tok2id = {str(k): int(v) for k, v in tok2id_raw.items()}
    return id2tok, tok2id


def encode_ingredient_list(ingredient_list, tok2id: dict) -> List[int]:
    """
    Encode a list of normalized ingredients to IDs.
    Handles various input formats: list, numpy array, tuple, string representation of list, None, etc.
    """
    import numpy as np
    
    # Handle None or NaN
    if ingredient_list is None or (isinstance(ingredient_list, float) and pd.isna(ingredient_list)):
        return []
    
    # Convert numpy array to list
    if isinstance(ingredient_list, np.ndarray):
        ingredient_list = ingredient_list.tolist()
    
    # If it's a string, try to parse it as a list
    if isinstance(ingredient_list, str):
        s = ingredient_list.strip()
        if not s or s.lower() in ["none", "nan", "null", "[]"]:
            return []
        # Try to parse as JSON or Python list
        if (s.startswith("[") and s.endswith("]")) or (s.startswith("(") and s.endswith(")")):
            try:
                import ast
                ingredient_list = ast.literal_eval(s)
            except:
                try:
                    ingredient_list = json.loads(s)
                except:
                    # If parsing fails, treat as single string
                    ingredient_list = [s]
        else:
            # Single string, not a list
            ingredient_list = [s]
    
    # Now ingredient_list should be a list-like object
    if not isinstance(ingredient_list, (list, tuple, np.ndarray)):
        return []
    
    encoded = []
    for ing in ingredient_list:
        if ing is None:
            continue
        ing_str = str(ing).strip()
        if not ing_str:
            continue
        # Try exact match first
        ing_id = tok2id.get(ing_str, None)
        if ing_id is None:
            # Try with different whitespace normalization
            ing_normalized = " ".join(ing_str.split())
            ing_id = tok2id.get(ing_normalized, 0)
        encoded.append(ing_id)
    
    return encoded if encoded else []


def main():
    parser = argparse.ArgumentParser(
        description="Encode inferred ingredients to IDs"
    )
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="Input parquet file with inferred_ingredients column",
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="Output parquet file (will add/update encoded_ingredients column)",
    )
    parser.add_argument(
        "--ingredient-token-to-id",
        type=str,
        default="./data/encoded/ingredient_token_to_id.json",
        help="Path to token-to-ID mapping JSON (default: ./data/encoded/ingredient_token_to_id.json)",
    )
    parser.add_argument(
        "--ingredient-id-to-token",
        type=str,
        default="./data/encoded/ingredient_id_to_token.json",
        help="Path to ID-to-token mapping JSON (default: ./data/encoded/ingredient_id_to_token.json)",
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Optional config file for logging",
    )
    args = parser.parse_args()
    
    # Setup logging
    logger = setup_logging(args.config) if args.config else logging.getLogger(__name__)
    if not logger.handlers:
        logging.basicConfig(
            level=logging.INFO,
            format="[%(asctime)s] %(levelname)s %(name)s: %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        logger = logging.getLogger(__name__)
    
    # Load encoder maps
    tok2id_path = Path(args.ingredient_token_to_id)
    id2tok_path = Path(args.ingredient_id_to_token)
    
    if not tok2id_path.exists():
        logger.error(f"Token-to-ID map not found: {tok2id_path}")
        return 1
    
    logger.info(f"Loading encoder maps...")
    id2tok, tok2id = load_encoder_maps(id2tok_path, tok2id_path)
    
    if not tok2id:
        logger.error("Failed to load token-to-ID mapping")
        return 1
    
    logger.info(f"Loaded token→ID map: {len(tok2id):,} tokens")
    
    # Load input data
    input_path = Path(args.input)
    if not input_path.exists():
        logger.error(f"Input file not found: {input_path}")
        return 1
    
    logger.info(f"Loading input data from {input_path}...")
    
    # Read parquet file
    pf = pq.ParquetFile(str(input_path))
    logger.info(f"Input file has {pf.num_row_groups} row groups")
    
    # Check if inferred_ingredients column exists
    schema = pf.schema_arrow
    col_names = [field.name for field in schema]
    
    if "inferred_ingredients" not in col_names:
        logger.error("Column 'inferred_ingredients' not found in input file")
        logger.error(f"Available columns: {col_names}")
        return 1
    
    # Process row groups and encode
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    logger.info("Encoding ingredients...")
    
    # Read all data (or process in chunks if too large)
    # Use pyarrow to preserve list types correctly
    pf = pq.ParquetFile(str(input_path))
    
    # Read in chunks to handle large files
    chunks = []
    for rg in range(pf.num_row_groups):
        df_chunk = pf.read_row_group(rg).to_pandas()
        chunks.append(df_chunk)
    df = pd.concat(chunks, ignore_index=True)
    logger.info(f"Loaded {len(df):,} rows")
    
    # Debug: Check data types and sample values
    logger.info("Debugging data format...")
    logger.info(f"  Column dtypes: {df.dtypes.to_dict()}")
    
    # Check sample of inferred_ingredients
    import numpy as np
    sample_ing = df["inferred_ingredients"].iloc[0] if len(df) > 0 else None
    logger.info(f"  Sample inferred_ingredients (row 0):")
    logger.info(f"    Type: {type(sample_ing)}")
    logger.info(f"    Value: {sample_ing}")
    
    # Convert to list if numpy array
    if isinstance(sample_ing, np.ndarray):
        sample_ing_list = sample_ing.tolist()
    elif isinstance(sample_ing, (list, tuple)):
        sample_ing_list = list(sample_ing)
    else:
        sample_ing_list = []
    
    if len(sample_ing_list) > 0:
        logger.info(f"    First item: '{sample_ing_list[0]}' (type: {type(sample_ing_list[0])})")
        # Check if it's in tok2id
        first_key = str(sample_ing_list[0]).strip()
        in_vocab = first_key in tok2id
        logger.info(f"    In tok2id: {in_vocab}")
        if in_vocab:
            logger.info(f"    ID: {tok2id[first_key]}")
        else:
            # Show some sample keys from tok2id
            sample_keys = list(tok2id.keys())[:10]
            logger.info(f"    Sample tok2id keys: {sample_keys}")
            # Try to find similar keys
            first_lower = first_key.lower()
            similar = [k for k in tok2id.keys() if first_lower in k.lower() or k.lower() in first_lower][:5]
            if similar:
                logger.info(f"    Similar keys in vocab: {similar}")
    elif sample_ing is not None:
        logger.warning(f"    Sample is not a list/array! Type: {type(sample_ing)}, Value: {sample_ing}")
    
    # Encode ingredients
    logger.info("Encoding inferred_ingredients to IDs...")
    encoded_lists = []
    zero_count = 0
    total_ingredients = 0
    sample_issues = []  # Track sample encoding issues for debugging
    
    for idx, ing_list in enumerate(df["inferred_ingredients"]):
        if idx % 10000 == 0 and idx > 0:
            logger.info(f"Processed {idx:,} / {len(df):,} rows...")
        
        # Debug first few entries
        if idx < 5:
            logger.debug(f"Row {idx}: type={type(ing_list)}, value={ing_list}")
            if isinstance(ing_list, (list, tuple)) and len(ing_list) > 0:
                logger.debug(f"  First ingredient: '{ing_list[0]}' (type={type(ing_list[0])})")
                # Check if first ingredient is in tok2id
                first_ing = str(ing_list[0]).strip()
                if first_ing in tok2id:
                    logger.debug(f"  ✓ Found in tok2id: {tok2id[first_ing]}")
                else:
                    logger.debug(f"  ✗ Not found in tok2id. Sample keys: {list(tok2id.keys())[:5]}")
        
        encoded = encode_ingredient_list(ing_list, tok2id)
        encoded_lists.append(encoded)
        
        # Count zeros for diagnostics
        zeros = sum(1 for x in encoded if x == 0)
        zero_count += zeros
        total_ingredients += len(encoded)
        
        # Track issues for first few rows
        if idx < 5 and len(encoded) == 0 and ing_list is not None:
            if isinstance(ing_list, (list, tuple)) and len(ing_list) > 0:
                sample_issues.append((idx, ing_list[:3], encoded))
    
    # Add encoded column
    df["encoded_ingredients"] = encoded_lists
    
    # Log statistics
    logger.info(f"Encoding complete:")
    logger.info(f"  Total rows: {len(df):,}")
    logger.info(f"  Total ingredients: {total_ingredients:,}")
    if total_ingredients > 0:
        logger.info(f"  Ingredients with ID=0 (unknown): {zero_count:,} ({zero_count/total_ingredients*100:.1f}%)")
    else:
        logger.warning("  No ingredients were encoded! This suggests a data format issue.")
        logger.warning("  Checking sample data...")
        
        # Show sample of what we're trying to encode
        sample_df = df[["inferred_ingredients"]].head(10)
        for idx, row in sample_df.iterrows():
            ing_list = row["inferred_ingredients"]
            logger.warning(f"  Row {idx}: type={type(ing_list)}, value={ing_list}")
            if isinstance(ing_list, (list, tuple)) and len(ing_list) > 0:
                first = str(ing_list[0]).strip()
                logger.warning(f"    First item: '{first}' (in tok2id: {first in tok2id})")
                # Show similar keys
                if first not in tok2id:
                    # Find similar keys
                    similar = [k for k in tok2id.keys() if first.lower() in k.lower() or k.lower() in first.lower()][:3]
                    if similar:
                        logger.warning(f"    Similar keys in tok2id: {similar}")
    
    logger.info(f"  First few encoded ingredients:")
    logger.info(f"\n{df[['inferred_ingredients', 'encoded_ingredients']].head()}")
    logger.info(f"Saving to {output_path}...")
    df.to_parquet(output_path, index=False, compression="zstd")
    logger.info(f"Saved encoded dataset to {output_path}")
    
    logger.info("=" * 60)
    logger.info("Encoding complete")
    logger.info("=" * 60)
    
    return 0


if __name__ == "__main__":
    exit(main())

================================================================================
FILE: preprocess_pipeline\scripts\7_train_cuisine_classifier.py
================================================================================
# preprocess_pipeline/scripts/train_cuisine_classifier.py
from __future__ import annotations

from pathlib import Path
import argparse
import sys as _sys

# Make `preprocess_pipeline/` importable when running from repo root
_SYS_PATH_ROOT = Path.cwd() / "preprocess_pipeline"
if str(_SYS_PATH_ROOT) not in _sys.path:
    _sys.path.append(str(_SYS_PATH_ROOT))

import yaml

from common.logging_setup import setup_logging
from cuisine_classifier.config import load_configs_from_dict, DATA, TRAIN, OUT
from cuisine_classifier.utils import set_global_seed
from cuisine_classifier.data_prep import prepare_docbins_from_config
from cuisine_classifier.training import train_classifier_from_docbins


def main() -> None:
    ap = argparse.ArgumentParser(description="Train cuisine classification model")
    ap.add_argument(
        "--config",
        type=str,
        default="preprocess_pipeline/config/cuisine_classifier.yaml",
        help="Path to cuisine classifier config YAML",
    )
    args = ap.parse_args()

    config_path = Path(args.config)
    if not config_path.exists():
        raise FileNotFoundError(f"Config not found: {config_path}")

    # Load full pipeline config once
    with open(config_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if not isinstance(cfg, dict):
        raise ValueError(f"YAML at {config_path} did not parse to a dict.")

    # Logging first
    setup_logging(cfg)

    # Populate DATA / TRAIN / OUT globals for cuisine_classifier modules
    load_configs_from_dict(cfg)

    # Seed + run training pipeline
    set_global_seed(TRAIN.RANDOM_SEED)
    prepare_docbins_from_config()
    train_classifier_from_docbins()
    
    # Cleanup: Remove intermediate docbins after training (keep only the final model)
    import logging
    import shutil
    logger = logging.getLogger(__name__)
    
    cleanup_docbins = cfg.get("cuisine_classifier", {}).get("cleanup_docbins", True)
    if cleanup_docbins:
        logger.info("=" * 60)
        logger.info("Cleaning up intermediate training artifacts (docbins)")
        logger.info("=" * 60)
        
        docbin_dirs = [OUT.TRAIN_DIR, OUT.VALID_DIR]
        for docbin_dir in docbin_dirs:
            if docbin_dir.exists():
                try:
                    # Count files before deletion
                    spacy_files = list(docbin_dir.glob("*.spacy"))
                    if spacy_files:
                        logger.info(f"Removing {len(spacy_files)} docbin shard(s) from {docbin_dir}")
                        shutil.rmtree(docbin_dir)
                        logger.info(f"Deleted {docbin_dir}")
                    else:
                        logger.debug(f"No docbin files found in {docbin_dir}")
                except Exception as e:
                    logger.warning(f"Failed to delete {docbin_dir}: {e}")
        
        logger.info("Training complete. Final model saved at:")
        logger.info(f"  - {OUT.MODEL_DIR}")
        logger.info("Intermediate docbins have been cleaned up.")
    else:
        logger.info("Training complete. Final model saved at:")
        logger.info(f"  - {OUT.MODEL_DIR}")
        logger.info("Docbins preserved (cleanup_docbins=false in config)")


if __name__ == "__main__":
    main()



================================================================================
FILE: preprocess_pipeline\scripts\__init__.py
================================================================================
