{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e182a47",
   "metadata": {},
   "source": [
    "# Ingredient NER — Training, Normalization & Encoding\n",
    "\n",
    "This notebook trains a spaCy Named Entity Recognition (NER) model to identify **ingredients** in free text. \n",
    "It also supports **lexicon-driven bootstrapping** (optional) to auto-label raw text, and includes **normalization** + **encoding** to integer IDs using your pipeline artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91cd79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, install dependencies (uncomment and run in your environment)\n",
    "# !pip install -U spacy pyarrow pandas scikit-learn tqdm\n",
    "# Optional for transformer-based NER:\n",
    "# !pip install -U spacy-transformers torch\n",
    "# Download a small English model if you don't have one:\n",
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10a482",
   "metadata": {},
   "source": [
    "## 0) Setup & optional installs\n",
    "\n",
    "- Keep the `pip` cells commented in your managed environment (e.g., when dependencies are preinstalled).\n",
    "- If you’re missing `spacy-transformers` or `torch`, uncomment and run the `pip` cells.\n",
    "- CUDA is optional; if available, we’ll use it automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed972686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA available: 12.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Optional installs (uncomment as needed)\n",
    "# %pip install -U spacy spacy-transformers torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install -U pandas pyarrow scikit-learn tqdm\n",
    "\n",
    "import os, sys, json, ast, math, random, warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    _HAS_PA = True\n",
    "except Exception:\n",
    "    _HAS_PA = False\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.tokens import Doc, DocBin\n",
    "from spacy.training import Example\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Torch is optional (only for transformers / GPU); we gracefully degrade otherwise.\n",
    "try:\n",
    "    import torch\n",
    "except Exception as _e:\n",
    "    torch = None\n",
    "    warnings.warn(\"Torch not available. Training will fall back to CPU tok2vec if transformers cannot be used.\")\n",
    "\n",
    "def set_global_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch is not None:\n",
    "        try:\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Prefer GPU for spaCy-backed Torch components if present\n",
    "if torch is not None:\n",
    "    if torch.cuda.is_available():\n",
    "        os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "        spacy.prefer_gpu()\n",
    "        if hasattr(torch.backends, \"cudnn\"):\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        print(\"✅ CUDA available:\", torch.version.cuda if hasattr(torch, \"version\") else \"unknown\")\n",
    "    else:\n",
    "        spacy.prefer_cpu()\n",
    "        print(\"ℹ️ CUDA not available. Using CPU.\")\n",
    "else:\n",
    "    spacy.prefer_cpu()\n",
    "    print(\"ℹ️ Torch not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfb2fb",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configuration\n",
    "\n",
    "Adjust the paths below to point to your training corpus and pipeline artifacts.  \n",
    "You can train from **CSV or Parquet**. The notebook expects:\n",
    "- A text column (e.g., `raw_text`, `ingredients_text`, etc.) that contains free text.\n",
    "- Optionally, a **lexicon** JSON (list of canonical ingredient phrases) to bootstrap labels.\n",
    "- Optionally, a **dedupe mapping** JSONL (phrase → canonical) generated by your W2V or cosine dedupe stage.\n",
    "- Optionally, `IngredientEncoder` maps to encode tokens into IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941a9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data config: {'DATA_IS_PARQUET': False, 'TRAIN_PATH': WindowsPath('../../data/raw/wilmerarltstrmberg_data.csv'), 'NER_LIST_COL': 'NER', 'TEXT_COL': None, 'LEXICON_JSON': None, 'DEDUPE_JSONL': WindowsPath('../../data/normalized/cosine_dedupe_map.jsonl'), 'ING_ID2TOK_JSON': WindowsPath('../../data/encoded/ingredient_id_to_token.json'), 'ING_TOK2ID_JSON': WindowsPath('../../data/encoded/ingredient_token_to_id.json')}\n",
      "Train config: {'RANDOM_SEED': 42, 'VALID_FRACTION': 0.1, 'SHARD_SIZE': 2000, 'EVAL_SNAPSHOT_MAX': 1500, 'TRANSFORMER_MODEL': 'distilbert-base-uncased', 'WINDOW': 64, 'STRIDE': 48, 'LR': 5e-05, 'DROPOUT': 0.1, 'N_EPOCHS': 10, 'FREEZE_LAYERS': 2, 'USE_AMP': True, 'CLEAR_CACHE_EVERY': 200, 'EARLY_STOPPING_PATIENCE': 3}\n",
      "Out config: {'OUT_DIR': WindowsPath('../../models/ingredient_ner_trf'), 'MODEL_DIR': WindowsPath('../../models/ingredient_ner_trf/model-best'), 'BOOT_DIR': WindowsPath('../../models/ingredient_ner_trf/bootstrapped'), 'TRAIN_DIR': WindowsPath('../../models/ingredient_ner_trf/bootstrapped/train'), 'VALID_DIR': WindowsPath('../../models/ingredient_ner_trf/bootstrapped/valid'), 'PRED_OUT': WindowsPath('../../data/training/predictions.parquet')}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class DataCfg:\n",
    "    # Training data\n",
    "    DATA_IS_PARQUET: bool = False\n",
    "    TRAIN_PATH: Path = Path(\"../../data/raw/wilmerarltstrmberg_data.csv\")\n",
    "\n",
    "    # If you already have a list-like NER column (e.g., [\"salt\", \"sugar\", ...]), set this:\n",
    "    NER_LIST_COL: str = \"NER\"      # column containing list-like ingredient strings\n",
    "\n",
    "    # If training from raw text + lexicon (bootstrapping), set TEXT_COL and LEXICON_JSON:\n",
    "    TEXT_COL: str | None = None    # e.g., \"ingredients_text\" (free text). Leave None if not using.\n",
    "    LEXICON_JSON: Path | None = None  # e.g., Path(\"../../data/lexicon/ingredients.json\")\n",
    "\n",
    "    # Optional dedupe map (JSONL lines: {\"from\": \"...\", \"to\": \"...\"})\n",
    "    DEDUPE_JSONL: Path | None = Path(\"../../data/normalized/cosine_dedupe_map.jsonl\")\n",
    "\n",
    "    # IngredientEncoder maps (Stage 6 artifacts)\n",
    "    ING_ID2TOK_JSON: Path | None = Path(\"../../data/encoded/ingredient_id_to_token.json\")\n",
    "    ING_TOK2ID_JSON: Path | None = Path(\"../../data/encoded/ingredient_token_to_id.json\")\n",
    "\n",
    "@dataclass\n",
    "class TrainCfg:\n",
    "    RANDOM_SEED: int = 42\n",
    "    VALID_FRACTION: float = 0.1\n",
    "    SHARD_SIZE: int = 2000\n",
    "    EVAL_SNAPSHOT_MAX: int = 1500\n",
    "    BATCH_SIZE: int = 256\n",
    "    # Transformer options (fallback to tok2vec if not available)\n",
    "    TRANSFORMER_MODEL: str = \"distilbert-base-uncased\"\n",
    "    WINDOW: int = 64\n",
    "    STRIDE: int = 48\n",
    "    LR: float = 5e-5\n",
    "    DROPOUT: float = 0.1\n",
    "    N_EPOCHS: int = 10\n",
    "    FREEZE_LAYERS: int = 2\n",
    "    USE_AMP: bool = True\n",
    "    CLEAR_CACHE_EVERY: int = 200\n",
    "    EARLY_STOPPING_PATIENCE: int = 3   # epochs without F1 improvement\n",
    "\n",
    "@dataclass\n",
    "class OutCfg:\n",
    "    OUT_DIR: Path = Path(\"../../models/ingredient_ner_trf\")\n",
    "    MODEL_DIR: Path = Path(\"../../models/ingredient_ner_trf/model-best\")\n",
    "    BOOT_DIR: Path  = Path(\"../../models/ingredient_ner_trf/bootstrapped\")\n",
    "    TRAIN_DIR: Path = Path(\"../../models/ingredient_ner_trf/bootstrapped/train\")\n",
    "    VALID_DIR: Path = Path(\"../../models/ingredient_ner_trf/bootstrapped/valid\")\n",
    "    PRED_OUT: Path  = Path(\"../../data/training/predictions.parquet\")\n",
    "\n",
    "DATA = DataCfg()\n",
    "TRAIN = TrainCfg()\n",
    "OUT = OutCfg()\n",
    "\n",
    "# Create output directories\n",
    "for p in [OUT.OUT_DIR, OUT.TRAIN_DIR, OUT.VALID_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Data config:\", asdict(DATA))\n",
    "print(\"Train config:\", asdict(TRAIN))\n",
    "print(\"Out config:\", asdict(OUT))\n",
    "\n",
    "set_global_seed(TRAIN.RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd6997c",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Utilities\n",
    "\n",
    "Helpers for data loading, list-like parsing, and string normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8ecd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path: Path, is_parquet: bool, col: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a single column from CSV/Parquet and return a clean DataFrame.\"\"\"\n",
    "    if is_parquet:\n",
    "        if not _HAS_PA:\n",
    "            raise RuntimeError(\"pyarrow is required to read Parquet files. Please install pyarrow.\")\n",
    "        pf = pq.ParquetFile(str(path))\n",
    "        frames = [pf.read_row_group(i).to_pandas() for i in range(pf.num_row_groups)]\n",
    "        df = pd.concat(frames, ignore_index=True)\n",
    "    else:\n",
    "        df = pd.read_csv(path, dtype=str)\n",
    "    if col not in df.columns:\n",
    "        raise KeyError(f\"Column '{col}' not found in {list(df.columns)[:20]}...\")\n",
    "    return df[[col]].dropna().reset_index(drop=True)\n",
    "\n",
    "def parse_listlike(v):\n",
    "    if isinstance(v, (list, tuple)): \n",
    "        return [str(x).strip() for x in v if str(x).strip()]\n",
    "    s = str(v).strip()\n",
    "    if not s: \n",
    "        return []\n",
    "    for parser in (ast.literal_eval, json.loads):\n",
    "        try:\n",
    "            out = parser(s)\n",
    "            if isinstance(out, (list, tuple)):\n",
    "                return [str(x).strip() for x in out if str(x).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return [x.strip() for x in s.split(\",\") if x.strip()]\n",
    "\n",
    "def join_with_offsets(tokens, sep: str = \", \"):\n",
    "    text, spans, pos = [], [], 0\n",
    "    for i, tok in enumerate(tokens):\n",
    "        start, end = pos, pos + len(tok)\n",
    "        text.append(tok); spans.append((start, end)); pos = end\n",
    "        if i < len(tokens)-1: text.append(sep); pos += len(sep)\n",
    "    return \"\".join(text), spans\n",
    "\n",
    "def normalize_token(s: str) -> str:\n",
    "    return \" \".join(str(s).strip().lower().split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa38c7",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Create training docs\n",
    "\n",
    "Two options:\n",
    "\n",
    "1. **From list-like NER column** (`DATA.NER_LIST_COL`): Each row is a list of ingredient strings.  \n",
    "   We synthesize a text like `'salt, sugar, ...'` and label each ingredient span as `INGREDIENT`.\n",
    "2. **From raw text + lexicon** (`DATA.TEXT_COL` & `DATA.LEXICON_JSON`): Load a lexicon and label spans using `EntityRuler`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b8d7104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Synthesizing from list column: 100%|██████████| 2231142/2231142 [02:21<00:00, 15723.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs prepared: 2,231,142 | Source mode: list-column\n",
      "Total labeled entities: 18419930\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def docs_from_list_column(df: pd.DataFrame, col: str) -> list[Doc]:\n",
    "    blank = spacy.blank(\"en\")\n",
    "    out = []\n",
    "    for lst in tqdm(df[col].tolist(), desc=\"Synthesizing from list column\"):\n",
    "        toks = parse_listlike(lst)\n",
    "        if not toks:\n",
    "            out.append(blank.make_doc(\"\"))\n",
    "            continue\n",
    "        text, offs = join_with_offsets(toks)\n",
    "        d = blank.make_doc(text)\n",
    "        ents = []\n",
    "        for (a, b) in offs:\n",
    "            sp = d.char_span(a, b, label=\"INGREDIENT\", alignment_mode=\"contract\")\n",
    "            if sp is not None: ents.append(sp)\n",
    "        d.ents = spacy.util.filter_spans(ents)\n",
    "        out.append(d)\n",
    "    return out\n",
    "\n",
    "def load_lexicon(path: Path | None) -> list[str]:\n",
    "    if path is None:\n",
    "        return []\n",
    "    p = Path(path)\n",
    "    if not p.exists(): \n",
    "        warnings.warn(f\"Lexicon not found at {p}. Skipping.\")\n",
    "        return []\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    # Expect either {\"terms\": [...]} or a simple list [...]\n",
    "    if isinstance(data, dict) and \"terms\" in data:\n",
    "        terms = data[\"terms\"]\n",
    "    else:\n",
    "        terms = data\n",
    "    # normalize\n",
    "    terms = [normalize_token(t) for t in terms if str(t).strip()]\n",
    "    terms = sorted(set(terms))\n",
    "    print(f\"Loaded {len(terms):,} lexicon terms.\")\n",
    "    return terms\n",
    "\n",
    "def build_entity_ruler(nlp: spacy.language.Language, phrases: list[str]):\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    patterns = [{\"label\": \"INGREDIENT\", \"pattern\": t} for t in phrases]\n",
    "    ruler.add_patterns(patterns)\n",
    "    return ruler\n",
    "\n",
    "def docs_from_text_plus_lexicon(df: pd.DataFrame, text_col: str, lexicon_terms: list[str]) -> list[Doc]:\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    if not lexicon_terms:\n",
    "        raise ValueError(\"No lexicon terms provided; cannot bootstrap from raw text.\")\n",
    "    build_entity_ruler(nlp, lexicon_terms)\n",
    "    out = []\n",
    "    for text in tqdm(df[text_col].astype(str).tolist(), desc=\"Bootstrapping with EntityRuler\"):\n",
    "        d = nlp.make_doc(text)\n",
    "        d = nlp(d)  # apply ruler\n",
    "        # Keep only INGREDIENT, deduplicate spans\n",
    "        d.ents = spacy.util.filter_spans([e for e in d.ents if e.label_ == \"INGREDIENT\"])\n",
    "        out.append(d)\n",
    "    return out\n",
    "\n",
    "# ---- Build docs according to available columns ----\n",
    "docs_all = []\n",
    "if DATA.NER_LIST_COL and DATA.NER_LIST_COL in pd.read_csv(DATA.TRAIN_PATH, nrows=1).columns:\n",
    "    df_list = load_data(DATA.TRAIN_PATH, DATA.DATA_IS_PARQUET, DATA.NER_LIST_COL)\n",
    "    docs_all = docs_from_list_column(df_list, DATA.NER_LIST_COL)\n",
    "    source_mode = \"list-column\"\n",
    "elif DATA.TEXT_COL and DATA.LEXICON_JSON:\n",
    "    # use bootstrapping from raw text + lexicon\n",
    "    df_text = load_data(DATA.TRAIN_PATH, DATA.DATA_IS_PARQUET, DATA.TEXT_COL)\n",
    "    lex = load_lexicon(DATA.LEXICON_JSON)\n",
    "    docs_all = docs_from_text_plus_lexicon(df_text, DATA.TEXT_COL, lex)\n",
    "    source_mode = \"text+lexicon\"\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"No valid data source inferred. Set DATA.NER_LIST_COL (list-like labels) \"\n",
    "        \"or DATA.TEXT_COL + DATA.LEXICON_JSON (bootstrapping).\"\n",
    "    )\n",
    "\n",
    "print(f\"Docs prepared: {len(docs_all):,} | Source mode: {source_mode}\")\n",
    "print(\"Total labeled entities:\", sum(len(d.ents) for d in docs_all))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4cc5cc",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Train/validation split and DocBin sharding\n",
    "\n",
    "We write train/valid **DocBin** shards to disk to keep memory usage low and make resumes easy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "941b41fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train=2,008,027 | valid=223,115\n",
      "Wrote 2008027 docs to ..\\..\\models\\ingredient_ner_trf\\bootstrapped\\train in 1005 shard(s).\n",
      "Wrote 223115 docs to ..\\..\\models\\ingredient_ner_trf\\bootstrapped\\valid in 112 shard(s).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_docs, valid_docs = train_test_split(\n",
    "    docs_all, test_size=TRAIN.VALID_FRACTION, random_state=TRAIN.RANDOM_SEED\n",
    ")\n",
    "print(f\"train={len(train_docs):,} | valid={len(valid_docs):,}\")\n",
    "\n",
    "def write_docbins(docs: list[Doc], out_dir: Path, shard_size: int = TRAIN.SHARD_SIZE):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    n = len(docs)\n",
    "    shards = math.ceil(n / max(1, shard_size))\n",
    "    for i in range(shards):\n",
    "        db = DocBin(store_user_data=False)\n",
    "        for d in docs[i*shard_size : (i+1)*shard_size]:\n",
    "            db.add(d)\n",
    "        db.to_disk(out_dir / f\"shard_{i:04d}.spacy\")\n",
    "    print(f\"Wrote {n} docs to {out_dir} in {shards} shard(s).\")\n",
    "\n",
    "write_docbins(train_docs, OUT.TRAIN_DIR)\n",
    "write_docbins(valid_docs, OUT.VALID_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9a51a",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Train spaCy NER\n",
    "We'll train a small spaCy model from the bootstrapped annotations. For stronger performance, consider installing\n",
    "`spacy-transformers` and swapping to a transformer backbone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5f546",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TrainCfg' object has no attribute 'LEARNING_RATE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 44\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Losses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - P/R/F1: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ments_p\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ments_r\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ments_f\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nlp\n\u001b[0;32m     40\u001b[0m nlp_trained \u001b[38;5;241m=\u001b[39m train_spacy_ner(\n\u001b[0;32m     41\u001b[0m     train_docs\u001b[38;5;241m=\u001b[39mtrain_docs,\n\u001b[0;32m     42\u001b[0m     valid_docs\u001b[38;5;241m=\u001b[39mvalid_docs,\n\u001b[0;32m     43\u001b[0m     n_epochs\u001b[38;5;241m=\u001b[39mTRAIN\u001b[38;5;241m.\u001b[39mN_EPOCHS,\n\u001b[1;32m---> 44\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[43mTRAIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m,\n\u001b[0;32m     45\u001b[0m     dropout\u001b[38;5;241m=\u001b[39mTRAIN\u001b[38;5;241m.\u001b[39mDROPOUT,\n\u001b[0;32m     46\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mTRAIN\u001b[38;5;241m.\u001b[39mBATCH_SIZE\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Persist model\u001b[39;00m\n\u001b[0;32m     50\u001b[0m OUT\u001b[38;5;241m.\u001b[39mMODEL_DIR\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TrainCfg' object has no attribute 'LEARNING_RATE'"
     ]
    }
   ],
   "source": [
    "\n",
    "from spacy.language import Language\n",
    "import random\n",
    "from spacy.training import Example\n",
    "\n",
    "def train_spacy_ner(train_docs, valid_docs, n_epochs=10, lr=0.001, dropout=0.2, batch_size=128) -> Language:\n",
    "    # Initialize a blank English pipeline and add NER\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "    ner.add_label(\"INGREDIENT\")\n",
    "\n",
    "    # Convert docs to Examples (NER only)\n",
    "    train_examples = []\n",
    "    for d in train_docs:\n",
    "        # Keep only NER annotations (spans from EntityRuler bootstrapping)\n",
    "        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in d.ents]\n",
    "        train_examples.append(Example.from_dict(nlp.make_doc(d.text), {\"entities\": ents}))\n",
    "\n",
    "    valid_examples = []\n",
    "    for d in valid_docs:\n",
    "        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in d.ents]\n",
    "        valid_examples.append(Example.from_dict(nlp.make_doc(d.text), {\"entities\": ents}))\n",
    "\n",
    "    optimizer = nlp.initialize(lambda: train_examples)\n",
    "    print(\"Initialized\", nlp.pipe_names)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        random.shuffle(train_examples)\n",
    "        losses = {}\n",
    "        # Minibatch training\n",
    "        for i in range(0, len(train_examples), batch_size):\n",
    "            batch = train_examples[i:i+batch_size]\n",
    "            nlp.update(batch, sgd=optimizer, drop=dropout, losses=losses)\n",
    "        # Simple evaluation\n",
    "        with nlp.select_pipes(disable=[p for p in nlp.pipe_names if p != \"ner\"]):\n",
    "            scores = nlp.evaluate(valid_examples)\n",
    "        print(f\"Epoch {epoch+1:02d}/{n_epochs} - Losses: {losses} - P/R/F1: \"\n",
    "              f\"{scores['ents_p']:.3f}/{scores['ents_r']:.3f}/{scores['ents_f']:.3f}\")\n",
    "    return nlp\n",
    "\n",
    "nlp_trained = train_spacy_ner(\n",
    "    train_docs=train_docs,\n",
    "    valid_docs=valid_docs,\n",
    "    n_epochs=TRAIN.N_EPOCHS,\n",
    "    lr=TRAIN.LR,\n",
    "    dropout=TRAIN.DROPOUT,\n",
    "    batch_size=TRAIN.SHARD_SIZE\n",
    ")\n",
    "\n",
    "# Persist model\n",
    "OUT.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "nlp_trained.to_disk(OUT.MODEL_DIR)\n",
    "print(f\"Saved model → {OUT.MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a93eb",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Build NER pipeline (transformer or tok2vec fallback)\n",
    "\n",
    "We prefer **transformers** for better transfer learning. If `spacy-transformers` or `torch`/CUDA is missing, we fallback to a **tok2vec** model for CPU training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aed898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_nlp_transformer() -> spacy.language.Language:\n",
    "    \"\"\"Build a small-window transformer + NER with optional layer freezing.\"\"\"\n",
    "    try:\n",
    "        import spacy_transformers  # noqa: F401\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"spacy-transformers is not available.\") from e\n",
    "\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    trf_cfg = {\n",
    "        \"model\": {\n",
    "            \"@architectures\": \"spacy-transformers.TransformerModel.v3\",\n",
    "            \"name\": TRAIN.TRANSFORMER_MODEL,\n",
    "            \"tokenizer_config\": {\"use_fast\": True},\n",
    "            \"transformer_config\": {},\n",
    "            \"mixed_precision\": bool(TRAIN.USE_AMP),\n",
    "            \"grad_scaler_config\": {\"enabled\": bool(TRAIN.USE_AMP)},\n",
    "            \"get_spans\": {\n",
    "                \"@span_getters\": \"spacy-transformers.strided_spans.v1\",\n",
    "                \"window\": int(TRAIN.WINDOW),\n",
    "                \"stride\": int(TRAIN.STRIDE),\n",
    "            },\n",
    "        },\n",
    "        \"set_extra_annotations\": {\n",
    "            \"@annotation_setters\": \"spacy-transformers.null_annotation_setter.v1\"\n",
    "        },\n",
    "        \"max_batch_items\": 4096,\n",
    "    }\n",
    "    nlp.add_pipe(\"transformer\", config=trf_cfg)\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "    ner.add_label(\"INGREDIENT\")\n",
    "\n",
    "    # Optional layer freezing\n",
    "    if TRAIN.FREEZE_LAYERS > 0:\n",
    "        try:\n",
    "            trf = nlp.get_pipe(\"transformer\").model\n",
    "            hf = trf.transformer.model\n",
    "            blocks = None\n",
    "            if hasattr(hf, \"transformer\") and hasattr(hf.transformer, \"layer\"):  # distilbert\n",
    "                blocks = hf.transformer.layer\n",
    "            elif hasattr(hf, \"encoder\") and hasattr(hf.encoder, \"layer\"):        # bert/roberta\n",
    "                blocks = hf.encoder.layer\n",
    "            if blocks is not None:\n",
    "                k = min(TRAIN.FREEZE_LAYERS, len(blocks))\n",
    "                for i in range(k):\n",
    "                    for p in blocks[i].parameters():\n",
    "                        p.requires_grad = False\n",
    "                print(f\"[transformer] Froze {k} lower layer(s).\")\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Could not freeze layers: {e}\")\n",
    "    return nlp\n",
    "\n",
    "def build_nlp_tok2vec() -> spacy.language.Language:\n",
    "    \"\"\"CPU-friendly tok2vec + NER fallback.\"\"\"\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    nlp.add_pipe(\"tok2vec\")\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "    ner.add_label(\"INGREDIENT\")\n",
    "    print(\"Using tok2vec fallback (no transformers).\")\n",
    "    return nlp\n",
    "\n",
    "def choose_nlp():\n",
    "    if torch is not None:\n",
    "        has_trf = True\n",
    "        try:\n",
    "            import spacy_transformers  # noqa\n",
    "        except Exception:\n",
    "            has_trf = False\n",
    "        if has_trf:\n",
    "            try:\n",
    "                return build_nlp_transformer(), \"transformer\"\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Falling back to tok2vec due to: {e}\")\n",
    "                return build_nlp_tok2vec(), \"tok2vec\"\n",
    "    # No torch or transformers\n",
    "    return build_nlp_tok2vec(), \"tok2vec\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d62a0",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Training loop with early stopping\n",
    "\n",
    "We stream `Example`s from DocBin shards, update in micro-batches, evaluate on a snapshot of validation examples, and save **model-best** when F1 improves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53347215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iter_examples_from_docbins(nlp, dir_path: Path, shuffle: bool = False):\n",
    "    shard_paths = sorted(p for p in dir_path.glob(\"*.spacy\"))\n",
    "    if shuffle:\n",
    "        random.shuffle(shard_paths)\n",
    "    for sp_path in shard_paths:\n",
    "        db = DocBin().from_disk(sp_path)\n",
    "        for d in db.get_docs(nlp.vocab):\n",
    "            ents = [(e.start_char, e.end_char, e.label_) for e in d.ents]\n",
    "            yield Example.from_dict(nlp.make_doc(d.text), {\"entities\": ents})\n",
    "\n",
    "def sample_validation(nlp, dir_path: Path, cap: int = 1500):\n",
    "    out, n = [], 0\n",
    "    for eg in iter_examples_from_docbins(nlp, dir_path, shuffle=False):\n",
    "        out.append(eg); n += 1\n",
    "        if n >= cap:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def compounding_batch(epoch: int, total_epochs: int, start: int = 8, end: int = 16) -> int:\n",
    "    if total_epochs <= 1:\n",
    "        return end\n",
    "    r = epoch / (total_epochs - 1)\n",
    "    return max(1, int(round(start * ((end / start) ** r))))\n",
    "\n",
    "def train_ner(train_dir: Path, valid_dir: Path):\n",
    "    nlp, mode = choose_nlp()\n",
    "    print(\"Pipeline mode:\", mode)\n",
    "\n",
    "    # Warm init\n",
    "    warm = []\n",
    "    for i, eg in enumerate(iter_examples_from_docbins(nlp, train_dir, shuffle=True)):\n",
    "        warm.append(eg)\n",
    "        if i >= min(256,  max(16, 100)):  # small warm set\n",
    "            break\n",
    "    optimizer = nlp.initialize(lambda: warm)\n",
    "    if hasattr(optimizer, \"learn_rate\"):\n",
    "        optimizer.learn_rate = float(TRAIN.LR)\n",
    "\n",
    "    # Validation snapshot for fast eval\n",
    "    valid_snapshot = sample_validation(nlp, valid_dir, cap=TRAIN.EVAL_SNAPSHOT_MAX)\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in range(TRAIN.N_EPOCHS):\n",
    "        losses = {}\n",
    "        micro_bs = compounding_batch(epoch, TRAIN.N_EPOCHS, start=8, end=16)\n",
    "        buf = []\n",
    "        updates = 0\n",
    "\n",
    "        for eg in iter_examples_from_docbins(nlp, train_dir, shuffle=True):\n",
    "            buf.append(eg)\n",
    "            if len(buf) < micro_bs:\n",
    "                continue\n",
    "            nlp.update(buf, sgd=optimizer, drop=TRAIN.DROPOUT, losses=losses)\n",
    "            buf.clear()\n",
    "            updates += 1\n",
    "            if (torch is not None) and torch.cuda.is_available() and updates % TRAIN.CLEAR_CACHE_EVERY == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        if buf:\n",
    "            nlp.update(buf, sgd=optimizer, drop=TRAIN.DROPOUT, losses=losses)\n",
    "            buf.clear()\n",
    "\n",
    "        # quick eval\n",
    "        with nlp.select_pipes(disable=[p for p in nlp.pipe_names if p != \"ner\"]):\n",
    "            scores = nlp.evaluate(valid_snapshot)\n",
    "        p = float(scores.get(\"ents_p\") or 0.0)\n",
    "        r = float(scores.get(\"ents_r\") or 0.0)\n",
    "        f1 = float(scores.get(\"ents_f\") or 0.0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d}/{TRAIN.N_EPOCHS} | μbs={micro_bs:<3d} | loss={losses.get('ner', 0):.1f} | P/R/F1={p:.3f}/{r:.3f}/{f1:.3f}\")\n",
    "\n",
    "        # Early stopping & best model save\n",
    "        improved = f1 > best_f1 + 1e-6\n",
    "        if improved:\n",
    "            best_f1 = f1\n",
    "            bad_epochs = 0\n",
    "            OUT.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "            nlp.to_disk(OUT.MODEL_DIR)\n",
    "            print(f\"  ↳ Saved model-best → {OUT.MODEL_DIR} (F1={f1:.3f})\")\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= TRAIN.EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"Early stopping after {bad_epochs} non-improving epoch(s).\")\n",
    "                break\n",
    "\n",
    "    print(\"Best F1 observed:\", best_f1 if best_f1 >= 0 else 0.0)\n",
    "    return nlp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train now (may take time depending on data and model size)\n",
    "# nlp_trained = train_ner(OUT.TRAIN_DIR, OUT.VALID_DIR)\n",
    "# print(\"Model saved to:\", OUT.MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6721767",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Normalization, dedupe & encoding maps\n",
    "\n",
    "Utilities to (a) load the dedupe JSONL mapping, (b) load `IngredientEncoder` maps, and (c) apply predictions → normalized strings → integer IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb906df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, Union\n",
    "\n",
    "def load_jsonl_map(path: Union[str, Path]) -> Dict[str, str]:\n",
    "    mapping = {}\n",
    "    p = Path(path) if path is not None else None\n",
    "    if not p or not p.exists():\n",
    "        return mapping\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                obj = json.loads(line)\n",
    "                src = normalize_token(str(obj.get(\"from\", \"\")))\n",
    "                dst = normalize_token(str(obj.get(\"to\", \"\")))\n",
    "                if src and dst:\n",
    "                    mapping[src] = dst\n",
    "    return mapping\n",
    "\n",
    "def load_encoder_maps(id2tok_path: Path | None, tok2id_path: Path | None):\n",
    "    if not id2tok_path or not tok2id_path:\n",
    "        return None, None\n",
    "    if (not Path(id2tok_path).exists()) or (not Path(tok2id_path).exists()):\n",
    "        return None, None\n",
    "    with open(id2tok_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        id2tok_raw = json.load(f)\n",
    "    with open(tok2id_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tok2id_raw = json.load(f)\n",
    "    id2tok = {int(k): str(v) for k, v in id2tok_raw.items()}\n",
    "    tok2id = {str(k): int(v) for k, v in tok2id_raw.items()}\n",
    "    return id2tok, tok2id\n",
    "\n",
    "def apply_dedupe(tok: str, mapping: Dict[str, str]) -> str:\n",
    "    return mapping.get(tok, tok) if mapping else tok\n",
    "\n",
    "dedupe_map = load_jsonl_map(DATA.DEDUPE_JSONL) if DATA.DEDUPE_JSONL else {}\n",
    "ing_id2tok, ing_tok2id = load_encoder_maps(DATA.ING_ID2TOK_JSON, DATA.ING_TOK2ID_JSON)\n",
    "print(f\"Dedupe entries: {len(dedupe_map):,} | Encoder maps present: {ing_id2tok is not None}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89e4e61",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Inference helper\n",
    "\n",
    "Load the saved model, run NER on a **target dataset**, normalize strings, dedupe (optional), and encode to IDs (optional).  \n",
    "Sampling options are available to speed up quick checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf17ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7) Structured inference — original vs cleaned (wide + tall) ---\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from IPython.display import display, HTML\n",
    "from html import escape\n",
    "\n",
    "def _unique_preserve_order(seq):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "def _extract_ingredient_rows(doc, dedupe: Optional[dict] = None, tok2id: Optional[dict] = None):\n",
    "    \"\"\"Return a list of per-entity dicts with offsets + normalized/canonical forms.\"\"\"\n",
    "    rows = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ != \"INGREDIENT\":\n",
    "            continue\n",
    "        raw = ent.text\n",
    "        norm = normalize_token(raw)\n",
    "        canon = apply_dedupe(norm, dedupe)\n",
    "        tok_id = tok2id.get(canon, 0) if tok2id else None\n",
    "        rows.append({\n",
    "            \"raw\": raw,\n",
    "            \"start\": int(ent.start_char),\n",
    "            \"end\": int(ent.end_char),\n",
    "            \"label\": ent.label_,\n",
    "            \"norm\": norm,\n",
    "            \"canonical\": canon,\n",
    "            \"id\": int(tok_id) if tok_id is not None else None,\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def predict_normalize_encode_structured(\n",
    "    nlp_dir: Path,\n",
    "    data_path: Path,\n",
    "    is_parquet: bool,\n",
    "    text_col: str,\n",
    "    dedupe: Optional[dict] = None,\n",
    "    tok2id: Optional[dict] = None,\n",
    "    out_path: Optional[Path] = None,\n",
    "    batch_size: int = 256,\n",
    "    # sampling knobs (use exactly one)\n",
    "    sample_n: Optional[int] = None,\n",
    "    sample_frac: Optional[float] = None,\n",
    "    head_n: Optional[int] = None,\n",
    "    start: int = 0,\n",
    "    stop: Optional[int] = None,\n",
    "    sample_seed: int = 42,\n",
    "    # performance\n",
    "    n_process: int = 1,  # keep 1 for GPU/transformers\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      df_wide: one row per input, columns=[text_col, NER_raw, NER_clean, Ingredients?, spans_json]\n",
    "      df_tall: one row per extracted entity with offsets and normalized/canonical forms\n",
    "    If out_path is set, writes two parquet files: <stem>_wide.parquet and <stem>_tall.parquet\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(nlp_dir)\n",
    "\n",
    "    df_in = load_data(data_path, is_parquet, text_col)\n",
    "\n",
    "    # Apply ONE sampling strategy\n",
    "    if head_n is not None:\n",
    "        df_in = df_in.head(head_n)\n",
    "    elif sample_n is not None:\n",
    "        df_in = df_in.sample(n=min(sample_n, len(df_in)), random_state=sample_seed)\n",
    "    elif sample_frac is not None:\n",
    "        df_in = df_in.sample(frac=min(max(sample_frac, 0.0), 1.0), random_state=sample_seed)\n",
    "    elif start != 0 or stop is not None:\n",
    "        df_in = df_in.iloc[start:stop]\n",
    "\n",
    "    texts = df_in[text_col].astype(str).tolist()\n",
    "\n",
    "    wide_rows = []\n",
    "    tall_records = []\n",
    "\n",
    "    for i, doc in enumerate(tqdm(nlp.pipe(texts, batch_size=batch_size, n_process=n_process),\n",
    "                                 total=len(texts), desc=\"Infer (structured)\")):\n",
    "        rows = _extract_ingredient_rows(doc, dedupe=dedupe, tok2id=tok2id)\n",
    "\n",
    "        raw_list   = _unique_preserve_order([r[\"raw\"] for r in rows])\n",
    "        clean_list = _unique_preserve_order([r[\"canonical\"] for r in rows if r[\"canonical\"]])\n",
    "        id_list    = [r[\"id\"] for r in rows if r[\"id\"] is not None] if tok2id else None\n",
    "\n",
    "        # wide entry (compact)\n",
    "        wide_entry = {\n",
    "            text_col: texts[i],\n",
    "            \"NER_raw\": raw_list,\n",
    "            \"NER_clean\": clean_list,\n",
    "            \"spans_json\": json.dumps(rows, ensure_ascii=False),  # arrow-friendly\n",
    "        }\n",
    "        if tok2id:\n",
    "            wide_entry[\"Ingredients\"] = id_list\n",
    "        wide_rows.append(wide_entry)\n",
    "\n",
    "        # tall entries (one row per entity, great for QA/exploration)\n",
    "        for r in rows:\n",
    "            tall_records.append({\n",
    "                \"row_id\": i,\n",
    "                text_col: texts[i],\n",
    "                \"ent_text\": r[\"raw\"],\n",
    "                \"start\": r[\"start\"],\n",
    "                \"end\": r[\"end\"],\n",
    "                \"label\": r[\"label\"],\n",
    "                \"norm\": r[\"norm\"],\n",
    "                \"canonical\": r[\"canonical\"],\n",
    "                \"id\": r[\"id\"],\n",
    "            })\n",
    "\n",
    "    df_wide = pd.DataFrame(wide_rows)\n",
    "    df_tall = pd.DataFrame(tall_records)\n",
    "\n",
    "    if out_path is not None:\n",
    "        base = Path(out_path)\n",
    "        wide_path = base.with_name(base.stem + \"_wide.parquet\")\n",
    "        tall_path = base.with_name(base.stem + \"_tall.parquet\")\n",
    "        pq.write_table(pa.Table.from_pandas(df_wide, preserve_index=False).replace_schema_metadata(None), wide_path)\n",
    "        pq.write_table(pa.Table.from_pandas(df_tall, preserve_index=False).replace_schema_metadata(None), tall_path)\n",
    "        print(f\"Wrote → {wide_path.name} and {tall_path.name} in {wide_path.parent}\")\n",
    "\n",
    "    return df_wide, df_tall\n",
    "\n",
    "# ---- Quick visual helpers ----\n",
    "\n",
    "def preview_side_by_side(df_wide: pd.DataFrame, text_col: str, n: int = 8):\n",
    "    \"\"\"Simple tabular 'original vs cleaned' preview.\"\"\"\n",
    "    cols = [text_col, \"NER_raw\", \"NER_clean\"] + ([\"Ingredients\"] if \"Ingredients\" in df_wide.columns else [])\n",
    "    display(df_wide.loc[:, cols].head(n))\n",
    "\n",
    "def _render_marked(text: str, spans: list[dict]) -> str:\n",
    "    \"\"\"Mark entities inline; tooltip shows norm/canonical/id for quick QA.\"\"\"\n",
    "    spans = sorted(spans, key=lambda r: r[\"start\"])\n",
    "    pos = 0\n",
    "    out = []\n",
    "    for r in spans:\n",
    "        out.append(escape(text[pos:r[\"start\"]]))\n",
    "        frag = escape(text[r[\"start\"]:r[\"end\"]])\n",
    "        tip  = f'norm=\"{r[\"norm\"]}\" | canonical=\"{r[\"canonical\"]}\" | id={r[\"id\"] if r[\"id\"] is not None else \"-\"}'\n",
    "        out.append(f'<mark title=\"{escape(tip)}\">{frag}</mark>')\n",
    "        pos = r[\"end\"]\n",
    "    out.append(escape(text[pos:]))\n",
    "    return \"\".join(out)\n",
    "\n",
    "def html_preview(df_wide: pd.DataFrame, text_col: str, n: int = 8):\n",
    "    \"\"\"Inline HTML with highlighted entities and cleaned list below.\"\"\"\n",
    "    rows = []\n",
    "    for _, row in df_wide.head(n).iterrows():\n",
    "        spans = json.loads(row[\"spans_json\"])\n",
    "        marked = _render_marked(row[text_col], spans)\n",
    "        cleaned = \", \".join(row.get(\"NER_clean\") or [])\n",
    "        rows.append(f\"\"\"\n",
    "        <div class=\"one\">\n",
    "          <div class=\"orig\">{marked}</div>\n",
    "          <div class=\"clean\"><strong>NER_clean:</strong> {escape(cleaned)}</div>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "    style = \"\"\"\n",
    "    <style>\n",
    "      .one{border:1px solid #ddd; padding:10px; margin:8px 0; border-radius:6px;}\n",
    "      .orig{margin-bottom:6px; line-height:1.5}\n",
    "      mark{padding:0 2px; border-radius:3px}\n",
    "      .clean{font-family:monospace}\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    display(HTML(style + \"\\n\".join(rows)))\n",
    "\n",
    "def describe_predictions(df_wide: pd.DataFrame, top_k: int = 20):\n",
    "    \"\"\"Small summary to sanity-check output distribution.\"\"\"\n",
    "    s = (df_wide[\"NER_clean\"].explode().value_counts().head(top_k))\n",
    "    print(f\"Rows: {len(df_wide):,} | rows with ≥1 pred: {(df_wide['NER_clean'].map(bool)).sum():,}\")\n",
    "    print(f\"Mean #unique preds/row: {df_wide['NER_clean'].map(len).mean():.2f}\")\n",
    "    display(s.to_frame(\"freq\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb744e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example call (keeps your paths/variables):\n",
    "df_wide, df_tall = predict_normalize_encode_structured(\n",
    "    nlp_dir=OUT.MODEL_DIR,\n",
    "    data_path=DATA.TRAIN_PATH,             # replace with your new dataset path as needed\n",
    "    is_parquet=DATA.DATA_IS_PARQUET,\n",
    "    text_col=\"ingredients_text\",      # <-- change this to your free-text column\n",
    "    dedupe=dedupe_map,\n",
    "    tok2id=ing_tok2id,\n",
    "    out_path=OUT.PRED_OUT.with_name(\"pred_sample10k.parquet\"),  # writes *_wide and *_tall\n",
    "    sample_n=10_000,\n",
    "    sample_seed=123,\n",
    "    n_process=1                       # keep 1 when using transformers/GPU\n",
    ")\n",
    "\n",
    "# Side-by-side table and HTML highlights\n",
    "preview_side_by_side(df_wide, text_col=\"ingredients_text\", n=10)\n",
    "html_preview(df_wide, text_col=\"ingredients_text\", n=10)\n",
    "\n",
    "# Quick distributional sanity check\n",
    "describe_predictions(df_wide)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
