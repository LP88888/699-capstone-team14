{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76418ad7",
   "metadata": {},
   "source": [
    "# Ingredient Normalization (Data-Driven, PMI-Based)\n",
    "\n",
    "## This notebook implements a **fully data-driven** normalization for ingredient phrases at scale (~2 M rows).\n",
    "\n",
    "-   1. Streams your dataset to count unigrams/bigrams/trigrams  \n",
    "-   2. Computes PMI-style association scores  \n",
    "-   3. Builds a canonical vocabulary (no hard-coded lists)  \n",
    "-   4. Segments each NER item by greedy longest-match using that vocabulary  \n",
    "-   5. Writes a cleaned column (`NER_clean`) back to disk\n",
    "\n",
    "### Designed for large CSVs: uses chunked ingestion and optional on-disk checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b276f7",
   "metadata": {},
   "source": [
    "### Ingredient Normalization Pipeline\n",
    "\n",
    "<img src=\"./ingredient_row_sequence.png\" alt=\"Ingredient normalization pipeline\" width=\"850\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd9efb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional installs (run once if missing)\n",
    "# !pip install pyarrow\n",
    "\n",
    "# # Update requirements.txt \n",
    "# !pip freeze > ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545fbbcf",
   "metadata": {},
   "source": [
    "## Config and global imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "812d2da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, gc, json, math, re, ast\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_DATA_PATH = Path(\"../data/wilmerarltstrmberg_data.csv\")  # raw CSV\n",
    "# Create a small sample for quick iteration/tests\n",
    "DATA_PATH = Path(\"../data/sample_data.csv\")\n",
    "# df = pd.read_csv(RAW_DATA_PATH, nrows=10_000, dtype=str) # remove nrows for full data and comment if path already exists\n",
    "# df.to_csv(DATA_PATH, index=False) # comment if path already exists\n",
    "\n",
    "OUTPUT_PATH = Path(\"../data/recipes_data_clean.parquet\")\n",
    "VOCAB_JSON = Path(\"../data/ingredient_vocab_stats.json\")\n",
    "NER_COL = \"NER\"\n",
    "CHUNK_SIZE = 200_000\n",
    "\n",
    "# Thresholds\n",
    "MIN_UNIGRAM = 50\n",
    "MIN_BIGRAM = 50\n",
    "MIN_TRIGRAM = 30\n",
    "PMI_BIGRAM = 3.0\n",
    "PMI_TRIGRAM = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e3ec4",
   "metadata": {},
   "source": [
    "## StatsNormalizer (1 to 4 gram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a262b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsNormalizer:\n",
    "    \"\"\"\n",
    "    N-gram normalizer with:\n",
    "      - Streaming counts (1..4)\n",
    "      - PMI-based canon + entropy/child-share gates\n",
    "      - Greedy 4->3->2->1 segmentation\n",
    "      - Span-preserving fuzzy snap (rapidfuzz if available)\n",
    "      - Dynamic 4/3/2 fallbacks when canon misses strong collocations\n",
    "      - CSV->Parquet writer\n",
    "\n",
    "    Public:\n",
    "      ingest_csv, ingest_df, build_vocab,\n",
    "      segment_item, transform_df, transform_csv_to_parquet,\n",
    "      save_vocab, load_vocab\n",
    "    \"\"\"\n",
    "    # ---- utilities ----\n",
    "    @staticmethod\n",
    "    def _tok(s):\n",
    "        import re\n",
    "        return re.findall(r\"[a-z']+\", str(s).lower())\n",
    "\n",
    "    @staticmethod\n",
    "    def _ngrams(tokens, n):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            yield tuple(tokens[i:i+n])\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_ner_entry(entry):\n",
    "        if entry is None or (isinstance(entry, float) and pd.isna(entry)):\n",
    "            return []\n",
    "        s = str(entry).strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        try:\n",
    "            parsed = ast.literal_eval(s)\n",
    "            if isinstance(parsed, list):\n",
    "                return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "        return [x.strip() for x in s.split(\",\") if x.strip()]\n",
    "\n",
    "    # ---- init ----\n",
    "    def __init__(self,\n",
    "                 max_ngram=4,\n",
    "                 # canon thresholds\n",
    "                 min_unigram=50, min_bigram=50, min_trigram=30, min_fourgram=20,\n",
    "                 pmi_bigram=3.0, pmi_trigram=2.0, pmi_fourgram=2.0,\n",
    "                 # trigram gates\n",
    "                 min_child_share=0.12, max_right_entropy=1.0,\n",
    "                 # 4-gram gates (trigram-head branching)\n",
    "                 min_child_share4=0.05, max_right_entropy3=1.3,\n",
    "                 # dynamic fallbacks\n",
    "                 pmi_bigram_fallback=2.6,  min_bigram_fallback=20,\n",
    "                 pmi_trigram_fallback=2.2, min_trigram_fallback=12,\n",
    "                 min_child_share_fallback=0.06, max_right_entropy_fallback=1.4,\n",
    "                 pmi_fourgram_fallback=1.8, min_fourgram_fallback=10,\n",
    "                 min_child_share4_fallback=0.04, max_right_entropy3_fallback=1.5,\n",
    "                 # snap settings\n",
    "                 snap_score_cutoff=92, snap_near_perfect=96):\n",
    "        from collections import defaultdict\n",
    "        self.max_ngram = int(max_ngram)\n",
    "\n",
    "        self.min_unigram, self.min_bigram, self.min_trigram, self.min_fourgram = (\n",
    "            min_unigram, min_bigram, min_trigram, min_fourgram\n",
    "        )\n",
    "        self.pmi_bigram, self.pmi_trigram, self.pmi_fourgram = PMI_BIGRAM, PMI_TRIGRAM, pmi_fourgram\n",
    "        # allow external overrides\n",
    "        self.pmi_bigram = pmi_bigram\n",
    "        self.pmi_trigram = pmi_trigram\n",
    "\n",
    "        self.min_child_share, self.max_right_entropy = min_child_share, max_right_entropy\n",
    "        self.min_child_share4, self.max_right_entropy3 = min_child_share4, max_right_entropy3\n",
    "\n",
    "        self.pmi_bigram_fallback, self.min_bigram_fallback = pmi_bigram_fallback, min_bigram_fallback\n",
    "        self.pmi_trigram_fallback, self.min_trigram_fallback = pmi_trigram_fallback, min_trigram_fallback\n",
    "        self.min_child_share_fallback, self.max_right_entropy_fallback = (\n",
    "            min_child_share_fallback, max_right_entropy_fallback\n",
    "        )\n",
    "        self.pmi_fourgram_fallback, self.min_fourgram_fallback = pmi_fourgram_fallback, min_fourgram_fallback\n",
    "        self.min_child_share4_fallback, self.max_right_entropy3_fallback = (\n",
    "            min_child_share4_fallback, max_right_entropy3_fallback\n",
    "        )\n",
    "\n",
    "        self.snap_score_cutoff, self.snap_near_perfect = snap_score_cutoff, snap_near_perfect\n",
    "\n",
    "        self.token_total = 0\n",
    "        self.c1, self.c2, self.c3, self.c4 = Counter(), Counter(), Counter(), Counter()\n",
    "\n",
    "        self._followers  = defaultdict(Counter)  # (a,b)->c\n",
    "        self._followers3 = defaultdict(Counter)  # (a,b,c)->d\n",
    "\n",
    "        self.canon = set()\n",
    "        self._canon_ready = False\n",
    "        self._canon_phrases = None\n",
    "        self._canon_buckets = None\n",
    "\n",
    "    # ---- ingest ----\n",
    "    def ingest_df(self, df, ner_col=\"NER\"):\n",
    "        for entry in df[ner_col]:\n",
    "            for item in self._parse_ner_entry(entry):\n",
    "                t = self._tok(item)\n",
    "                if not t:\n",
    "                    continue\n",
    "\n",
    "                self.c1.update(t)\n",
    "                self.token_total += len(t)\n",
    "\n",
    "                if self.max_ngram >= 2 and len(t) >= 2:\n",
    "                    self.c2.update(self._ngrams(t, 2))\n",
    "\n",
    "                if self.max_ngram >= 3 and len(t) >= 3:\n",
    "                    for i in range(len(t) - 2):\n",
    "                        a, b, c = t[i], t[i+1], t[i+2]\n",
    "                        self.c3[(a,b,c)] += 1\n",
    "                        self._followers[(a,b)][c] += 1\n",
    "\n",
    "                if self.max_ngram >= 4 and len(t) >= 4:\n",
    "                    for i in range(len(t) - 3):\n",
    "                        a, b, c, d = t[i], t[i+1], t[i+2], t[i+3]\n",
    "                        self.c4[(a,b,c,d)] += 1\n",
    "                        self._followers3[(a,b,c)][d] += 1\n",
    "\n",
    "    def ingest_csv(self, csv_path, ner_col=\"NER\", chunksize=200_000):\n",
    "        for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
    "            self.ingest_df(chunk, ner_col=ner_col)\n",
    "            del chunk; gc.collect()\n",
    "\n",
    "    # ---- stats ----\n",
    "    def _right_entropy(self, ab):\n",
    "        foll = self._followers.get(ab)\n",
    "        if not foll:\n",
    "            return 0.0\n",
    "        tot = sum(foll.values())\n",
    "        if tot == 0: return 0.0\n",
    "        H = 0.0\n",
    "        for v in foll.values():\n",
    "            p = v / tot\n",
    "            H -= p * math.log(p + 1e-12)\n",
    "        return H\n",
    "\n",
    "    def _right_entropy3(self, abc):\n",
    "        foll = self._followers3.get(abc)\n",
    "        if not foll:\n",
    "            return 0.0\n",
    "        tot = sum(foll.values())\n",
    "        if tot == 0: return 0.0\n",
    "        H = 0.0\n",
    "        for v in foll.values():\n",
    "            p = v / tot\n",
    "            H -= p * math.log(p + 1e-12)\n",
    "        return H\n",
    "\n",
    "    def _child_share(self, abc):\n",
    "        cabc = self.c3.get(abc, 0)\n",
    "        cab  = self.c2.get(abc[:2], 0)\n",
    "        return (cabc / cab) if cab else 0.0\n",
    "\n",
    "    def _child_share4(self, abcd):\n",
    "        cabcd = self.c4.get(abcd, 0)\n",
    "        cabc  = self.c3.get(abcd[:3], 0)\n",
    "        return (cabcd / cabc) if cabc else 0.0\n",
    "\n",
    "    def _pmi_bigram(self, ab):\n",
    "        a, b = ab\n",
    "        cab = self.c2.get(ab, 0)\n",
    "        if cab == 0 or self.token_total == 0: return -1e9\n",
    "        pa = self.c1.get(a,0)/self.token_total\n",
    "        pb = self.c1.get(b,0)/self.token_total\n",
    "        pab = cab/self.token_total\n",
    "        return math.log((pab/(pa*pb))+1e-12)\n",
    "\n",
    "    def _pmi_trigram(self, abc):\n",
    "        a,b,c = abc\n",
    "        return (self._pmi_bigram((a,b)) + self._pmi_bigram((b,c)))/2.0\n",
    "\n",
    "    def _pmi_fourgram(self, abcd):\n",
    "        a,b,c,d = abcd\n",
    "        return (self._pmi_bigram((a,b)) + self._pmi_bigram((b,c)) + self._pmi_bigram((c,d))) / 3.0\n",
    "\n",
    "    # ---- build canon ----\n",
    "    def build_vocab(self):\n",
    "        self.canon.clear()\n",
    "\n",
    "        for w,c in self.c1.items():\n",
    "            if c >= self.min_unigram:\n",
    "                self.canon.add((w,))\n",
    "\n",
    "        for ab,c in self.c2.items():\n",
    "            if c >= self.min_bigram and self._pmi_bigram(ab) >= self.pmi_bigram:\n",
    "                self.canon.add(ab)\n",
    "\n",
    "        for abc,c in self.c3.items():\n",
    "            if c < self.min_trigram: continue\n",
    "            if self._pmi_trigram(abc) < self.pmi_trigram: continue\n",
    "            if self._child_share(abc) < self.min_child_share: continue\n",
    "            if self._right_entropy(abc[:2]) > self.max_right_entropy: continue\n",
    "            self.canon.add(abc)\n",
    "\n",
    "        for abcd,c in self.c4.items():\n",
    "            if c < self.min_fourgram: continue\n",
    "            if self._pmi_fourgram(abcd) < self.pmi_fourgram: continue\n",
    "            if self._child_share4(abcd) < self.min_child_share4: continue\n",
    "            if self._right_entropy3(abcd[:3]) > self.max_right_entropy3: continue\n",
    "            self.canon.add(abcd)\n",
    "\n",
    "        self._canon_ready = True\n",
    "        self._canon_phrases = None\n",
    "        self._canon_buckets = None\n",
    "\n",
    "    # ---- snap helpers ----\n",
    "    def _canon_bucket_init(self):\n",
    "        self._canon_phrases = [\" \".join(p) for p in self.canon]\n",
    "        buckets = {}\n",
    "        for ph in self._canon_phrases:\n",
    "            ft = ph.split()[0] if ph else \"\"\n",
    "            buckets.setdefault(ft, []).append(ph)\n",
    "        self._canon_buckets = buckets\n",
    "\n",
    "    def _snap_span(self, tokens, i, n):\n",
    "        \"\"\"\n",
    "        Try snapping tokens[i:i+n] to a canon phrase with same length (or near-perfect match).\n",
    "        Uses rapidfuzz if available; otherwise no-op.\n",
    "        \"\"\"\n",
    "        if i + n > len(tokens):\n",
    "            return None\n",
    "        if self._canon_phrases is None or self._canon_buckets is None:\n",
    "            self._canon_bucket_init()\n",
    "        try:\n",
    "            from rapidfuzz import process, fuzz\n",
    "        except Exception:\n",
    "            return None  # RF not installed; skip snapping\n",
    "\n",
    "        span = \" \".join(tokens[i:i+n])\n",
    "        bucket = self._canon_buckets.get(tokens[i], self._canon_phrases)\n",
    "        match = process.extractOne(span, bucket, scorer=fuzz.WRatio, score_cutoff=self.snap_score_cutoff)\n",
    "        if not match:\n",
    "            return None\n",
    "\n",
    "        # Robust to RF versions: tuple like (choice, score, idx) or object with indexing\n",
    "        cand = match[0]\n",
    "        score = match[1]\n",
    "\n",
    "        if len(cand.split()) == n or score >= self.snap_near_perfect:\n",
    "            return cand.split(), n\n",
    "        return None\n",
    "\n",
    "\n",
    "    # segmentation\n",
    "    def _longest_match(self, toks, i):\n",
    "        if not self._canon_ready:\n",
    "            raise RuntimeError(\"build_vocab() first\")\n",
    "\n",
    "        # Exact canon first (4 -> 3 -> 2 -> 1)\n",
    "        if self.max_ngram >= 4 and i+3 < len(toks) and tuple(toks[i:i+4]) in self.canon:\n",
    "            return tuple(toks[i:i+4]), 4\n",
    "        if self.max_ngram >= 3 and i+2 < len(toks) and tuple(toks[i:i+3]) in self.canon:\n",
    "            return tuple(toks[i:i+3]), 3\n",
    "        if self.max_ngram >= 2 and i+1 < len(toks) and tuple(toks[i:i+2]) in self.canon:\n",
    "            return tuple(toks[i:i+2]), 2\n",
    "        if (toks[i],) in self.canon:\n",
    "            return (toks[i],), 1\n",
    "\n",
    "        # Snap to canon (3, then 2)\n",
    "        if self.max_ngram >= 3:\n",
    "            snapped = self._snap_span(toks, i, 3)\n",
    "            if snapped: return tuple(snapped[0]), snapped[1]\n",
    "        if self.max_ngram >= 2:\n",
    "            snapped = self._snap_span(toks, i, 2)\n",
    "            if snapped: return tuple(snapped[0]), snapped[1]\n",
    "\n",
    "        # Dynamic fallbacks (4,3,2)\n",
    "        if self.max_ngram >= 4 and i+3 < len(toks):\n",
    "            abcd = (toks[i], toks[i+1], toks[i+2], toks[i+3])\n",
    "            cabcd = self.c4.get(abcd, 0)\n",
    "            if cabcd >= self.min_fourgram_fallback:\n",
    "                if (self._pmi_fourgram(abcd) >= self.pmi_fourgram_fallback and\n",
    "                    self._child_share4(abcd) >= self.min_child_share4_fallback and\n",
    "                    self._right_entropy3(abcd[:3]) <= self.max_right_entropy3_fallback):\n",
    "                    return abcd, 4\n",
    "\n",
    "        if self.max_ngram >= 3 and i+2 < len(toks):\n",
    "            abc = (toks[i], toks[i+1], toks[i+2])\n",
    "            cabc = self.c3.get(abc, 0)\n",
    "            if cabc >= self.min_trigram_fallback:\n",
    "                if (self._pmi_trigram(abc) >= self.pmi_trigram_fallback and\n",
    "                    self._child_share(abc) >= self.min_child_share_fallback and\n",
    "                    self._right_entropy(abc[:2]) <= self.max_right_entropy_fallback):\n",
    "                    return abc, 3\n",
    "\n",
    "        if self.max_ngram >= 2 and i+1 < len(toks):\n",
    "            ab = (toks[i], toks[i+1])\n",
    "            cab = self.c2.get(ab, 0)\n",
    "            if cab >= self.min_bigram_fallback and self._pmi_bigram(ab) >= self.pmi_bigram_fallback:\n",
    "                return ab, 2\n",
    "\n",
    "        return (toks[i],), 1\n",
    "\n",
    "    def segment_item(self, text):\n",
    "        t = self._tok(text)\n",
    "        out, i = [], 0\n",
    "        while i < len(t):\n",
    "            phrase, k = self._longest_match(t, i)\n",
    "            out.append(\" \".join(phrase))\n",
    "            i += k\n",
    "\n",
    "        # de-dup while keeping order\n",
    "        seen, clean = set(), []\n",
    "        for x in out:\n",
    "            if x not in seen:\n",
    "                clean.append(x); seen.add(x)\n",
    "\n",
    "        # drop immediate repetition of previous tail\n",
    "        pruned = []\n",
    "        for x in clean:\n",
    "            if pruned and x == pruned[-1].split()[-1]:\n",
    "                continue\n",
    "            pruned.append(x)\n",
    "        return pruned\n",
    "\n",
    "    # dataframe & IO \n",
    "    def transform_df(self, df, ner_col=\"NER\", out_col=\"NER_clean\", dedupe_row=False):\n",
    "        results = []\n",
    "        for v in df[ner_col]:\n",
    "            segs = [seg for item in self._parse_ner_entry(v) for seg in self.segment_item(item)]\n",
    "            if dedupe_row:\n",
    "                seen, uniq = set(), []\n",
    "                for s in segs:\n",
    "                    if s not in seen:\n",
    "                        uniq.append(s); seen.add(s)\n",
    "                results.append(uniq)\n",
    "            else:\n",
    "                results.append(segs)\n",
    "        df[out_col] = results\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _sanitize_for_arrow(df, list_col=\"NER_clean\"):\n",
    "        import pyarrow as pa  # only to check types later\n",
    "        df = df.copy()\n",
    "\n",
    "        # Ensure list[str]\n",
    "        def _to_list_of_str(x):\n",
    "            if isinstance(x, (list, tuple)):\n",
    "                return [str(y) for y in x]\n",
    "            if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "                return []\n",
    "            try:\n",
    "                val = json.loads(x)\n",
    "                if isinstance(val, list):\n",
    "                    return [str(y) for y in val]\n",
    "            except Exception:\n",
    "                pass\n",
    "            return [str(x)]\n",
    "\n",
    "        if list_col in df.columns:\n",
    "            df[list_col] = df[list_col].apply(_to_list_of_str)\n",
    "\n",
    "        # Minimal sanitization for non-list columns\n",
    "        for col in df.columns:\n",
    "            if col == list_col:\n",
    "                continue\n",
    "            s = df[col]\n",
    "            if s.dtype == object:\n",
    "                def _to_scalar_str(v):\n",
    "                    if isinstance(v, (list, tuple, dict, set)):\n",
    "                        return json.dumps(v, ensure_ascii=False)\n",
    "                    return \"\" if v is None or (isinstance(v, float) and pd.isna(v)) else str(v)\n",
    "                df[col] = s.map(_to_scalar_str)\n",
    "        return df\n",
    "\n",
    "    def transform_csv_to_parquet(self, csv_path, out_path, ner_col=\"NER\", chunksize=200_000):\n",
    "        import pyarrow as pa, pyarrow.parquet as pq\n",
    "        writer = None\n",
    "        for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
    "            chunk = self.transform_df(chunk, ner_col=ner_col, out_col=\"NER_clean\")\n",
    "            chunk = self._sanitize_for_arrow(chunk, list_col=\"NER_clean\")\n",
    "\n",
    "            table = pa.Table.from_pandas(chunk, preserve_index=False).replace_schema_metadata(None)\n",
    "            fields = []\n",
    "            for f in table.schema:\n",
    "                if f.name == \"NER_clean\" and not pa.types.is_list(f.type):\n",
    "                    fields.append(pa.field(\"NER_clean\", pa.list_(pa.string())))\n",
    "                else:\n",
    "                    fields.append(f)\n",
    "            target_schema = pa.schema(fields)\n",
    "            try:\n",
    "                table = table.cast(target_schema, safe=False)\n",
    "            except Exception:\n",
    "                arrays = [pa.array(arr, type=pa.list_(pa.string())) for arr in table.column(\"NER_clean\").to_pylist()]\n",
    "                table = table.set_column(table.schema.get_field_index(\"NER_clean\"), \"NER_clean\", pa.chunked_array(arrays))\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(out_path, target_schema, compression=\"zstd\")\n",
    "            writer.write_table(table)\n",
    "\n",
    "            del chunk, table; gc.collect()\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "    # persistence \n",
    "    def save_vocab(self, path):\n",
    "        data = {\"token_total\": int(self.token_total),\n",
    "                \"canon\": [\" \".join(p) for p in sorted(self.canon)]}\n",
    "        Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load_vocab(cls, path):\n",
    "        data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "        obj = cls()\n",
    "        obj.canon = set(tuple(p.split()) for p in data[\"canon\"])\n",
    "        obj._canon_ready = True\n",
    "        obj._canon_phrases = None\n",
    "        obj._canon_buckets = None\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef4508",
   "metadata": {},
   "source": [
    "## PASS 1: Count n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6749868c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming counts from ..\\data\\sample_data.csv\n",
      "Total tokens: 105242\n"
     ]
    }
   ],
   "source": [
    "normalizer = StatsNormalizer(\n",
    "    max_ngram=4,\n",
    "    min_unigram=MIN_UNIGRAM,\n",
    "    min_bigram=MIN_BIGRAM,\n",
    "    min_trigram=MIN_TRIGRAM,\n",
    "    pmi_bigram=PMI_BIGRAM,\n",
    "    pmi_trigram=PMI_TRIGRAM,\n",
    "    min_child_share=0.01,     # keep more trigrams\n",
    "    max_right_entropy=1.0\n",
    ")\n",
    "print(\"Streaming counts from\", DATA_PATH)\n",
    "normalizer.ingest_csv(DATA_PATH, ner_col=NER_COL, chunksize=CHUNK_SIZE)\n",
    "print(\"Total tokens:\", normalizer.token_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6bf431",
   "metadata": {},
   "source": [
    "## PASS 2: Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4026cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canonical phrases: 373\n",
      "Saved vocab: ..\\data\\ingredient_vocab_stats.json\n"
     ]
    }
   ],
   "source": [
    "normalizer.build_vocab()\n",
    "normalizer.save_vocab(VOCAB_JSON)\n",
    "print(\"Canonical phrases:\", len(normalizer.canon))\n",
    "print(\"Saved vocab:\", VOCAB_JSON)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2370b5",
   "metadata": {},
   "source": [
    "## PASS A: Build a global spell/fuzzy map (threaded, fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cae22655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique NER items: 3969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spell/Fuzzy map: 100%|██████████| 2/2 [00:38<00:00, 19.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote spell map: ..\\data\\ner_spell_map.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from rapidfuzz import process, fuzz\n",
    "from spellchecker import SpellChecker\n",
    "import threading\n",
    "\n",
    "UNIQ_PATH = Path(\"../data/ner_unique.txt\")\n",
    "MAP_PATH  = Path(\"../data/ner_spell_map.jsonl\")\n",
    "FTHRESH   = 90\n",
    "BATCH     = 2000\n",
    "WORKERS   = min(16, (os.cpu_count() or 2) * 2)\n",
    "\n",
    "# 1) Collect unique raw items (stream)\n",
    "seen = set()\n",
    "for chunk in pd.read_csv(DATA_PATH, chunksize=CHUNK_SIZE, dtype=str):\n",
    "    for entry in chunk[NER_COL]:\n",
    "        for item in normalizer._parse_ner_entry(entry):\n",
    "            if item:\n",
    "                seen.add(item)\n",
    "items = sorted(seen)\n",
    "UNIQ_PATH.write_text(\"\\n\".join(items), encoding=\"utf-8\")\n",
    "print(\"Unique NER items:\", len(items))\n",
    "\n",
    "# 2) Prepare canon & buckets from PMI vocab\n",
    "canon_phrases = [\" \".join(p) for p in normalizer.canon]\n",
    "canon_set = set(canon_phrases)\n",
    "buckets = {}\n",
    "for p in canon_phrases:\n",
    "    toks = p.split()\n",
    "    ft = toks[0] if toks else \"\"\n",
    "    buckets.setdefault(ft, []).append(p)\n",
    "\n",
    "# Thread-local spellchecker\n",
    "_tls = threading.local()\n",
    "vocab_tokens = [t for ph in canon_phrases for t in ph.split()]\n",
    "def _get_spell():\n",
    "    sc = getattr(_tls, \"sc\", None)\n",
    "    if sc is None:\n",
    "        sc = SpellChecker(distance=2)\n",
    "        sc.word_frequency.load_words(vocab_tokens)\n",
    "        _tls.sc = sc\n",
    "    return sc\n",
    "\n",
    "def _fix_batch(batch):\n",
    "    \"\"\"Token-level spell; phrase snap only if same length or near-perfect.\"\"\"\n",
    "    sc = _get_spell()\n",
    "    out = []\n",
    "    for raw in batch:\n",
    "        toks = normalizer._tok(raw)\n",
    "        if not toks:\n",
    "            out.append((raw, raw)); continue\n",
    "\n",
    "        toks2 = [sc.correction(t) or t for t in toks]\n",
    "        corrected = \" \".join(toks2)\n",
    "        if corrected in canon_set:\n",
    "            out.append((raw, corrected)); continue\n",
    "\n",
    "        choices = buckets.get(toks2[0], canon_phrases)\n",
    "        match = process.extractOne(corrected, choices, scorer=fuzz.WRatio, score_cutoff=92)\n",
    "        if match:\n",
    "            cand, score = match[0], match[1]\n",
    "            len_ok = (len(cand.split()) == len(toks2))\n",
    "            near_perfect = score >= 96\n",
    "            out.append((raw, cand if (len_ok or near_perfect) else corrected))\n",
    "        else:\n",
    "            out.append((raw, corrected))\n",
    "    return out\n",
    "\n",
    "def _chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "batches = list(_chunks(items, BATCH))\n",
    "with ThreadPoolExecutor(max_workers=WORKERS) as ex, open(MAP_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "    for res in tqdm(ex.map(_fix_batch, batches), total=len(batches), desc=\"Spell/Fuzzy map\"):\n",
    "        for raw, fixed in res:\n",
    "            out.write(json.dumps({\"raw\": raw, \"fixed\": fixed}) + \"\\n\")\n",
    "print(\"Wrote spell map:\", MAP_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fdb19a",
   "metadata": {},
   "source": [
    "## PASS B: Apply the map during write (fast runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a220b77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote cleaned (spell-mapped) file: ..\\data\\recipes_data_clean_spell.parquet\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa, pyarrow.parquet as pq\n",
    "\n",
    "OUTPUT_PATH_SPELL = Path(\"../data/recipes_data_clean_spell.parquet\")\n",
    "\n",
    "# load map into memory\n",
    "spell_map = {}\n",
    "with open(MAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        spell_map[obj[\"raw\"]] = obj[\"fixed\"]\n",
    "\n",
    "def _apply_map(entry):\n",
    "    return [spell_map.get(x, x) for x in normalizer._parse_ner_entry(entry)]\n",
    "\n",
    "def write_with_spellmap(normalizer, csv_path, out_path, ner_col=\"NER\", chunksize=CHUNK_SIZE):\n",
    "    writer = None\n",
    "    for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
    "        chunk[\"NER_spellchecked\"] = chunk[ner_col].apply(_apply_map)\n",
    "        chunk = normalizer.transform_df(chunk, ner_col=\"NER_spellchecked\", out_col=\"NER_clean\")\n",
    "        chunk = normalizer._sanitize_for_arrow(chunk, list_col=\"NER_clean\")\n",
    "\n",
    "        table = pa.Table.from_pandas(chunk, preserve_index=False).replace_schema_metadata(None)\n",
    "        fields = []\n",
    "        for f in table.schema:\n",
    "            if f.name == \"NER_clean\" and not pa.types.is_list(f.type):\n",
    "                fields.append(pa.field(\"NER_clean\", pa.list_(pa.string())))\n",
    "            else:\n",
    "                fields.append(f)\n",
    "        schema = pa.schema(fields)\n",
    "        try:\n",
    "            table = table.cast(schema, safe=False)\n",
    "        except Exception:\n",
    "            arrays = [pa.array(arr, type=pa.list_(pa.string())) for arr in table.column(\"NER_clean\").to_pylist()]\n",
    "            table = table.set_column(table.schema.get_field_index(\"NER_clean\"), \"NER_clean\", pa.chunked_array(arrays))\n",
    "\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(out_path, schema, compression=\"zstd\", compression_level=1)\n",
    "        writer.write_table(table)\n",
    "        del chunk, table; gc.collect()\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "write_with_spellmap(\n",
    "    normalizer=normalizer,\n",
    "    csv_path=DATA_PATH,\n",
    "    out_path=OUTPUT_PATH_SPELL,\n",
    "    ner_col=NER_COL,\n",
    "    chunksize=CHUNK_SIZE\n",
    ")\n",
    "print(\"Wrote cleaned (spell-mapped) file:\", OUTPUT_PATH_SPELL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe8ddc0",
   "metadata": {},
   "source": [
    "## PASS C: Quick evaluation (optional, light & informative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "768718de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "BASELINE_PATH = Path(\"../data/recipes_data_clean.parquet\")       # from PASS 3 below\n",
    "SPELL_PATH    = Path(\"../data/recipes_data_clean_spell.parquet\") # from Pass B\n",
    "MAP_PATH      = Path(\"../data/ner_spell_map.jsonl\")\n",
    "\n",
    "def scan_parquet(path, columns=(\"NER_clean\",)):\n",
    "    pf = pq.ParquetFile(path)\n",
    "    for rg in range(pf.num_row_groups):\n",
    "        tbl = pf.read_row_group(rg, columns=list(columns))\n",
    "        df = tbl.to_pandas()\n",
    "        yield df\n",
    "        del df, tbl; gc.collect()\n",
    "\n",
    "def parquet_vocab_and_lengths(path):\n",
    "    vocab = set()\n",
    "    total_rows = total_items = 0\n",
    "    for df_part in scan_parquet(path, columns=(\"NER_clean\",)):\n",
    "        total_rows += len(df_part)\n",
    "        lens = df_part[\"NER_clean\"].apply(len)\n",
    "        total_items += lens.sum()\n",
    "        for lst in df_part[\"NER_clean\"]:\n",
    "            for tok in lst:\n",
    "                vocab.add(tok)\n",
    "    avg_len = (total_items / total_rows) if total_rows else 0.0\n",
    "    return {\n",
    "        \"rows\": total_rows,\n",
    "        \"avg_ingredients_per_row\": avg_len,\n",
    "        \"total_ingredients_emitted\": int(total_items),\n",
    "        \"vocab_size\": len(vocab),\n",
    "        \"vocab\": vocab\n",
    "    }\n",
    "\n",
    "def count_in_csv(csv_path, ner_col, target_set, chunksize=200_000):\n",
    "    cnt = Counter()\n",
    "    for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
    "        for entry in chunk[ner_col]:\n",
    "            for item in normalizer._parse_ner_entry(entry):\n",
    "                if item in target_set:\n",
    "                    cnt[item] += 1\n",
    "        del chunk; gc.collect()\n",
    "    return cnt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739eec79",
   "metadata": {},
   "source": [
    "## PASS 3: Segment & write\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "934a0f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote cleaned file: ..\\data\\recipes_data_clean.parquet\n"
     ]
    }
   ],
   "source": [
    "normalizer.transform_csv_to_parquet(\n",
    "    csv_path=DATA_PATH,\n",
    "    out_path=OUTPUT_PATH,\n",
    "    ner_col=NER_COL,\n",
    "    chunksize=CHUNK_SIZE\n",
    ")\n",
    "print(\"Wrote cleaned file:\", OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24276b91",
   "metadata": {},
   "source": [
    "## Sanity tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40832da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span tests passed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NER</th>\n",
       "      <th>NER_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"bite size shredded rice biscuits\", \"vanilla\"...</td>\n",
       "      <td>[bite, size, shredded, rice, biscuits, vanilla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"cream of mushroom soup\", \"beef\", \"sour cream...</td>\n",
       "      <td>[cream of mushroom soup, beef, sour cream, chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"frozen corn\", \"pepper\", \"cream cheese\", \"gar...</td>\n",
       "      <td>[frozen, corn, pepper, cream, cheese, garlic, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"chicken gravy\", \"cream of mushroom soup\", \"c...</td>\n",
       "      <td>[chicken, gravy, cream of mushroom soup, chick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"graham cracker crumbs\", \"powdered sugar\", \"p...</td>\n",
       "      <td>[graham cracker, crumbs, powdered, sugar, pean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[\"sour cream\", \"bacon\", \"pepper\", \"extra lean ...</td>\n",
       "      <td>[sour cream, bacon, pepper, extra, lean ground...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[\"buttermilk\", \"egg\", \"sugar\", \"vanilla\", \"sod...</td>\n",
       "      <td>[buttermilk, egg, sugar, vanilla, soda, flour,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[\"egg\", \"pepper\", \"crackers\", \"cream-style cor...</td>\n",
       "      <td>[egg, pepper, crackers, cream style, corn, who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[\"oil\", \"tomatoes\", \"green peppers\", \"water\", ...</td>\n",
       "      <td>[oil, tomatoes, green peppers, water, onions, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[\"condensed milk\", \"lemons\", \"graham cracker c...</td>\n",
       "      <td>[condensed milk, lemons, graham cracker, crust...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 NER  \\\n",
       "0  [\"bite size shredded rice biscuits\", \"vanilla\"...   \n",
       "1  [\"cream of mushroom soup\", \"beef\", \"sour cream...   \n",
       "2  [\"frozen corn\", \"pepper\", \"cream cheese\", \"gar...   \n",
       "3  [\"chicken gravy\", \"cream of mushroom soup\", \"c...   \n",
       "4  [\"graham cracker crumbs\", \"powdered sugar\", \"p...   \n",
       "5  [\"sour cream\", \"bacon\", \"pepper\", \"extra lean ...   \n",
       "6  [\"buttermilk\", \"egg\", \"sugar\", \"vanilla\", \"sod...   \n",
       "7  [\"egg\", \"pepper\", \"crackers\", \"cream-style cor...   \n",
       "8  [\"oil\", \"tomatoes\", \"green peppers\", \"water\", ...   \n",
       "9  [\"condensed milk\", \"lemons\", \"graham cracker c...   \n",
       "\n",
       "                                           NER_clean  \n",
       "0  [bite, size, shredded, rice, biscuits, vanilla...  \n",
       "1  [cream of mushroom soup, beef, sour cream, chi...  \n",
       "2  [frozen, corn, pepper, cream, cheese, garlic, ...  \n",
       "3  [chicken, gravy, cream of mushroom soup, chick...  \n",
       "4  [graham cracker, crumbs, powdered, sugar, pean...  \n",
       "5  [sour cream, bacon, pepper, extra, lean ground...  \n",
       "6  [buttermilk, egg, sugar, vanilla, soda, flour,...  \n",
       "7  [egg, pepper, crackers, cream style, corn, who...  \n",
       "8  [oil, tomatoes, green peppers, water, onions, ...  \n",
       "9  [condensed milk, lemons, graham cracker, crust...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = [\"brown sugar ham\", \"brown sugar\", \"dark brown sugar\",\n",
    "         \"cream of mushroom soup\", \"extra lean ground beef\", \"graham cracker crusts\"]\n",
    "for s in tests:\n",
    "    seg = normalizer.segment_item(s)\n",
    "    joined = \" | \".join(seg)\n",
    "    # require 'brown sugar' collocation\n",
    "    if \"brown sugar\" in s:\n",
    "        assert \"brown sugar\" in joined, (s, seg)\n",
    "print(\"Span tests passed.\")\n",
    "\n",
    "# Peek output head\n",
    "df_check = pd.read_parquet(OUTPUT_PATH).head(10)\n",
    "df_check[[NER_COL, \"NER_clean\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
