{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "309cd8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8610882d",
   "metadata": {},
   "source": [
    "# Ingredient Normalization (Data-Driven, PMI-Based)\n",
    "This notebook implements a **fully data-driven pipeline** for large-scale ingredient normalization.\n",
    "\n",
    "### ðŸ”§ **Pipeline Summary**\n",
    "| Stage | Component | Purpose | Output |\n",
    "|--------|------------|----------|---------|\n",
    "| **1** | Config & Setup | Define file paths, thresholds, constants | Paths + Params |\n",
    "| **2** | **StatsNormalizer** | Build a PMI-based vocabulary of canonical ingredient collocations (uni/bi/tri-grams) | `ingredient_vocab_stats.json` |\n",
    "| **3** | **Spell/Fuzzy Mapping (Threaded)** | Fast correction of raw ingredient variants using spelling and fuzzy matching | `ner_spell_map.jsonl` |\n",
    "| **4** | **Cosine Similarity Deduplication** | Identify semantically similar ingredient terms using sentence embeddings (MiniLM) | `cosine_dedupe_map.jsonl` |\n",
    "| **5** | **Apply Dedup Map** | Replace duplicates in Parquet and write deduped file | `recipes_data_clean_spell_dedup.parquet` |\n",
    "| **6** | **Summary** | Print stats, vocab size, and output summary | Console output |\n",
    "\n",
    "### ðŸ“¦ **Outputs Generated**\n",
    "- `recipes_data_clean.parquet` â€” PMI-cleaned dataset\n",
    "- `recipes_data_clean_spell.parquet` â€” cleaned + spell-mapped dataset\n",
    "- `recipes_data_clean_spell_dedup.parquet` â€” final cosine-deduped dataset\n",
    "- `ingredient_vocab_stats.json` â€” canonical vocabulary statistics\n",
    "- `cosine_dedupe_map.jsonl` â€” mapping of similar terms\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Runtime Tips**\n",
    "- Use smaller `CHUNK_SIZE` (e.g., 100,000) if memory-limited.\n",
    "- Use `SentenceTransformer('all-MiniLM-L6-v2', device='cuda')` for GPU acceleration.\n",
    "- Reuse saved vocab and dedup maps across re-runs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline Visualization**\n",
    "\n",
    "<img src=\"./ingredient_row_sequence.png\" alt=\"Ingredient normalization pipeline\" width=\"850\"/>`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## âš™ï¸ Code Sections Below\n",
    "The notebook proceeds through these main stages:\n",
    "1. Config & Setup\n",
    "2. StatsNormalizer (PMI Collocations)\n",
    "3. Spell/Fuzzy Map (Threaded)\n",
    "4. Cosine Similarity Deduplication\n",
    "5. Apply Dedup Map to Parquet\n",
    "6. Final Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "495281e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional installs (run once if missing)\n",
    "# !pip install pyarrow\n",
    "\n",
    "# # Update requirements.txt \n",
    "# !pip freeze > ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f6f86",
   "metadata": {},
   "source": [
    "## Config and global imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea1fdc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, gc, json, math, re, ast\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Iterable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_DATA_PATH = Path(\"../data/wilmerarltstrmberg_data.csv\")\n",
    "OUTPUT_PATH = Path(\"../data/recipes_data_clean.parquet\")\n",
    "OUTPUT_PATH_SPELL = Path(\"../data/recipes_data_clean_spell.parquet\")\n",
    "VOCAB_JSON = Path(\"../data/ingredient_vocab_stats.json\")\n",
    "NER_COL = \"NER\"\n",
    "CHUNK_SIZE = 200_000\n",
    "\n",
    "MIN_UNIGRAM = 50\n",
    "MIN_BIGRAM = 50\n",
    "MIN_TRIGRAM = 30\n",
    "PMI_BIGRAM = 3.0\n",
    "PMI_TRIGRAM = 2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3eba8",
   "metadata": {},
   "source": [
    "## 2. StatsNormalizer: PMI-based collocation model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2fd586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-featured StatsNormalizer (drop-in replacement)\n",
    "\n",
    "import ast, gc, json, math, re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "class StatsNormalizer:\n",
    "    \"\"\"\n",
    "    PMI-based n-gram normalizer with:\n",
    "      - Streaming counts (uni/bi/tri/4-grams)\n",
    "      - PMI + child-share + entropy gates to build a canonical vocabulary\n",
    "      - Greedy 4â†’3â†’2â†’1 segmentation with optional span-preserving fuzzy snap\n",
    "      - CSVâ†’Parquet writer (Arrow-friendly, list<string> for NER_clean)\n",
    "      - Save/load of learned vocabulary\n",
    "\n",
    "    Public API:\n",
    "      - ingest_csv / ingest_df\n",
    "      - build_vocab\n",
    "      - segment_item\n",
    "      - transform_df\n",
    "      - transform_csv_to_parquet\n",
    "      - save_vocab / load_vocab\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------- Utilities ----------------\n",
    "    @staticmethod\n",
    "    def _tok(s):\n",
    "        return re.findall(r\"[a-z']+\", str(s).lower())\n",
    "\n",
    "    @staticmethod\n",
    "    def _ngrams(tokens, n):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            yield tuple(tokens[i:i+n])\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_ner_entry(entry):\n",
    "        # Accepts list-like strings \"['a','b']\" or JSON-ish strings, or comma-separated.\n",
    "        if entry is None or (isinstance(entry, float) and pd.isna(entry)):\n",
    "            return []\n",
    "        s = str(entry).strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        # try Python literal list\n",
    "        try:\n",
    "            parsed = ast.literal_eval(s)\n",
    "            if isinstance(parsed, list):\n",
    "                return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "        # comma-separated fallback\n",
    "        return [x.strip() for x in s.split(\",\") if x.strip()]\n",
    "\n",
    "    # ---------------- Init ----------------\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_ngram=4,\n",
    "        # canon thresholds\n",
    "        min_unigram=50, min_bigram=50, min_trigram=30, min_fourgram=20,\n",
    "        pmi_bigram=3.0, pmi_trigram=2.0, pmi_fourgram=2.0,\n",
    "        # trigram gates\n",
    "        min_child_share=0.12, max_right_entropy=1.0,\n",
    "        # 4-gram gates (trigram-head branching)\n",
    "        min_child_share4=0.05, max_right_entropy3=1.3,\n",
    "        # dynamic fallbacks (optional safety nets; can be tuned/relaxed)\n",
    "        pmi_bigram_fallback=2.6,  min_bigram_fallback=20,\n",
    "        pmi_trigram_fallback=2.2, min_trigram_fallback=12,\n",
    "        min_child_share_fallback=0.06, max_right_entropy_fallback=1.4,\n",
    "        pmi_fourgram_fallback=1.8, min_fourgram_fallback=10,\n",
    "        min_child_share4_fallback=0.04, max_right_entropy3_fallback=1.5,\n",
    "        # snap settings\n",
    "        snap_score_cutoff=92, snap_near_perfect=96\n",
    "    ):\n",
    "        # config\n",
    "        self.max_ngram = int(max_ngram)\n",
    "\n",
    "        self.min_unigram, self.min_bigram, self.min_trigram, self.min_fourgram = (\n",
    "            min_unigram, min_bigram, min_trigram, min_fourgram\n",
    "        )\n",
    "        self.pmi_bigram, self.pmi_trigram, self.pmi_fourgram = (\n",
    "            pmi_bigram, pmi_trigram, pmi_fourgram\n",
    "        )\n",
    "\n",
    "        self.min_child_share, self.max_right_entropy = min_child_share, max_right_entropy\n",
    "        self.min_child_share4, self.max_right_entropy3 = min_child_share4, max_right_entropy3\n",
    "\n",
    "        self.pmi_bigram_fallback, self.min_bigram_fallback = pmi_bigram_fallback, min_bigram_fallback\n",
    "        self.pmi_trigram_fallback, self.min_trigram_fallback = pmi_trigram_fallback, min_trigram_fallback\n",
    "        self.min_child_share_fallback, self.max_right_entropy_fallback = (\n",
    "            min_child_share_fallback, max_right_entropy_fallback\n",
    "        )\n",
    "        self.pmi_fourgram_fallback, self.min_fourgram_fallback = pmi_fourgram_fallback, min_fourgram_fallback\n",
    "        self.min_child_share4_fallback, self.max_right_entropy3_fallback = (\n",
    "            min_child_share4_fallback, max_right_entropy3_fallback\n",
    "        )\n",
    "\n",
    "        self.snap_score_cutoff, self.snap_near_perfect = snap_score_cutoff, snap_near_perfect\n",
    "\n",
    "        # counts\n",
    "        self.token_total = 0\n",
    "        self.c1, self.c2, self.c3, self.c4 = Counter(), Counter(), Counter(), Counter()\n",
    "\n",
    "        # follower distributions for entropy\n",
    "        self._followers  = defaultdict(Counter)  # (a,b)->c\n",
    "        self._followers3 = defaultdict(Counter)  # (a,b,c)->d\n",
    "\n",
    "        # canon & caches\n",
    "        self.canon = set()\n",
    "        self._canon_ready = False\n",
    "        self._canon_phrases = None\n",
    "        self._canon_buckets = None\n",
    "\n",
    "    # ---------------- Ingestion ----------------\n",
    "    def ingest_df(self, df, ner_col=\"NER\"):\n",
    "        for entry in df[ner_col]:\n",
    "            for item in self._parse_ner_entry(entry):\n",
    "                toks = self._tok(item)\n",
    "                if not toks:\n",
    "                    continue\n",
    "\n",
    "                self.c1.update(toks)\n",
    "                self.token_total += len(toks)\n",
    "\n",
    "                if self.max_ngram >= 2 and len(toks) >= 2:\n",
    "                    self.c2.update(self._ngrams(toks, 2))\n",
    "\n",
    "                if self.max_ngram >= 3 and len(toks) >= 3:\n",
    "                    for i in range(len(toks) - 2):\n",
    "                        a, b, c = toks[i], toks[i+1], toks[i+2]\n",
    "                        self.c3[(a, b, c)] += 1\n",
    "                        self._followers[(a, b)][c] += 1\n",
    "\n",
    "                if self.max_ngram >= 4 and len(toks) >= 4:\n",
    "                    for i in range(len(toks) - 3):\n",
    "                        a, b, c, d = toks[i], toks[i+1], toks[i+2], toks[i+3]\n",
    "                        self.c4[(a, b, c, d)] += 1\n",
    "                        self._followers3[(a, b, c)][d] += 1\n",
    "\n",
    "    def ingest_csv(self, csv_path, ner_col=\"NER\", chunksize=200_000):\n",
    "        for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
    "            self.ingest_df(chunk, ner_col=ner_col)\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "\n",
    "    # ---------------- Stats (PMI/Entropy/Share) ----------------\n",
    "    def _right_entropy(self, ab):\n",
    "        foll = self._followers.get(ab)\n",
    "        if not foll:\n",
    "            return 0.0\n",
    "        tot = sum(foll.values())\n",
    "        if tot == 0:\n",
    "            return 0.0\n",
    "        H = 0.0\n",
    "        for v in foll.values():\n",
    "            p = v / tot\n",
    "            H -= p * math.log(p + 1e-12)\n",
    "        return H\n",
    "\n",
    "    def _right_entropy3(self, abc):\n",
    "        foll = self._followers3.get(abc)\n",
    "        if not foll:\n",
    "            return 0.0\n",
    "        tot = sum(foll.values())\n",
    "        if tot == 0:\n",
    "            return 0.0\n",
    "        H = 0.0\n",
    "        for v in foll.values():\n",
    "            p = v / tot\n",
    "            H -= p * math.log(p + 1e-12)\n",
    "        return H\n",
    "\n",
    "    def _child_share(self, abc):\n",
    "        cabc = self.c3.get(abc, 0)\n",
    "        cab  = self.c2.get(abc[:2], 0)\n",
    "        return (cabc / cab) if cab else 0.0\n",
    "\n",
    "    def _child_share4(self, abcd):\n",
    "        cabcd = self.c4.get(abcd, 0)\n",
    "        cabc  = self.c3.get(abcd[:3], 0)\n",
    "        return (cabcd / cabc) if cabc else 0.0\n",
    "\n",
    "    def _pmi_bigram(self, ab):\n",
    "        a, b = ab\n",
    "        cab = self.c2.get(ab, 0)\n",
    "        if cab == 0 or self.token_total == 0:\n",
    "            return -1e9\n",
    "        pa = self.c1.get(a, 0) / self.token_total\n",
    "        pb = self.c1.get(b, 0) / self.token_total\n",
    "        pab = cab / self.token_total\n",
    "        return math.log((pab / (pa * pb)) + 1e-12)\n",
    "\n",
    "    def _pmi_trigram(self, abc):\n",
    "        a, b, c = abc\n",
    "        return (self._pmi_bigram((a, b)) + self._pmi_bigram((b, c))) / 2.0\n",
    "\n",
    "    def _pmi_fourgram(self, abcd):\n",
    "        a, b, c, d = abcd\n",
    "        return (\n",
    "            self._pmi_bigram((a, b))\n",
    "            + self._pmi_bigram((b, c))\n",
    "            + self._pmi_bigram((c, d))\n",
    "        ) / 3.0\n",
    "\n",
    "    # ---------------- Build Canon ----------------\n",
    "    def build_vocab(self):\n",
    "        self.canon.clear()\n",
    "\n",
    "        # unigrams\n",
    "        for w, c in self.c1.items():\n",
    "            if c >= self.min_unigram:\n",
    "                self.canon.add((w,))\n",
    "\n",
    "        # bigrams\n",
    "        for ab, c in self.c2.items():\n",
    "            if c >= self.min_bigram and self._pmi_bigram(ab) >= self.pmi_bigram:\n",
    "                self.canon.add(ab)\n",
    "\n",
    "        # trigrams\n",
    "        for abc, c in self.c3.items():\n",
    "            if c < self.min_trigram:\n",
    "                continue\n",
    "            if self._pmi_trigram(abc) < self.pmi_trigram:\n",
    "                continue\n",
    "            if self._child_share(abc) < self.min_child_share:\n",
    "                continue\n",
    "            if self._right_entropy(abc[:2]) > self.max_right_entropy:\n",
    "                continue\n",
    "            self.canon.add(abc)\n",
    "\n",
    "        # four-grams\n",
    "        for abcd, c in self.c4.items():\n",
    "            if c < self.min_fourgram:\n",
    "                continue\n",
    "            if self._pmi_fourgram(abcd) < self.pmi_fourgram:\n",
    "                continue\n",
    "            if self._child_share4(abcd) < self.min_child_share4:\n",
    "                continue\n",
    "            if self._right_entropy3(abcd[:3]) > self.max_right_entropy3:\n",
    "                continue\n",
    "            self.canon.add(abcd)\n",
    "\n",
    "        self._canon_ready = True\n",
    "        self._canon_phrases = None\n",
    "        self._canon_buckets = None\n",
    "\n",
    "    # ---------------- Snap helpers ----------------\n",
    "    def _canon_bucket_init(self):\n",
    "        self._canon_phrases = [\" \".join(p) for p in self.canon]\n",
    "        buckets = {}\n",
    "        for ph in self._canon_phrases:\n",
    "            ft = ph.split()[0] if ph else \"\"\n",
    "            buckets.setdefault(ft, []).append(ph)\n",
    "        self._canon_buckets = buckets\n",
    "\n",
    "    def _snap_span(self, tokens, i, n):\n",
    "        \"\"\"\n",
    "        Try snapping tokens[i:i+n] to a canon phrase with same length (or near-perfect match).\n",
    "        Uses rapidfuzz if available; otherwise returns None.\n",
    "        \"\"\"\n",
    "        if i + n > len(tokens): return None\n",
    "        if self._canon_phrases is None or self._canon_buckets is None:\n",
    "            self._canon_bucket_init()\n",
    "        try:\n",
    "            from rapidfuzz import process, fuzz\n",
    "        except Exception:\n",
    "            return None\n",
    "        span = \" \".join(tokens[i:i+n])\n",
    "        bucket = self._canon_buckets.get(tokens[i], self._canon_phrases)\n",
    "        match = process.extractOne(span, bucket, scorer=fuzz.WRatio, score_cutoff=self.snap_score_cutoff)\n",
    "        if not match:\n",
    "            return None\n",
    "        cand, score = match[0], match[1]\n",
    "        # Do not shrink unless near-perfect\n",
    "        if len(cand.split()) < n and score < self.snap_near_perfect:\n",
    "            return None\n",
    "        if len(cand.split()) == n or score >= self.snap_near_perfect:\n",
    "            return cand.split(), n\n",
    "        return None\n",
    "\n",
    "    # ---------------- Segmentation ----------------\n",
    "    def post_compact(self, tokens, cos_map=None, max_window=3):\n",
    "        \"\"\"\n",
    "        Try to re-glue adjacent tokens if the combined phrase is strong by:\n",
    "        - exact canon membership, or\n",
    "        - bigram/trigram fallback thresholds, or\n",
    "        - cosine-dedupe map equivalence.\n",
    "        \"\"\"\n",
    "        if not tokens:\n",
    "            return tokens\n",
    "        i, out = 0, []\n",
    "        N = len(tokens)\n",
    "        cos_map = cos_map or {}\n",
    "\n",
    "        def _ok_bigram(ab):\n",
    "            ab_t = tuple(StatsNormalizer._tok(ab))\n",
    "            if len(ab_t) != 2: return False\n",
    "            return (self.c2.get(ab_t, 0) >= self.min_bigram_fallback and\n",
    "                    self._pmi_bigram(ab_t) >= self.pmi_bigram_fallback)\n",
    "\n",
    "        def _ok_trigram(abc):\n",
    "            abc_t = tuple(StatsNormalizer._tok(abc))\n",
    "            if len(abc_t) != 3: return False\n",
    "            return (self.c3.get(abc_t, 0) >= self.min_trigram_fallback and\n",
    "                    self._pmi_trigram(abc_t) >= self.pmi_trigram_fallback)\n",
    "\n",
    "        while i < N:\n",
    "            # try 3-gram glue\n",
    "            if i+2 < N:\n",
    "                tri = f\"{tokens[i]} {tokens[i+1]} {tokens[i+2]}\"\n",
    "                tri_t = tuple(StatsNormalizer._tok(tri))\n",
    "                if tri_t in self.canon or _ok_trigram(tri):\n",
    "                    out.append(\" \".join(tri_t)); i += 3; continue\n",
    "            # try 2-gram glue\n",
    "            if i+1 < N:\n",
    "                bi = f\"{tokens[i]} {tokens[i+1]}\"\n",
    "                bi_t = tuple(StatsNormalizer._tok(bi))\n",
    "                if (bi_t in self.canon or _ok_bigram(bi) or\n",
    "                    cos_map.get(tokens[i]) == cos_map.get(tokens[i+1]) and cos_map.get(tokens[i]) is not None):\n",
    "                    out.append(\" \".join(bi_t)); i += 2; continue\n",
    "            out.append(tokens[i]); i += 1\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def _longest_match(self, toks, i):\n",
    "        if not self._canon_ready:\n",
    "            raise RuntimeError(\"build_vocab() first\")\n",
    "\n",
    "        # exact canon first\n",
    "        if self.max_ngram >= 4 and i+3 < len(toks) and tuple(toks[i:i+4]) in self.canon:\n",
    "            return tuple(toks[i:i+4]), 4\n",
    "        if self.max_ngram >= 3 and i+2 < len(toks) and tuple(toks[i:i+3]) in self.canon:\n",
    "            return tuple(toks[i:i+3]), 3\n",
    "        if self.max_ngram >= 2 and i+1 < len(toks) and tuple(toks[i:i+2]) in self.canon:\n",
    "            return tuple(toks[i:i+2]), 2\n",
    "        if (toks[i],) in self.canon:\n",
    "            return (toks[i],), 1\n",
    "\n",
    "        # snap to canon (span-preserving)\n",
    "        if self.max_ngram >= 3:\n",
    "            snapped = self._snap_span(toks, i, 3)\n",
    "            if snapped:\n",
    "                return tuple(snapped[0]), snapped[1]\n",
    "        if self.max_ngram >= 2:\n",
    "            snapped = self._snap_span(toks, i, 2)\n",
    "            if snapped:\n",
    "                return tuple(snapped[0]), snapped[1]\n",
    "\n",
    "        # dynamic fallbacks based on stats (optional but helpful)\n",
    "        if self.max_ngram >= 4 and i+3 < len(toks):\n",
    "            abcd = (toks[i], toks[i+1], toks[i+2], toks[i+3])\n",
    "            cabcd = self.c4.get(abcd, 0)\n",
    "            if cabcd >= self.min_fourgram_fallback:\n",
    "                if (self._pmi_fourgram(abcd) >= self.pmi_fourgram_fallback and\n",
    "                    self._child_share4(abcd)  >= self.min_child_share4_fallback and\n",
    "                    self._right_entropy3(abcd[:3]) <= self.max_right_entropy3_fallback):\n",
    "                    return abcd, 4\n",
    "\n",
    "        if self.max_ngram >= 3 and i+2 < len(toks):\n",
    "            abc = (toks[i], toks[i+1], toks[i+2])\n",
    "            cabc = self.c3.get(abc, 0)\n",
    "            if cabc >= self.min_trigram_fallback:\n",
    "                if (self._pmi_trigram(abc) >= self.pmi_trigram_fallback and\n",
    "                    self._child_share(abc)  >= self.min_child_share_fallback and\n",
    "                    self._right_entropy(abc[:2]) <= self.max_right_entropy_fallback):\n",
    "                    return abc, 3\n",
    "\n",
    "        if self.max_ngram >= 2 and i+1 < len(toks):\n",
    "            ab = (toks[i], toks[i+1])\n",
    "            cab = self.c2.get(ab, 0)\n",
    "            if cab >= self.min_bigram_fallback and self._pmi_bigram(ab) >= self.pmi_bigram_fallback:\n",
    "                return ab, 2\n",
    "\n",
    "        # fallback unigram\n",
    "        return (toks[i],), 1\n",
    "\n",
    "    def segment_item(self, text):\n",
    "        t = self._tok(text)\n",
    "        out, i = [], 0\n",
    "        while i < len(t):\n",
    "            phrase, k = self._longest_match(t, i)\n",
    "            out.append(\" \".join(phrase))\n",
    "            i += k\n",
    "\n",
    "        # de-dup while preserving order\n",
    "        seen, clean = set(), []\n",
    "        for x in out:\n",
    "            if x not in seen:\n",
    "                clean.append(x); seen.add(x)\n",
    "\n",
    "        clean = self.post_compact(clean)  # or self.post_compact(clean, cos_map=cosine_map)\n",
    "        \n",
    "        # drop immediate repetition of previous tail\n",
    "        pruned = []\n",
    "        for x in clean:\n",
    "            if pruned and x == pruned[-1].split()[-1]:\n",
    "                continue\n",
    "            pruned.append(x)\n",
    "        return pruned\n",
    "\n",
    "    # ---------------- DataFrame & IO ----------------\n",
    "    def transform_df(self, df, ner_col=\"NER\", out_col=\"NER_clean\", dedupe_row=False):\n",
    "        results = []\n",
    "        for v in df[ner_col]:\n",
    "            segs = [seg for item in self._parse_ner_entry(v) for seg in self.segment_item(item)]\n",
    "            if dedupe_row:\n",
    "                seen, uniq = set(), []\n",
    "                for s in segs:\n",
    "                    if s not in seen:\n",
    "                        uniq.append(s); seen.add(s)\n",
    "                results.append(uniq)\n",
    "            else:\n",
    "                results.append(segs)\n",
    "        df[out_col] = results\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _sanitize_for_arrow(df, list_col=\"NER_clean\"):\n",
    "        # Make Arrow-friendly: list<string> for list_col; stringify complex objects elsewhere.\n",
    "        df = df.copy()\n",
    "\n",
    "        def _to_list_of_str(x):\n",
    "            if isinstance(x, (list, tuple, np.ndarray)):\n",
    "                return [str(y) for y in x]\n",
    "            if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "                return []\n",
    "            # best-effort parse of serialized lists\n",
    "            try:\n",
    "                parsed = ast.literal_eval(str(x))\n",
    "                if isinstance(parsed, (list, tuple, np.ndarray)):\n",
    "                    return [str(y) for y in parsed]\n",
    "            except Exception:\n",
    "                pass\n",
    "            return [str(x)]\n",
    "\n",
    "        if list_col in df.columns:\n",
    "            df[list_col] = df[list_col].apply(_to_list_of_str)\n",
    "\n",
    "        for col in df.columns:\n",
    "            if col == list_col:\n",
    "                continue\n",
    "            s = df[col]\n",
    "            if s.dtype == object:\n",
    "                def _to_scalar_str(v):\n",
    "                    if isinstance(v, (list, tuple, dict, set, np.ndarray)):\n",
    "                        return json.dumps(list(v) if isinstance(v, np.ndarray) else v, ensure_ascii=False)\n",
    "                    return \"\" if v is None or (isinstance(v, float) and pd.isna(v)) else str(v)\n",
    "                df[col] = s.map(_to_scalar_str)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform_csv_to_parquet(self, csv_path, out_path, ner_col=\"NER\", chunksize=200_000):\n",
    "        \"\"\"\n",
    "        Stream a CSV, segment NER items using the learned canon/fallbacks,\n",
    "        and write Parquet with NER_clean as list<string>.\n",
    "        \"\"\"\n",
    "        writer = None\n",
    "        for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
    "            # segment\n",
    "            chunk = self.transform_df(chunk, ner_col=ner_col, out_col=\"NER_clean\")\n",
    "            # sanitize for Arrow\n",
    "            chunk = self._sanitize_for_arrow(chunk, list_col=\"NER_clean\")\n",
    "\n",
    "            # to Arrow table, force list<string> for NER_clean\n",
    "            table = pa.Table.from_pandas(chunk, preserve_index=False).replace_schema_metadata(None)\n",
    "            fields = []\n",
    "            for f in table.schema:\n",
    "                if f.name == \"NER_clean\" and not pa.types.is_list(f.type):\n",
    "                    fields.append(pa.field(\"NER_clean\", pa.list_(pa.string())))\n",
    "                else:\n",
    "                    fields.append(f)\n",
    "            target_schema = pa.schema(fields)\n",
    "            try:\n",
    "                table = table.cast(target_schema, safe=False)\n",
    "            except Exception:\n",
    "                arrays = [pa.array(arr, type=pa.list_(pa.string())) for arr in table.column(\"NER_clean\").to_pylist()]\n",
    "                table = table.set_column(table.schema.get_field_index(\"NER_clean\"),\n",
    "                                         \"NER_clean\", pa.chunked_array(arrays))\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(out_path, target_schema, compression=\"zstd\")\n",
    "            writer.write_table(table)\n",
    "\n",
    "            del chunk, table\n",
    "            gc.collect()\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "    # ---------------- Persistence ----------------\n",
    "    def save_vocab(self, path):\n",
    "        data = {\n",
    "            \"token_total\": int(self.token_total),\n",
    "            \"canon\": [\" \".join(p) for p in sorted(self.canon)]\n",
    "        }\n",
    "        Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load_vocab(cls, path):\n",
    "        data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "        obj = cls()\n",
    "        obj.canon = set(tuple(p.split()) for p in data[\"canon\"])\n",
    "        obj._canon_ready = True\n",
    "        obj._canon_phrases = None\n",
    "        obj._canon_buckets = None\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580fb10e",
   "metadata": {},
   "source": [
    "## 3. Spell + Fuzzy Mapping (threaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebc60610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from rapidfuzz import process, fuzz\n",
    "from spellchecker import SpellChecker\n",
    "import pandas as pd, json, os, threading\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_spell_map(canon_phrases, csv_path, ner_col=\"NER\", out_path=Path(\"../data/ner_spell_map.jsonl\"),\n",
    "                    chunksize=200_000, batch_size=2000, max_workers=None,\n",
    "                    score_cutoff=92, near_perfect=96):\n",
    "    \"\"\"\n",
    "    Robust builder for a global spell/fuzzy mapping:\n",
    "      - Streams unique raw items from csv_path[ner_col]\n",
    "      - Token-level spell correction, then phrase snap to canon\n",
    "      - Span-preserving: only shrink/expand if near-perfect\n",
    "    Writes JSONL with {\"raw\": ..., \"fixed\": ...} per line.\n",
    "    \"\"\"\n",
    "    if not canon_phrases:\n",
    "        raise ValueError(\"build_spell_map: canon_phrases is empty. Did you run build_vocab()?\")\n",
    "\n",
    "    # Build first-token buckets for faster fuzzy\n",
    "    buckets = {}\n",
    "    for p in canon_phrases:\n",
    "        toks = str(p).split()\n",
    "        if not toks:\n",
    "            continue\n",
    "        buckets.setdefault(toks[0], []).append(p)\n",
    "    canon_set = set(canon_phrases)\n",
    "\n",
    "    # Collect unique raw items\n",
    "    seen = set()\n",
    "    for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
    "        col = chunk.get(ner_col)\n",
    "        if col is None:\n",
    "            continue\n",
    "        for entry in col:\n",
    "            # Parse as list-like if possible, else treat as comma-separated\n",
    "            items = []\n",
    "            if entry is None:\n",
    "                pass\n",
    "            else:\n",
    "                s = str(entry).strip()\n",
    "                if s:\n",
    "                    try:\n",
    "                        parsed = ast.literal_eval(s)\n",
    "                        if isinstance(parsed, (list, tuple)):\n",
    "                            items = [str(x) for x in parsed if str(x).strip()]\n",
    "                    except Exception:\n",
    "                        items = [x.strip() for x in s.split(\",\") if x.strip()]\n",
    "            for it in items:\n",
    "                if it: seen.add(str(it))\n",
    "        del chunk\n",
    "\n",
    "    items = sorted(seen)\n",
    "\n",
    "    # Thread-local spellchecker to avoid rebuild per call\n",
    "    _tls = threading.local()\n",
    "    vocab_tokens = [t for ph in canon_phrases for t in str(ph).split()]\n",
    "\n",
    "    def _get_spell():\n",
    "        sc = getattr(_tls, \"sc\", None)\n",
    "        if sc is None:\n",
    "            sc = SpellChecker(distance=2)\n",
    "            if vocab_tokens:\n",
    "                sc.word_frequency.load_words(vocab_tokens)\n",
    "            _tls.sc = sc\n",
    "        return sc\n",
    "\n",
    "    # Helper: safer tok\n",
    "    _WORD_RE = re.compile(r\"[a-z']+\")\n",
    "    def _tok(text):\n",
    "        return _WORD_RE.findall(str(text).lower())\n",
    "\n",
    "    def _fix_batch(batch):\n",
    "        sc = _get_spell()\n",
    "        out = []\n",
    "        for raw in batch:\n",
    "            raw_str = str(raw)\n",
    "            toks = _tok(raw_str)\n",
    "\n",
    "            # If tokenization yields nothing, keep original\n",
    "            if not toks:\n",
    "                out.append((raw_str, raw_str))\n",
    "                continue\n",
    "\n",
    "            # Word-level spell correct (keep span)\n",
    "            toks2 = [sc.correction(t) or t for t in toks]\n",
    "            if not toks2:\n",
    "                out.append((raw_str, raw_str))\n",
    "                continue\n",
    "\n",
    "            corrected = \" \".join(toks2)\n",
    "\n",
    "            # Early exit if exact canon\n",
    "            if corrected in canon_set:\n",
    "                out.append((raw_str, corrected))\n",
    "                continue\n",
    "\n",
    "            # Choose candidate pool (fallback to full canon if no first token)\n",
    "            if toks2:\n",
    "                choices = buckets.get(toks2[0], canon_phrases)\n",
    "            else:\n",
    "                choices = canon_phrases\n",
    "\n",
    "            # Rapidfuzz can return tuple-like (choice, score, idx) or object\n",
    "            match = process.extractOne(corrected, choices, scorer=fuzz.WRatio, score_cutoff=score_cutoff)\n",
    "\n",
    "            if match:\n",
    "                cand = match[0]\n",
    "                score = match[1] if len(match) > 1 else 100\n",
    "                len_ok = (len(cand.split()) == len(toks2))\n",
    "                if len_ok or score >= near_perfect:\n",
    "                    out.append((raw_str, cand))\n",
    "                else:\n",
    "                    out.append((raw_str, corrected))\n",
    "            else:\n",
    "                out.append((raw_str, corrected))\n",
    "        return out\n",
    "\n",
    "    # Batch and run threaded\n",
    "    def _chunks(lst, n):\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i+n]\n",
    "\n",
    "    batches = list(_chunks(items, batch_size))\n",
    "    if max_workers is None:\n",
    "        cpu = (os.cpu_count() or 2)\n",
    "        max_workers = min(16, 2 * cpu)\n",
    "\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex, open(out_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        for res in tqdm(ex.map(_fix_batch, batches), total=len(batches), desc=\"Spell/Fuzzy map\"):\n",
    "            for raw, fixed in res:\n",
    "                out.write(json.dumps({\"raw\": raw, \"fixed\": fixed}) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote spell map: {out_path}\")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def frequent_surface_phrases(csv_path, ner_col=\"NER\", chunksize=200_000, min_phrase=40):\n",
    "    \"\"\"Counts full NER items as phrases; returns a set of phrases meeting min_phrase.\"\"\"\n",
    "    import pandas as pd, ast, gc\n",
    "    seen = Counter()\n",
    "    for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
    "        col = chunk[ner_col].astype(str)\n",
    "        for s in col:\n",
    "            s = s.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            # Accept list-like or raw string\n",
    "            try:\n",
    "                v = ast.literal_eval(s)\n",
    "                if isinstance(v, (list, tuple)):\n",
    "                    for it in v:\n",
    "                        it = str(it).strip().lower()\n",
    "                        if it:\n",
    "                            seen[it] += 1\n",
    "                else:\n",
    "                    seen[str(v).strip().lower()] += 1\n",
    "            except Exception:\n",
    "                seen[s.lower()] += 1\n",
    "        del chunk; gc.collect()\n",
    "    return {p for p,c in seen.items() if c >= min_phrase}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd4069",
   "metadata": {},
   "source": [
    "## 4. Cosine Similarity Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be611a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "def vocab_from_parquet(path, col=\"NER_clean\"):\n",
    "    import numpy as np, ast\n",
    "    pf = pq.ParquetFile(path)\n",
    "    vocab = Counter()\n",
    "    for rg in range(pf.num_row_groups):\n",
    "        df = pf.read_row_group(rg, columns=[col]).to_pandas()\n",
    "        for x in df[col]:\n",
    "            if isinstance(x, (list, tuple, np.ndarray)):\n",
    "                vocab.update(str(t).strip() for t in x if str(t).strip())\n",
    "            elif isinstance(x, str) and x.startswith(\"[\"):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(x)\n",
    "                    if isinstance(parsed, (list, tuple, np.ndarray)):\n",
    "                        vocab.update(str(t).strip() for t in parsed if str(t).strip())\n",
    "                except Exception:\n",
    "                    pass\n",
    "        gc.collect()\n",
    "    return vocab\n",
    "\n",
    "def cosine_dedupe(vocab, threshold=0.88, topk=20, out_path=Path(\"../data/cosine_dedupe_map.jsonl\")):\n",
    "    phrases = [p for p, c in vocab.items() if c >= 1]\n",
    "    if len(phrases) == 0:\n",
    "        raise RuntimeError(\"No phrases in vocab for cosine deduplication.\")\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    emb = model.encode([p.lower() for p in phrases], convert_to_tensor=True, show_progress_bar=True)\n",
    "    emb = torch.nn.functional.normalize(emb, p=2, dim=-1)\n",
    "    emb_np = emb.detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    import faiss\n",
    "    index = faiss.IndexFlatIP(emb_np.shape[1])\n",
    "    index.add(emb_np)\n",
    "    sims, idxs = index.search(emb_np, min(topk, len(phrases)))\n",
    "\n",
    "    parent = list(range(len(phrases)))\n",
    "    def find(x):\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "    def union(x, y):\n",
    "        rx, ry = find(x), find(y)\n",
    "        if rx != ry: parent[ry] = rx\n",
    "\n",
    "    for i in range(len(phrases)):\n",
    "        for j in range(1, sims.shape[1]):\n",
    "            if sims[i, j] >= threshold:\n",
    "                union(i, idxs[i, j])\n",
    "\n",
    "    from collections import defaultdict\n",
    "    clusters = defaultdict(list)\n",
    "    for i in range(len(phrases)):\n",
    "        clusters[find(i)].append(i)\n",
    "\n",
    "    id_to_canon = {}\n",
    "    freqs = np.array([vocab[p] for p in phrases])\n",
    "    for root, ids in clusters.items():\n",
    "        c = sorted(ids, key=lambda k: (-freqs[k], len(phrases[k]), phrases[k]))[0]\n",
    "        for i in ids: id_to_canon[i] = c\n",
    "\n",
    "    cosine_map = {phrases[i]: phrases[id_to_canon[i]] for i in range(len(phrases))}\n",
    "    changed = sum(1 for k, v in cosine_map.items() if k != v)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for k, v in cosine_map.items():\n",
    "            if k != v: f.write(json.dumps({\"from\": k, \"to\": v}) + \"\\n\")\n",
    "    print(f\"Cosine dedupe map written ({changed} changes) â†’ {out_path}\")\n",
    "    return cosine_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6df41",
   "metadata": {},
   "source": [
    "## 5. Apply cosine map to Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15790605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cosine_map_to_parquet(in_path, out_path, mapping, list_col=\"NER_clean\"):\n",
    "    pf = pq.ParquetFile(in_path)\n",
    "    writer = None\n",
    "    for rg in range(pf.num_row_groups):\n",
    "        tbl = pf.read_row_group(rg)\n",
    "        df = tbl.to_pandas()\n",
    "        if list_col in df.columns:\n",
    "            df[list_col] = df[list_col].apply(\n",
    "                lambda lst: [mapping.get(str(x), str(x)) for x in (lst if isinstance(lst, (list, tuple, np.ndarray)) else [])]\n",
    "            )\n",
    "        table = pa.Table.from_pandas(df, preserve_index=False).replace_schema_metadata(None)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(out_path, table.schema, compression=\"zstd\")\n",
    "        writer.write_table(table)\n",
    "        del df, table; gc.collect()\n",
    "    if writer: writer.close()\n",
    "    print(f\"âœ… Wrote cosine-deduped file â†’ {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e80f9",
   "metadata": {},
   "source": [
    "## 6. Final summary printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebeef0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pipeline(vocab, data_path=DATA_PATH):\n",
    "    print(f\"\"\"\n",
    "âœ… Ingredient Normalization Pipeline Complete\n",
    "--------------------------------------------\n",
    "Input CSV:     {data_path}\n",
    "Output (PMI):  {OUTPUT_PATH}\n",
    "Output (Spell):{OUTPUT_PATH_SPELL}\n",
    "Cosine Map:    ../data/cosine_dedupe_map.jsonl\n",
    "\n",
    "Vocabulary size: {len(vocab)} unique terms\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d6834",
   "metadata": {},
   "source": [
    "## 7.  RUN FULL PIPELINE (End-to-End Controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a957097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ingredient Normalization Pipeline...\n",
      "DEV MODE: sampling 10000 rows from raw data for faster iteration.\n",
      "Sample data written to: ..\\data\\sample_data.csv\n",
      "Data source: ..\\data\\sample_data.csv\n",
      "Output parquet will be written to: ..\\data\\recipes_data_clean_spell.parquet\n",
      "Ingesting CSV and computing n-gram statistics...\n",
      "Building canonical vocabulary...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m normalizer\u001b[38;5;241m.\u001b[39mbuild_vocab()\n\u001b[0;32m     71\u001b[0m SURFACE_MIN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m  \u001b[38;5;66;03m# tune 20â€“80 depending on corpus size\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m strong_surface \u001b[38;5;241m=\u001b[39m \u001b[43mfrequent_surface_phrases\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRAW_DATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mner_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNER_COL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_phrase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSURFACE_MIN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m pre_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(normalizer\u001b[38;5;241m.\u001b[39mcanon)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ph \u001b[38;5;129;01min\u001b[39;00m strong_surface:\n",
      "Cell \u001b[1;32mIn[15], line 146\u001b[0m, in \u001b[0;36mfrequent_surface_phrases\u001b[1;34m(csv_path, ner_col, chunksize, min_phrase)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mast\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m    145\u001b[0m seen \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path, chunksize\u001b[38;5;241m=\u001b[39mchunksize, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    147\u001b[0m     col \u001b[38;5;241m=\u001b[39m chunk[ner_col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m col:\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m   1984\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[1;32m-> 1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"Starting Ingredient Normalization Pipeline...\")\n",
    "\n",
    "# 0. DEV Env \n",
    "# Toggle between a small sample (for quick testing) and full dataset\n",
    "TESTING = True  # set to True for sample mode\n",
    "\n",
    "RESET_ALL = True  # set to True to reset all intermediate files\n",
    "if RESET_ALL:\n",
    "    # delete all jason/ jsonl and parquet files\n",
    "    for p in [VOCAB_JSON, OUTPUT_PATH, OUTPUT_PATH_SPELL,\n",
    "              Path(\"../data/ner_spell_map.jsonl\"),\n",
    "              Path(\"../data/cosine_dedupe_map.jsonl\"),\n",
    "              Path(\"../data/recipes_data_clean_spell_dedup.parquet\")]:\n",
    "        if p.exists():\n",
    "            p.unlink()\n",
    "            print(f\"Deleted: {p}\")\n",
    "\n",
    "\n",
    "if TESTING:\n",
    "    SAMPLE_SIZE = 10_000  # adjust as needed\n",
    "    print(f\"DEV MODE: sampling {SAMPLE_SIZE} rows from raw data for faster iteration.\")\n",
    "    df_sample = pd.read_csv(RAW_DATA_PATH, dtype=str).sample(\n",
    "        n=SAMPLE_SIZE, random_state=42\n",
    "    )\n",
    "    DATA_PATH = Path(\"../data/sample_data.csv\")\n",
    "    df_sample.to_csv(DATA_PATH, index=False)\n",
    "    path = DATA_PATH\n",
    "    print(f\"Sample data written to: {DATA_PATH}\")\n",
    "else:\n",
    "    print(\"FULL MODE: using full raw dataset.\")\n",
    "    path = RAW_DATA_PATH\n",
    "\n",
    "# Confirm paths\n",
    "print(f\"Data source: {path}\")\n",
    "print(f\"Output parquet will be written to: {OUTPUT_PATH_SPELL}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Instantiate and train StatsNormalizer ---\n",
    "normalizer = StatsNormalizer(\n",
    "    max_ngram=4,\n",
    "    min_unigram=MIN_UNIGRAM,\n",
    "    min_bigram=max(30, MIN_BIGRAM//1),      # a bit lower\n",
    "    min_trigram=max(20, MIN_TRIGRAM//1),\n",
    "    pmi_bigram=max(2.6, PMI_BIGRAM-0.4),\n",
    "    pmi_trigram=max(1.8, PMI_TRIGRAM-0.2),\n",
    "    min_child_share=0.06,                   # was 0.12\n",
    "    max_right_entropy=1.3,                  # was 1.0\n",
    "\n",
    "    # fallbacks\n",
    "    pmi_bigram_fallback=2.2,\n",
    "    min_bigram_fallback=15,\n",
    "    pmi_trigram_fallback=1.8,\n",
    "    min_trigram_fallback=10,\n",
    "    min_child_share_fallback=0.05,\n",
    "    max_right_entropy_fallback=1.5,\n",
    "\n",
    "    snap_score_cutoff=88,                   # was 92\n",
    "    snap_near_perfect=94                    # was 96\n",
    ")\n",
    "\n",
    "if not VOCAB_JSON.exists():\n",
    "    print(\"Ingesting CSV and computing n-gram statistics...\")\n",
    "    normalizer.ingest_csv(path, ner_col=NER_COL, chunksize=CHUNK_SIZE)\n",
    "    print(\"Building canonical vocabulary...\")\n",
    "    normalizer.build_vocab()\n",
    "    SURFACE_MIN = 40  # tune 20â€“80 depending on corpus size\n",
    "    strong_surface = frequent_surface_phrases(RAW_DATA_PATH, ner_col=NER_COL, min_phrase=SURFACE_MIN)\n",
    "    pre_len = len(normalizer.canon)\n",
    "    for ph in strong_surface:\n",
    "        toks = StatsNormalizer._tok(ph)\n",
    "        if 1 <= len(toks) <= normalizer.max_ngram:\n",
    "            normalizer.canon.add(tuple(toks))\n",
    "    normalizer._canon_ready = True  # ensure\n",
    "    print(f\"Surface-phrase canon added: {len(normalizer.canon)-pre_len}\")\n",
    "\n",
    "    normalizer.save_vocab(VOCAB_JSON)\n",
    "    print(f\"Saved vocab: {VOCAB_JSON}\")\n",
    "else:\n",
    "    print(\"Loading existing vocabulary...\")\n",
    "    normalizer = StatsNormalizer()\n",
    "    normalizer.canon = set(tuple(p.split()) for p in json.load(open(VOCAB_JSON))[\"canon\"])\n",
    "    normalizer._canon_ready = True\n",
    "\n",
    "# --- 2. Spell/Fuzzy Map ---\n",
    "SPELL_MAP_PATH = Path(\"../data/ner_spell_map.jsonl\")\n",
    "if not SPELL_MAP_PATH.exists():\n",
    "    print(\"Building spell/fuzzy map...\")\n",
    "    canon_phrases = [\" \".join(p) for p in normalizer.canon]\n",
    "    build_spell_map(canon_phrases, path, ner_col=NER_COL, out_path=SPELL_MAP_PATH)\n",
    "else:\n",
    "    print(\"Existing spell/fuzzy map found, skipping...\")\n",
    "\n",
    "# --- 3. Write cleaned + spellmapped parquet ---\n",
    "if not OUTPUT_PATH_SPELL.exists():\n",
    "    print(\"Transforming dataset using StatsNormalizer and spell map...\")\n",
    "    spell_map = {}\n",
    "    with open(SPELL_MAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            spell_map[obj[\"raw\"]] = obj[\"fixed\"]\n",
    "\n",
    "    normalizer.transform_csv_to_parquet(\n",
    "        csv_path=path,\n",
    "        out_path=OUTPUT_PATH_SPELL,\n",
    "        ner_col=NER_COL,\n",
    "        chunksize=CHUNK_SIZE\n",
    "    )\n",
    "    print(f\"Wrote cleaned (spell-mapped) file: {OUTPUT_PATH_SPELL}\")\n",
    "else:\n",
    "    print(\"Spellmapped Parquet already exists, skipping...\")\n",
    "\n",
    "# --- 4. Build vocab from cleaned Parquet ---\n",
    "print(\"Extracting vocabulary from cleaned Parquet...\")\n",
    "vocab = vocab_from_parquet(OUTPUT_PATH_SPELL)\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "# --- 5. Cosine Similarity Deduplication ---\n",
    "COS_MAP_PATH = Path(\"../data/cosine_dedupe_map.jsonl\")\n",
    "if not COS_MAP_PATH.exists():\n",
    "    print(\"Running cosine similarity deduplication...\")\n",
    "    cosine_map = cosine_dedupe(vocab, threshold=0.88, topk=20, out_path=COS_MAP_PATH)\n",
    "else:\n",
    "    print(\"Existing cosine map found, loading...\")\n",
    "    cosine_map = {}\n",
    "    with open(COS_MAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            cosine_map[obj[\"from\"]] = obj[\"to\"]\n",
    "\n",
    "# --- 6. Apply cosine dedup map ---\n",
    "FINAL_PATH = Path(\"../data/recipes_data_clean_spell_dedup.parquet\")\n",
    "if not FINAL_PATH.exists():\n",
    "    print(\"Applying cosine dedup map to Parquet...\")\n",
    "    apply_cosine_map_to_parquet(\n",
    "        in_path=OUTPUT_PATH_SPELL,\n",
    "        out_path=FINAL_PATH,\n",
    "        mapping=cosine_map,\n",
    "        list_col=\"NER_clean\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Deduped Parquet already exists, skipping...\")\n",
    "\n",
    "# --- 7. Summary ---\n",
    "summarize_pipeline(vocab, data_path=path)\n",
    "print(f\"Final deduped dataset: {FINAL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae060f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf95e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load final deduped parquet and show sample\n",
    "df_final = pd.read_parquet(\"../data/recipes_data_clean_spell_dedup.parquet\")\n",
    "print(\"Sample of final deduped dataset:\")\n",
    "\n",
    "# #display the full NER/ NER_clean columns with no truncation\n",
    "# with pd.option_context('display.max_colwidth', None, 'display.max_rows', None):\n",
    "#     display(df_final[[NER_COL, \"NER_clean\"]].head(100))\n",
    "\n",
    "bad = df_final[df_final[\"NER_clean\"].apply(lambda lst: any(len(x.split())==1 for x in lst))].sample(10)\n",
    "display(bad[[\"NER\", \"NER_clean\"]])\n",
    "\n",
    "lens = [len(x) for x in df_final[\"NER_clean\"]]\n",
    "pd.Series(lens).hist(bins=50)\n",
    "\n",
    "canon_vocab = {\" \".join(t) for t in normalizer.canon}\n",
    "unmatched = [tok for lst in df_final[\"NER_clean\"] for tok in lst if tok not in canon_vocab]\n",
    "print(f\"{len(set(unmatched))} unmatched out of {len(canon_vocab)} canon terms\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "canon_vocab = {\" \".join(t) for t in normalizer.canon}\n",
    "\n",
    "emitted = (\n",
    "    pd.Series(df_final[\"NER_clean\"])\n",
    "      .explode()\n",
    "      .dropna().astype(str).str.strip()\n",
    ")\n",
    "\n",
    "unmatched = emitted[~emitted.isin(canon_vocab)]\n",
    "coverage = 1.0 - unmatched.size / emitted.size\n",
    "print(f\"Canon coverage: {coverage:.2%}  ({unmatched.nunique()} unique unmatched / {emitted.nunique()} unique total)\")\n",
    "\n",
    "print(\"\\nTop 50 unmatched tokens (by frequency):\")\n",
    "print(unmatched.value_counts().head(50))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
