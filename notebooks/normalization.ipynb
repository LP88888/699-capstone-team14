{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309cd8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\georg.desktop-2fs9vf1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8610882d",
   "metadata": {},
   "source": [
    "# Ingredient Normalization (Data-Driven, PMI-Based)\n",
    "This notebook implements a **fully data-driven pipeline** for large-scale ingredient normalization.\n",
    "\n",
    "### ðŸ”§ **Pipeline Summary**\n",
    "| Stage | Component | Purpose | Output |\n",
    "|--------|------------|----------|---------|\n",
    "| **1** | Config & Setup | Define file paths, thresholds, constants | Paths + Params |\n",
    "| **2** | **StatsNormalizer** | Build a PMI-based vocabulary of canonical ingredient collocations (uni/bi/tri-grams) | `ingredient_vocab_stats.json` |\n",
    "| **3** | **Spell/Fuzzy Mapping (Threaded)** | Fast correction of raw ingredient variants using spelling and fuzzy matching | `ner_spell_map.jsonl` |\n",
    "| **4** | **Cosine Similarity Deduplication** | Identify semantically similar ingredient terms using sentence embeddings (MiniLM) | `cosine_dedupe_map.jsonl` |\n",
    "| **5** | **Apply Dedup Map** | Replace duplicates in Parquet and write deduped file | `recipes_data_clean_spell_dedup.parquet` |\n",
    "| **6** | **Summary** | Print stats, vocab size, and output summary | Console output |\n",
    "\n",
    "### ðŸ“¦ **Outputs Generated**\n",
    "- `recipes_data_clean.parquet` â€” PMI-cleaned dataset\n",
    "- `recipes_data_clean_spell.parquet` â€” cleaned + spell-mapped dataset\n",
    "- `recipes_data_clean_spell_dedup.parquet` â€” final cosine-deduped dataset\n",
    "- `ingredient_vocab_stats.json` â€” canonical vocabulary statistics\n",
    "- `cosine_dedupe_map.jsonl` â€” mapping of similar terms\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Runtime Tips**\n",
    "- Use smaller `CHUNK_SIZE` (e.g., 100,000) if memory-limited.\n",
    "- Use `SentenceTransformer('all-MiniLM-L6-v2', device='cuda')` for GPU acceleration.\n",
    "- Reuse saved vocab and dedup maps across re-runs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline Visualization**\n",
    "\n",
    "<img src=\"./ingredient_row_sequence.png\" alt=\"Ingredient normalization pipeline\" width=\"850\"/>`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## âš™ï¸ Code Sections Below\n",
    "The notebook proceeds through these main stages:\n",
    "1. Config & Setup\n",
    "2. StatsNormalizer (PMI Collocations)\n",
    "3. Spell/Fuzzy Map (Threaded)\n",
    "4. Cosine Similarity Deduplication\n",
    "5. Apply Dedup Map to Parquet\n",
    "6. Final Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495281e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional installs (run once if missing)\n",
    "# !pip install pyarrow\n",
    "\n",
    "# # Update requirements.txt \n",
    "# !pip freeze > ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f6f86",
   "metadata": {},
   "source": [
    "## Config and global imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1fdc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, gc, json, math, re, ast\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Iterable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_DATA_PATH = Path(\"../data/wilmerarltstrmberg_data.csv\")\n",
    "OUTPUT_PATH = Path(\"../data/recipes_data_clean.parquet\")\n",
    "OUTPUT_PATH_SPELL = Path(\"../data/recipes_data_clean_spell.parquet\")\n",
    "VOCAB_JSON = Path(\"../data/ingredient_vocab_stats.json\")\n",
    "NER_COL = \"NER\"\n",
    "CHUNK_SIZE = 200_000\n",
    "\n",
    "MIN_UNIGRAM = 50\n",
    "MIN_BIGRAM = 50\n",
    "MIN_TRIGRAM = 30\n",
    "PMI_BIGRAM = 3.0\n",
    "PMI_TRIGRAM = 2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3eba8",
   "metadata": {},
   "source": [
    "## 2. StatsNormalizer: PMI-based collocation model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf2fd586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-featured StatsNormalizer (drop-in replacement)\n",
    "\n",
    "import ast, gc, json, math, re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "class StatsNormalizer:\n",
    "    \"\"\"\n",
    "    PMI-based n-gram normalizer with:\n",
    "      - Streaming counts (uni/bi/tri/4-grams)\n",
    "      - PMI + child-share + entropy gates to build a canonical vocabulary\n",
    "      - Greedy 4â†’3â†’2â†’1 segmentation with optional span-preserving fuzzy snap\n",
    "      - CSVâ†’Parquet writer (Arrow-friendly, list<string> for NER_clean)\n",
    "      - Save/load of learned vocabulary\n",
    "\n",
    "    Public API:\n",
    "      - ingest_csv / ingest_df\n",
    "      - build_vocab\n",
    "      - segment_item\n",
    "      - transform_df\n",
    "      - transform_csv_to_parquet\n",
    "      - save_vocab / load_vocab\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------- Utilities ----------------\n",
    "    @staticmethod\n",
    "    def _tok(s):\n",
    "        return re.findall(r\"[a-z']+\", str(s).lower())\n",
    "\n",
    "    @staticmethod\n",
    "    def _ngrams(tokens, n):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            yield tuple(tokens[i:i+n])\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_ner_entry(entry):\n",
    "        # Accepts list-like strings \"['a','b']\" or JSON-ish strings, or comma-separated.\n",
    "        if entry is None or (isinstance(entry, float) and pd.isna(entry)):\n",
    "            return []\n",
    "        s = str(entry).strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        # try Python literal list\n",
    "        try:\n",
    "            parsed = ast.literal_eval(s)\n",
    "            if isinstance(parsed, list):\n",
    "                return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "        # comma-separated fallback\n",
    "        return [x.strip() for x in s.split(\",\") if x.strip()]\n",
    "\n",
    "    # ---------------- Init ----------------\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_ngram=4,\n",
    "        # canon thresholds\n",
    "        min_unigram=50, min_bigram=50, min_trigram=30, min_fourgram=20,\n",
    "        pmi_bigram=3.0, pmi_trigram=2.0, pmi_fourgram=2.0,\n",
    "        # trigram gates\n",
    "        min_child_share=0.12, max_right_entropy=1.0,\n",
    "        # 4-gram gates (trigram-head branching)\n",
    "        min_child_share4=0.05, max_right_entropy3=1.3,\n",
    "        # dynamic fallbacks (optional safety nets; can be tuned/relaxed)\n",
    "        pmi_bigram_fallback=2.6,  min_bigram_fallback=20,\n",
    "        pmi_trigram_fallback=2.2, min_trigram_fallback=12,\n",
    "        min_child_share_fallback=0.06, max_right_entropy_fallback=1.4,\n",
    "        pmi_fourgram_fallback=1.8, min_fourgram_fallback=10,\n",
    "        min_child_share4_fallback=0.04, max_right_entropy3_fallback=1.5,\n",
    "        # snap settings\n",
    "        snap_score_cutoff=92, snap_near_perfect=96\n",
    "    ):\n",
    "        # config\n",
    "        self.max_ngram = int(max_ngram)\n",
    "\n",
    "        self.min_unigram, self.min_bigram, self.min_trigram, self.min_fourgram = (\n",
    "            min_unigram, min_bigram, min_trigram, min_fourgram\n",
    "        )\n",
    "        self.pmi_bigram, self.pmi_trigram, self.pmi_fourgram = (\n",
    "            pmi_bigram, pmi_trigram, pmi_fourgram\n",
    "        )\n",
    "\n",
    "        self.min_child_share, self.max_right_entropy = min_child_share, max_right_entropy\n",
    "        self.min_child_share4, self.max_right_entropy3 = min_child_share4, max_right_entropy3\n",
    "\n",
    "        self.pmi_bigram_fallback, self.min_bigram_fallback = pmi_bigram_fallback, min_bigram_fallback\n",
    "        self.pmi_trigram_fallback, self.min_trigram_fallback = pmi_trigram_fallback, min_trigram_fallback\n",
    "        self.min_child_share_fallback, self.max_right_entropy_fallback = (\n",
    "            min_child_share_fallback, max_right_entropy_fallback\n",
    "        )\n",
    "        self.pmi_fourgram_fallback, self.min_fourgram_fallback = pmi_fourgram_fallback, min_fourgram_fallback\n",
    "        self.min_child_share4_fallback, self.max_right_entropy3_fallback = (\n",
    "            min_child_share4_fallback, max_right_entropy3_fallback\n",
    "        )\n",
    "\n",
    "        self.snap_score_cutoff, self.snap_near_perfect = snap_score_cutoff, snap_near_perfect\n",
    "\n",
    "        # counts\n",
    "        self.token_total = 0\n",
    "        self.c1, self.c2, self.c3, self.c4 = Counter(), Counter(), Counter(), Counter()\n",
    "\n",
    "        # follower distributions for entropy\n",
    "        self._followers  = defaultdict(Counter)  # (a,b)->c\n",
    "        self._followers3 = defaultdict(Counter)  # (a,b,c)->d\n",
    "\n",
    "        # canon & caches\n",
    "        self.canon = set()\n",
    "        self._canon_ready = False\n",
    "        self._canon_phrases = None\n",
    "        self._canon_buckets = None\n",
    "\n",
    "    # ---------------- Ingestion ----------------\n",
    "    def ingest_df(self, df, ner_col=\"NER\"):\n",
    "        for entry in df[ner_col]:\n",
    "            for item in self._parse_ner_entry(entry):\n",
    "                toks = self._tok(item)\n",
    "                if not toks:\n",
    "                    continue\n",
    "\n",
    "                self.c1.update(toks)\n",
    "                self.token_total += len(toks)\n",
    "\n",
    "                if self.max_ngram >= 2 and len(toks) >= 2:\n",
    "                    self.c2.update(self._ngrams(toks, 2))\n",
    "\n",
    "                if self.max_ngram >= 3 and len(toks) >= 3:\n",
    "                    for i in range(len(toks) - 2):\n",
    "                        a, b, c = toks[i], toks[i+1], toks[i+2]\n",
    "                        self.c3[(a, b, c)] += 1\n",
    "                        self._followers[(a, b)][c] += 1\n",
    "\n",
    "                if self.max_ngram >= 4 and len(toks) >= 4:\n",
    "                    for i in range(len(toks) - 3):\n",
    "                        a, b, c, d = toks[i], toks[i+1], toks[i+2], toks[i+3]\n",
    "                        self.c4[(a, b, c, d)] += 1\n",
    "                        self._followers3[(a, b, c)][d] += 1\n",
    "\n",
    "    def ingest_csv(self, csv_path, ner_col=\"NER\", chunksize=200_000):\n",
    "        for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
    "            self.ingest_df(chunk, ner_col=ner_col)\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "\n",
    "    # ---------------- Stats (PMI/Entropy/Share) ----------------\n",
    "    def _right_entropy(self, ab):\n",
    "        foll = self._followers.get(ab)\n",
    "        if not foll:\n",
    "            return 0.0\n",
    "        tot = sum(foll.values())\n",
    "        if tot == 0:\n",
    "            return 0.0\n",
    "        H = 0.0\n",
    "        for v in foll.values():\n",
    "            p = v / tot\n",
    "            H -= p * math.log(p + 1e-12)\n",
    "        return H\n",
    "\n",
    "    def _right_entropy3(self, abc):\n",
    "        foll = self._followers3.get(abc)\n",
    "        if not foll:\n",
    "            return 0.0\n",
    "        tot = sum(foll.values())\n",
    "        if tot == 0:\n",
    "            return 0.0\n",
    "        H = 0.0\n",
    "        for v in foll.values():\n",
    "            p = v / tot\n",
    "            H -= p * math.log(p + 1e-12)\n",
    "        return H\n",
    "\n",
    "    def _child_share(self, abc):\n",
    "        cabc = self.c3.get(abc, 0)\n",
    "        cab  = self.c2.get(abc[:2], 0)\n",
    "        return (cabc / cab) if cab else 0.0\n",
    "\n",
    "    def _child_share4(self, abcd):\n",
    "        cabcd = self.c4.get(abcd, 0)\n",
    "        cabc  = self.c3.get(abcd[:3], 0)\n",
    "        return (cabcd / cabc) if cabc else 0.0\n",
    "\n",
    "    def _pmi_bigram(self, ab):\n",
    "        a, b = ab\n",
    "        cab = self.c2.get(ab, 0)\n",
    "        if cab == 0 or self.token_total == 0:\n",
    "            return -1e9\n",
    "        pa = self.c1.get(a, 0) / self.token_total\n",
    "        pb = self.c1.get(b, 0) / self.token_total\n",
    "        pab = cab / self.token_total\n",
    "        return math.log((pab / (pa * pb)) + 1e-12)\n",
    "\n",
    "    def _pmi_trigram(self, abc):\n",
    "        a, b, c = abc\n",
    "        return (self._pmi_bigram((a, b)) + self._pmi_bigram((b, c))) / 2.0\n",
    "\n",
    "    def _pmi_fourgram(self, abcd):\n",
    "        a, b, c, d = abcd\n",
    "        return (\n",
    "            self._pmi_bigram((a, b))\n",
    "            + self._pmi_bigram((b, c))\n",
    "            + self._pmi_bigram((c, d))\n",
    "        ) / 3.0\n",
    "\n",
    "    # ---------------- Build Canon ----------------\n",
    "    def build_vocab(self):\n",
    "        self.canon.clear()\n",
    "\n",
    "        # unigrams\n",
    "        for w, c in self.c1.items():\n",
    "            if c >= self.min_unigram:\n",
    "                self.canon.add((w,))\n",
    "\n",
    "        # bigrams\n",
    "        for ab, c in self.c2.items():\n",
    "            if c >= self.min_bigram and self._pmi_bigram(ab) >= self.pmi_bigram:\n",
    "                self.canon.add(ab)\n",
    "\n",
    "        # trigrams\n",
    "        for abc, c in self.c3.items():\n",
    "            if c < self.min_trigram:\n",
    "                continue\n",
    "            if self._pmi_trigram(abc) < self.pmi_trigram:\n",
    "                continue\n",
    "            if self._child_share(abc) < self.min_child_share:\n",
    "                continue\n",
    "            if self._right_entropy(abc[:2]) > self.max_right_entropy:\n",
    "                continue\n",
    "            self.canon.add(abc)\n",
    "\n",
    "        # four-grams\n",
    "        for abcd, c in self.c4.items():\n",
    "            if c < self.min_fourgram:\n",
    "                continue\n",
    "            if self._pmi_fourgram(abcd) < self.pmi_fourgram:\n",
    "                continue\n",
    "            if self._child_share4(abcd) < self.min_child_share4:\n",
    "                continue\n",
    "            if self._right_entropy3(abcd[:3]) > self.max_right_entropy3:\n",
    "                continue\n",
    "            self.canon.add(abcd)\n",
    "\n",
    "        self._canon_ready = True\n",
    "        self._canon_phrases = None\n",
    "        self._canon_buckets = None\n",
    "\n",
    "    # ---------------- Snap helpers ----------------\n",
    "    def _canon_bucket_init(self):\n",
    "        self._canon_phrases = [\" \".join(p) for p in self.canon]\n",
    "        buckets = {}\n",
    "        for ph in self._canon_phrases:\n",
    "            ft = ph.split()[0] if ph else \"\"\n",
    "            buckets.setdefault(ft, []).append(ph)\n",
    "        self._canon_buckets = buckets\n",
    "\n",
    "    def _snap_span(self, tokens, i, n):\n",
    "        \"\"\"\n",
    "        Try snapping tokens[i:i+n] to a canon phrase with same length (or near-perfect match).\n",
    "        Uses rapidfuzz if available; otherwise returns None.\n",
    "        \"\"\"\n",
    "        if i + n > len(tokens): return None\n",
    "        if self._canon_phrases is None or self._canon_buckets is None:\n",
    "            self._canon_bucket_init()\n",
    "        try:\n",
    "            from rapidfuzz import process, fuzz\n",
    "        except Exception:\n",
    "            return None\n",
    "        span = \" \".join(tokens[i:i+n])\n",
    "        bucket = self._canon_buckets.get(tokens[i], self._canon_phrases)\n",
    "        match = process.extractOne(span, bucket, scorer=fuzz.WRatio, score_cutoff=self.snap_score_cutoff)\n",
    "        if not match:\n",
    "            return None\n",
    "        cand, score = match[0], match[1]\n",
    "        # Do not shrink unless near-perfect\n",
    "        if len(cand.split()) < n and score < self.snap_near_perfect:\n",
    "            return None\n",
    "        if len(cand.split()) == n or score >= self.snap_near_perfect:\n",
    "            return cand.split(), n\n",
    "        return None\n",
    "\n",
    "    # ---------------- Segmentation ----------------\n",
    "    def post_compact(self, tokens, cos_map=None, max_window=3):\n",
    "        \"\"\"\n",
    "        Try to re-glue adjacent tokens if the combined phrase is strong by:\n",
    "        - exact canon membership, or\n",
    "        - bigram/trigram fallback thresholds, or\n",
    "        - cosine-dedupe map equivalence.\n",
    "        \"\"\"\n",
    "        if not tokens:\n",
    "            return tokens\n",
    "        i, out = 0, []\n",
    "        N = len(tokens)\n",
    "        cos_map = cos_map or {}\n",
    "\n",
    "        def _ok_bigram(ab):\n",
    "            ab_t = tuple(StatsNormalizer._tok(ab))\n",
    "            if len(ab_t) != 2: return False\n",
    "            return (self.c2.get(ab_t, 0) >= self.min_bigram_fallback and\n",
    "                    self._pmi_bigram(ab_t) >= self.pmi_bigram_fallback)\n",
    "\n",
    "        def _ok_trigram(abc):\n",
    "            abc_t = tuple(StatsNormalizer._tok(abc))\n",
    "            if len(abc_t) != 3: return False\n",
    "            return (self.c3.get(abc_t, 0) >= self.min_trigram_fallback and\n",
    "                    self._pmi_trigram(abc_t) >= self.pmi_trigram_fallback)\n",
    "\n",
    "        while i < N:\n",
    "            # try 3-gram glue\n",
    "            if i+2 < N:\n",
    "                tri = f\"{tokens[i]} {tokens[i+1]} {tokens[i+2]}\"\n",
    "                tri_t = tuple(StatsNormalizer._tok(tri))\n",
    "                if tri_t in self.canon or _ok_trigram(tri):\n",
    "                    out.append(\" \".join(tri_t)); i += 3; continue\n",
    "            # try 2-gram glue\n",
    "            if i+1 < N:\n",
    "                bi = f\"{tokens[i]} {tokens[i+1]}\"\n",
    "                bi_t = tuple(StatsNormalizer._tok(bi))\n",
    "                if (bi_t in self.canon or _ok_bigram(bi) or\n",
    "                    cos_map.get(tokens[i]) == cos_map.get(tokens[i+1]) and cos_map.get(tokens[i]) is not None):\n",
    "                    out.append(\" \".join(bi_t)); i += 2; continue\n",
    "            out.append(tokens[i]); i += 1\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def _longest_match(self, toks, i):\n",
    "        if not self._canon_ready:\n",
    "            raise RuntimeError(\"build_vocab() first\")\n",
    "\n",
    "        # exact canon first\n",
    "        if self.max_ngram >= 4 and i+3 < len(toks) and tuple(toks[i:i+4]) in self.canon:\n",
    "            return tuple(toks[i:i+4]), 4\n",
    "        if self.max_ngram >= 3 and i+2 < len(toks) and tuple(toks[i:i+3]) in self.canon:\n",
    "            return tuple(toks[i:i+3]), 3\n",
    "        if self.max_ngram >= 2 and i+1 < len(toks) and tuple(toks[i:i+2]) in self.canon:\n",
    "            return tuple(toks[i:i+2]), 2\n",
    "        if (toks[i],) in self.canon:\n",
    "            return (toks[i],), 1\n",
    "\n",
    "        # snap to canon (span-preserving)\n",
    "        if self.max_ngram >= 3:\n",
    "            snapped = self._snap_span(toks, i, 3)\n",
    "            if snapped:\n",
    "                return tuple(snapped[0]), snapped[1]\n",
    "        if self.max_ngram >= 2:\n",
    "            snapped = self._snap_span(toks, i, 2)\n",
    "            if snapped:\n",
    "                return tuple(snapped[0]), snapped[1]\n",
    "\n",
    "        # dynamic fallbacks based on stats (optional but helpful)\n",
    "        if self.max_ngram >= 4 and i+3 < len(toks):\n",
    "            abcd = (toks[i], toks[i+1], toks[i+2], toks[i+3])\n",
    "            cabcd = self.c4.get(abcd, 0)\n",
    "            if cabcd >= self.min_fourgram_fallback:\n",
    "                if (self._pmi_fourgram(abcd) >= self.pmi_fourgram_fallback and\n",
    "                    self._child_share4(abcd)  >= self.min_child_share4_fallback and\n",
    "                    self._right_entropy3(abcd[:3]) <= self.max_right_entropy3_fallback):\n",
    "                    return abcd, 4\n",
    "\n",
    "        if self.max_ngram >= 3 and i+2 < len(toks):\n",
    "            abc = (toks[i], toks[i+1], toks[i+2])\n",
    "            cabc = self.c3.get(abc, 0)\n",
    "            if cabc >= self.min_trigram_fallback:\n",
    "                if (self._pmi_trigram(abc) >= self.pmi_trigram_fallback and\n",
    "                    self._child_share(abc)  >= self.min_child_share_fallback and\n",
    "                    self._right_entropy(abc[:2]) <= self.max_right_entropy_fallback):\n",
    "                    return abc, 3\n",
    "\n",
    "        if self.max_ngram >= 2 and i+1 < len(toks):\n",
    "            ab = (toks[i], toks[i+1])\n",
    "            cab = self.c2.get(ab, 0)\n",
    "            if cab >= self.min_bigram_fallback and self._pmi_bigram(ab) >= self.pmi_bigram_fallback:\n",
    "                return ab, 2\n",
    "\n",
    "        # fallback unigram\n",
    "        return (toks[i],), 1\n",
    "\n",
    "    def segment_item(self, text):\n",
    "        t = self._tok(text)\n",
    "        out, i = [], 0\n",
    "        while i < len(t):\n",
    "            phrase, k = self._longest_match(t, i)\n",
    "            out.append(\" \".join(phrase))\n",
    "            i += k\n",
    "\n",
    "        # de-dup while preserving order\n",
    "        seen, clean = set(), []\n",
    "        for x in out:\n",
    "            if x not in seen:\n",
    "                clean.append(x); seen.add(x)\n",
    "\n",
    "        clean = self.post_compact(clean)  # or self.post_compact(clean, cos_map=cosine_map)\n",
    "        \n",
    "        # drop immediate repetition of previous tail\n",
    "        pruned = []\n",
    "        for x in clean:\n",
    "            if pruned and x == pruned[-1].split()[-1]:\n",
    "                continue\n",
    "            pruned.append(x)\n",
    "        return pruned\n",
    "\n",
    "    # ---------------- DataFrame & IO ----------------\n",
    "    def transform_df(self, df, ner_col=\"NER\", out_col=\"NER_clean\", dedupe_row=False):\n",
    "        results = []\n",
    "        for v in df[ner_col]:\n",
    "            segs = [seg for item in self._parse_ner_entry(v) for seg in self.segment_item(item)]\n",
    "            if dedupe_row:\n",
    "                seen, uniq = set(), []\n",
    "                for s in segs:\n",
    "                    if s not in seen:\n",
    "                        uniq.append(s); seen.add(s)\n",
    "                results.append(uniq)\n",
    "            else:\n",
    "                results.append(segs)\n",
    "        df[out_col] = results\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _sanitize_for_arrow(df, list_col=\"NER_clean\"):\n",
    "        # Make Arrow-friendly: list<string> for list_col; stringify complex objects elsewhere.\n",
    "        df = df.copy()\n",
    "\n",
    "        def _to_list_of_str(x):\n",
    "            if isinstance(x, (list, tuple, np.ndarray)):\n",
    "                return [str(y) for y in x]\n",
    "            if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "                return []\n",
    "            # best-effort parse of serialized lists\n",
    "            try:\n",
    "                parsed = ast.literal_eval(str(x))\n",
    "                if isinstance(parsed, (list, tuple, np.ndarray)):\n",
    "                    return [str(y) for y in parsed]\n",
    "            except Exception:\n",
    "                pass\n",
    "            return [str(x)]\n",
    "\n",
    "        if list_col in df.columns:\n",
    "            df[list_col] = df[list_col].apply(_to_list_of_str)\n",
    "\n",
    "        for col in df.columns:\n",
    "            if col == list_col:\n",
    "                continue\n",
    "            s = df[col]\n",
    "            if s.dtype == object:\n",
    "                def _to_scalar_str(v):\n",
    "                    if isinstance(v, (list, tuple, dict, set, np.ndarray)):\n",
    "                        return json.dumps(list(v) if isinstance(v, np.ndarray) else v, ensure_ascii=False)\n",
    "                    return \"\" if v is None or (isinstance(v, float) and pd.isna(v)) else str(v)\n",
    "                df[col] = s.map(_to_scalar_str)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform_csv_to_parquet(self, csv_path, out_path, ner_col=\"NER\", chunksize=200_000):\n",
    "        \"\"\"\n",
    "        Stream a CSV, segment NER items using the learned canon/fallbacks,\n",
    "        and write Parquet with NER_clean as list<string>.\n",
    "        \"\"\"\n",
    "        writer = None\n",
    "        for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
    "            # segment\n",
    "            chunk = self.transform_df(chunk, ner_col=ner_col, out_col=\"NER_clean\")\n",
    "            # sanitize for Arrow\n",
    "            chunk = self._sanitize_for_arrow(chunk, list_col=\"NER_clean\")\n",
    "\n",
    "            # to Arrow table, force list<string> for NER_clean\n",
    "            table = pa.Table.from_pandas(chunk, preserve_index=False).replace_schema_metadata(None)\n",
    "            fields = []\n",
    "            for f in table.schema:\n",
    "                if f.name == \"NER_clean\" and not pa.types.is_list(f.type):\n",
    "                    fields.append(pa.field(\"NER_clean\", pa.list_(pa.string())))\n",
    "                else:\n",
    "                    fields.append(f)\n",
    "            target_schema = pa.schema(fields)\n",
    "            try:\n",
    "                table = table.cast(target_schema, safe=False)\n",
    "            except Exception:\n",
    "                arrays = [pa.array(arr, type=pa.list_(pa.string())) for arr in table.column(\"NER_clean\").to_pylist()]\n",
    "                table = table.set_column(table.schema.get_field_index(\"NER_clean\"),\n",
    "                                         \"NER_clean\", pa.chunked_array(arrays))\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(out_path, target_schema, compression=\"zstd\")\n",
    "            writer.write_table(table)\n",
    "\n",
    "            del chunk, table\n",
    "            gc.collect()\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "    # ---------------- Persistence ----------------\n",
    "    def save_vocab(self, path):\n",
    "        data = {\n",
    "            \"token_total\": int(self.token_total),\n",
    "            \"canon\": [\" \".join(p) for p in sorted(self.canon)]\n",
    "        }\n",
    "        Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load_vocab(cls, path):\n",
    "        data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "        obj = cls()\n",
    "        obj.canon = set(tuple(p.split()) for p in data[\"canon\"])\n",
    "        obj._canon_ready = True\n",
    "        obj._canon_phrases = None\n",
    "        obj._canon_buckets = None\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580fb10e",
   "metadata": {},
   "source": [
    "## 3. Spell + Fuzzy Mapping (threaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebc60610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from rapidfuzz import process, fuzz\n",
    "from spellchecker import SpellChecker\n",
    "import pandas as pd, json, os, threading\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_spell_map(canon_phrases, csv_path, ner_col=\"NER\", out_path=Path(\"../data/ner_spell_map.jsonl\"),\n",
    "                    chunksize=200_000, batch_size=2000, max_workers=None,\n",
    "                    score_cutoff=92, near_perfect=96):\n",
    "    \"\"\"\n",
    "    Robust builder for a global spell/fuzzy mapping:\n",
    "      - Streams unique raw items from csv_path[ner_col]\n",
    "      - Token-level spell correction, then phrase snap to canon\n",
    "      - Span-preserving: only shrink/expand if near-perfect\n",
    "    Writes JSONL with {\"raw\": ..., \"fixed\": ...} per line.\n",
    "    \"\"\"\n",
    "    if not canon_phrases:\n",
    "        raise ValueError(\"build_spell_map: canon_phrases is empty. Did you run build_vocab()?\")\n",
    "\n",
    "    # Build first-token buckets for faster fuzzy\n",
    "    buckets = {}\n",
    "    for p in canon_phrases:\n",
    "        toks = str(p).split()\n",
    "        if not toks:\n",
    "            continue\n",
    "        buckets.setdefault(toks[0], []).append(p)\n",
    "    canon_set = set(canon_phrases)\n",
    "\n",
    "    # Collect unique raw items\n",
    "    seen = set()\n",
    "    for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
    "        col = chunk.get(ner_col)\n",
    "        if col is None:\n",
    "            continue\n",
    "        for entry in col:\n",
    "            # Parse as list-like if possible, else treat as comma-separated\n",
    "            items = []\n",
    "            if entry is None:\n",
    "                pass\n",
    "            else:\n",
    "                s = str(entry).strip()\n",
    "                if s:\n",
    "                    try:\n",
    "                        parsed = ast.literal_eval(s)\n",
    "                        if isinstance(parsed, (list, tuple)):\n",
    "                            items = [str(x) for x in parsed if str(x).strip()]\n",
    "                    except Exception:\n",
    "                        items = [x.strip() for x in s.split(\",\") if x.strip()]\n",
    "            for it in items:\n",
    "                if it: seen.add(str(it))\n",
    "        del chunk\n",
    "\n",
    "    items = sorted(seen)\n",
    "\n",
    "    # Thread-local spellchecker to avoid rebuild per call\n",
    "    _tls = threading.local()\n",
    "    vocab_tokens = [t for ph in canon_phrases for t in str(ph).split()]\n",
    "\n",
    "    def _get_spell():\n",
    "        sc = getattr(_tls, \"sc\", None)\n",
    "        if sc is None:\n",
    "            sc = SpellChecker(distance=2)\n",
    "            if vocab_tokens:\n",
    "                sc.word_frequency.load_words(vocab_tokens)\n",
    "            _tls.sc = sc\n",
    "        return sc\n",
    "\n",
    "    # Helper: safer tok\n",
    "    _WORD_RE = re.compile(r\"[a-z']+\")\n",
    "    def _tok(text):\n",
    "        return _WORD_RE.findall(str(text).lower())\n",
    "\n",
    "    def _fix_batch(batch):\n",
    "        sc = _get_spell()\n",
    "        out = []\n",
    "        for raw in batch:\n",
    "            raw_str = str(raw)\n",
    "            toks = _tok(raw_str)\n",
    "\n",
    "            # If tokenization yields nothing, keep original\n",
    "            if not toks:\n",
    "                out.append((raw_str, raw_str))\n",
    "                continue\n",
    "\n",
    "            # Word-level spell correct (keep span)\n",
    "            toks2 = [sc.correction(t) or t for t in toks]\n",
    "            if not toks2:\n",
    "                out.append((raw_str, raw_str))\n",
    "                continue\n",
    "\n",
    "            corrected = \" \".join(toks2)\n",
    "\n",
    "            # Early exit if exact canon\n",
    "            if corrected in canon_set:\n",
    "                out.append((raw_str, corrected))\n",
    "                continue\n",
    "\n",
    "            # Choose candidate pool (fallback to full canon if no first token)\n",
    "            if toks2:\n",
    "                choices = buckets.get(toks2[0], canon_phrases)\n",
    "            else:\n",
    "                choices = canon_phrases\n",
    "\n",
    "            # Rapidfuzz can return tuple-like (choice, score, idx) or object\n",
    "            match = process.extractOne(corrected, choices, scorer=fuzz.WRatio, score_cutoff=score_cutoff)\n",
    "\n",
    "            if match:\n",
    "                cand = match[0]\n",
    "                score = match[1] if len(match) > 1 else 100\n",
    "                len_ok = (len(cand.split()) == len(toks2))\n",
    "                if len_ok or score >= near_perfect:\n",
    "                    out.append((raw_str, cand))\n",
    "                else:\n",
    "                    out.append((raw_str, corrected))\n",
    "            else:\n",
    "                out.append((raw_str, corrected))\n",
    "        return out\n",
    "\n",
    "    # Batch and run threaded\n",
    "    def _chunks(lst, n):\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i+n]\n",
    "\n",
    "    batches = list(_chunks(items, batch_size))\n",
    "    if max_workers is None:\n",
    "        cpu = (os.cpu_count() or 2)\n",
    "        max_workers = min(16, 2 * cpu)\n",
    "\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex, open(out_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        for res in tqdm(ex.map(_fix_batch, batches), total=len(batches), desc=\"Spell/Fuzzy map\"):\n",
    "            for raw, fixed in res:\n",
    "                out.write(json.dumps({\"raw\": raw, \"fixed\": fixed}) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote spell map: {out_path}\")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def frequent_surface_phrases(csv_path, ner_col=\"NER\", chunksize=200_000, min_phrase=40):\n",
    "    \"\"\"Counts full NER items as phrases; returns a set of phrases meeting min_phrase.\"\"\"\n",
    "    import pandas as pd, ast, gc\n",
    "    seen = Counter()\n",
    "    for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
    "        col = chunk[ner_col].astype(str)\n",
    "        for s in col:\n",
    "            s = s.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            # Accept list-like or raw string\n",
    "            try:\n",
    "                v = ast.literal_eval(s)\n",
    "                if isinstance(v, (list, tuple)):\n",
    "                    for it in v:\n",
    "                        it = str(it).strip().lower()\n",
    "                        if it:\n",
    "                            seen[it] += 1\n",
    "                else:\n",
    "                    seen[str(v).strip().lower()] += 1\n",
    "            except Exception:\n",
    "                seen[s.lower()] += 1\n",
    "        del chunk; gc.collect()\n",
    "    return {p for p,c in seen.items() if c >= min_phrase}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd4069",
   "metadata": {},
   "source": [
    "## 4. Cosine Similarity Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be611a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvocab_from_parquet\u001b[39m(path, col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNER_clean\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\sentence_transformers\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     13\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     CrossEncoder,\n\u001b[0;32m     17\u001b[0m     CrossEncoderModelCardData,\n\u001b[0;32m     18\u001b[0m     CrossEncoderTrainer,\n\u001b[0;32m     19\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\sentence_transformers\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_optimized_onnx_model\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_dynamic_quantized_onnx_model, export_static_quantized_openvino_model\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\sentence_transformers\\backend\\load.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _save_pretrained_wrapper, backend_should_export, backend_warn_to_save\n\u001b[0;32m     11\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\transformers\\__init__.py:958\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m    956\u001b[0m _import_structure \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mset\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _import_structure\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m--> 958\u001b[0m import_structure \u001b[38;5;241m=\u001b[39m \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m import_structure[\u001b[38;5;28mfrozenset\u001b[39m({})]\u001b[38;5;241m.\u001b[39mupdate(_import_structure)\n\u001b[0;32m    961\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m _LazyModule(\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m     extra_objects\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m: __version__},\n\u001b[0;32m    967\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:2867\u001b[0m, in \u001b[0;36mdefine_import_structure\u001b[1;34m(module_path, prefix)\u001b[0m\n\u001b[0;32m   2843\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m\n\u001b[0;32m   2844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m, prefix: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IMPORT_STRUCTURE_T:\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2846\u001b[0m \u001b[38;5;124;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[0;32m   2847\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2865\u001b[0m \u001b[38;5;124;03m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[0;32m   2866\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2867\u001b[0m     import_structure \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2868\u001b[0m     spread_dict \u001b[38;5;241m=\u001b[39m spread_import_structure(import_structure)\n\u001b[0;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:2580\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[1;34m(module_path)\u001b[0m\n\u001b[0;32m   2578\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(module_path):\n\u001b[0;32m   2579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__pycache__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(module_path, f)):\n\u001b[1;32m-> 2580\u001b[0m         import_structure[f] \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2582\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, f)):\n\u001b[0;32m   2583\u001b[0m         adjacent_modules\u001b[38;5;241m.\u001b[39mappend(f)\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:2604\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[1;34m(module_path)\u001b[0m\n\u001b[0;32m   2601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2602\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 2604\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   2605\u001b[0m     file_content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   2607\u001b[0m \u001b[38;5;66;03m# Remove the .py suffix\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m    byte sequences.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    310\u001b[0m         IncrementalDecoder\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors)\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;66;03m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "def vocab_from_parquet(path, col=\"NER_clean\"):\n",
    "    import numpy as np, ast\n",
    "    pf = pq.ParquetFile(path)\n",
    "    vocab = Counter()\n",
    "    for rg in range(pf.num_row_groups):\n",
    "        df = pf.read_row_group(rg, columns=[col]).to_pandas()\n",
    "        for x in df[col]:\n",
    "            if isinstance(x, (list, tuple, np.ndarray)):\n",
    "                vocab.update(str(t).strip() for t in x if str(t).strip())\n",
    "            elif isinstance(x, str) and x.startswith(\"[\"):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(x)\n",
    "                    if isinstance(parsed, (list, tuple, np.ndarray)):\n",
    "                        vocab.update(str(t).strip() for t in parsed if str(t).strip())\n",
    "                except Exception:\n",
    "                    pass\n",
    "        gc.collect()\n",
    "    return vocab\n",
    "\n",
    "def cosine_dedupe(vocab, threshold=0.88, topk=20, out_path=Path(\"../data/cosine_dedupe_map.jsonl\")):\n",
    "    phrases = [p for p, c in vocab.items() if c >= 1]\n",
    "    if len(phrases) == 0:\n",
    "        raise RuntimeError(\"No phrases in vocab for cosine deduplication.\")\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    emb = model.encode([p.lower() for p in phrases], convert_to_tensor=True, show_progress_bar=True)\n",
    "    emb = torch.nn.functional.normalize(emb, p=2, dim=-1)\n",
    "    emb_np = emb.detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    import faiss\n",
    "    index = faiss.IndexFlatIP(emb_np.shape[1])\n",
    "    index.add(emb_np)\n",
    "    sims, idxs = index.search(emb_np, min(topk, len(phrases)))\n",
    "\n",
    "    parent = list(range(len(phrases)))\n",
    "    def find(x):\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "    def union(x, y):\n",
    "        rx, ry = find(x), find(y)\n",
    "        if rx != ry: parent[ry] = rx\n",
    "\n",
    "    for i in range(len(phrases)):\n",
    "        for j in range(1, sims.shape[1]):\n",
    "            if sims[i, j] >= threshold:\n",
    "                union(i, idxs[i, j])\n",
    "\n",
    "    from collections import defaultdict\n",
    "    clusters = defaultdict(list)\n",
    "    for i in range(len(phrases)):\n",
    "        clusters[find(i)].append(i)\n",
    "\n",
    "    id_to_canon = {}\n",
    "    freqs = np.array([vocab[p] for p in phrases])\n",
    "    for root, ids in clusters.items():\n",
    "        c = sorted(ids, key=lambda k: (-freqs[k], len(phrases[k]), phrases[k]))[0]\n",
    "        for i in ids: id_to_canon[i] = c\n",
    "\n",
    "    cosine_map = {phrases[i]: phrases[id_to_canon[i]] for i in range(len(phrases))}\n",
    "    changed = sum(1 for k, v in cosine_map.items() if k != v)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for k, v in cosine_map.items():\n",
    "            if k != v: f.write(json.dumps({\"from\": k, \"to\": v}) + \"\\n\")\n",
    "    print(f\"Cosine dedupe map written ({changed} changes) â†’ {out_path}\")\n",
    "    return cosine_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6df41",
   "metadata": {},
   "source": [
    "## 5. Apply cosine map to Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15790605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cosine_map_to_parquet(in_path, out_path, mapping, list_col=\"NER_clean\"):\n",
    "    pf = pq.ParquetFile(in_path)\n",
    "    writer = None\n",
    "    for rg in range(pf.num_row_groups):\n",
    "        tbl = pf.read_row_group(rg)\n",
    "        df = tbl.to_pandas()\n",
    "        if list_col in df.columns:\n",
    "            df[list_col] = df[list_col].apply(\n",
    "                lambda lst: [mapping.get(str(x), str(x)) for x in (lst if isinstance(lst, (list, tuple, np.ndarray)) else [])]\n",
    "            )\n",
    "        table = pa.Table.from_pandas(df, preserve_index=False).replace_schema_metadata(None)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(out_path, table.schema, compression=\"zstd\")\n",
    "        writer.write_table(table)\n",
    "        del df, table; gc.collect()\n",
    "    if writer: writer.close()\n",
    "    print(f\"âœ… Wrote cosine-deduped file â†’ {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e80f9",
   "metadata": {},
   "source": [
    "## 6. Final summary printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeef0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pipeline(vocab, data_path=DATA_PATH):\n",
    "    print(f\"\"\"\n",
    "âœ… Ingredient Normalization Pipeline Complete\n",
    "--------------------------------------------\n",
    "Input CSV:     {data_path}\n",
    "Output (PMI):  {OUTPUT_PATH}\n",
    "Output (Spell):{OUTPUT_PATH_SPELL}\n",
    "Cosine Map:    ../data/cosine_dedupe_map.jsonl\n",
    "\n",
    "Vocabulary size: {len(vocab)} unique terms\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d6834",
   "metadata": {},
   "source": [
    "## 7.  RUN FULL PIPELINE (End-to-End Controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a957097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ingredient Normalization Pipeline...\n",
      "DEV MODE: sampling 10000 rows from raw data for faster iteration.\n",
      "Sample data written to: ..\\data\\sample_data.csv\n",
      "Data source: ..\\data\\sample_data.csv\n",
      "Output parquet will be written to: ..\\data\\recipes_data_clean_spell.parquet\n",
      "Ingesting CSV and computing n-gram statistics...\n",
      "Building canonical vocabulary...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m normalizer\u001b[38;5;241m.\u001b[39mbuild_vocab()\n\u001b[0;32m     71\u001b[0m SURFACE_MIN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m  \u001b[38;5;66;03m# tune 20â€“80 depending on corpus size\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m strong_surface \u001b[38;5;241m=\u001b[39m \u001b[43mfrequent_surface_phrases\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRAW_DATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mner_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNER_COL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_phrase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSURFACE_MIN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m pre_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(normalizer\u001b[38;5;241m.\u001b[39mcanon)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ph \u001b[38;5;129;01min\u001b[39;00m strong_surface:\n",
      "Cell \u001b[1;32mIn[15], line 146\u001b[0m, in \u001b[0;36mfrequent_surface_phrases\u001b[1;34m(csv_path, ner_col, chunksize, min_phrase)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mast\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m    145\u001b[0m seen \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path, chunksize\u001b[38;5;241m=\u001b[39mchunksize, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    147\u001b[0m     col \u001b[38;5;241m=\u001b[39m chunk[ner_col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m col:\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m   1984\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[1;32m-> 1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"Starting Ingredient Normalization Pipeline...\")\n",
    "\n",
    "# 0. DEV Env \n",
    "# Toggle between a small sample (for quick testing) and full dataset\n",
    "TESTING = True  # set to True for sample mode\n",
    "\n",
    "RESET_ALL = True  # set to True to reset all intermediate files\n",
    "if RESET_ALL:\n",
    "    # delete all jason/ jsonl and parquet files\n",
    "    for p in [VOCAB_JSON, OUTPUT_PATH, OUTPUT_PATH_SPELL,\n",
    "              Path(\"../data/ner_spell_map.jsonl\"),\n",
    "              Path(\"../data/cosine_dedupe_map.jsonl\"),\n",
    "              Path(\"../data/recipes_data_clean_spell_dedup.parquet\")]:\n",
    "        if p.exists():\n",
    "            p.unlink()\n",
    "            print(f\"Deleted: {p}\")\n",
    "\n",
    "\n",
    "if TESTING:\n",
    "    SAMPLE_SIZE = 10_000  # adjust as needed\n",
    "    print(f\"DEV MODE: sampling {SAMPLE_SIZE} rows from raw data for faster iteration.\")\n",
    "    df_sample = pd.read_csv(RAW_DATA_PATH, dtype=str).sample(\n",
    "        n=SAMPLE_SIZE, random_state=42\n",
    "    )\n",
    "    DATA_PATH = Path(\"../data/sample_data.csv\")\n",
    "    df_sample.to_csv(DATA_PATH, index=False)\n",
    "    path = DATA_PATH\n",
    "    print(f\"Sample data written to: {DATA_PATH}\")\n",
    "else:\n",
    "    print(\"FULL MODE: using full raw dataset.\")\n",
    "    path = RAW_DATA_PATH\n",
    "\n",
    "# Confirm paths\n",
    "print(f\"Data source: {path}\")\n",
    "print(f\"Output parquet will be written to: {OUTPUT_PATH_SPELL}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Instantiate and train StatsNormalizer ---\n",
    "normalizer = StatsNormalizer(\n",
    "    max_ngram=4,\n",
    "    min_unigram=MIN_UNIGRAM,\n",
    "    min_bigram=max(30, MIN_BIGRAM//1),      # a bit lower\n",
    "    min_trigram=max(20, MIN_TRIGRAM//1),\n",
    "    pmi_bigram=max(2.6, PMI_BIGRAM-0.4),\n",
    "    pmi_trigram=max(1.8, PMI_TRIGRAM-0.2),\n",
    "    min_child_share=0.06,                   # was 0.12\n",
    "    max_right_entropy=1.3,                  # was 1.0\n",
    "\n",
    "    # fallbacks\n",
    "    pmi_bigram_fallback=2.2,\n",
    "    min_bigram_fallback=15,\n",
    "    pmi_trigram_fallback=1.8,\n",
    "    min_trigram_fallback=10,\n",
    "    min_child_share_fallback=0.05,\n",
    "    max_right_entropy_fallback=1.5,\n",
    "\n",
    "    snap_score_cutoff=88,                   # was 92\n",
    "    snap_near_perfect=94                    # was 96\n",
    ")\n",
    "\n",
    "if not VOCAB_JSON.exists():\n",
    "    print(\"Ingesting CSV and computing n-gram statistics...\")\n",
    "    normalizer.ingest_csv(path, ner_col=NER_COL, chunksize=CHUNK_SIZE)\n",
    "    print(\"Building canonical vocabulary...\")\n",
    "    normalizer.build_vocab()\n",
    "    SURFACE_MIN = 40  # tune 20â€“80 depending on corpus size\n",
    "    strong_surface = frequent_surface_phrases(RAW_DATA_PATH, ner_col=NER_COL, min_phrase=SURFACE_MIN)\n",
    "    pre_len = len(normalizer.canon)\n",
    "    for ph in strong_surface:\n",
    "        toks = StatsNormalizer._tok(ph)\n",
    "        if 1 <= len(toks) <= normalizer.max_ngram:\n",
    "            normalizer.canon.add(tuple(toks))\n",
    "    normalizer._canon_ready = True  # ensure\n",
    "    print(f\"Surface-phrase canon added: {len(normalizer.canon)-pre_len}\")\n",
    "\n",
    "    normalizer.save_vocab(VOCAB_JSON)\n",
    "    print(f\"Saved vocab: {VOCAB_JSON}\")\n",
    "else:\n",
    "    print(\"Loading existing vocabulary...\")\n",
    "    normalizer = StatsNormalizer()\n",
    "    normalizer.canon = set(tuple(p.split()) for p in json.load(open(VOCAB_JSON))[\"canon\"])\n",
    "    normalizer._canon_ready = True\n",
    "\n",
    "# --- 2. Spell/Fuzzy Map ---\n",
    "SPELL_MAP_PATH = Path(\"../data/ner_spell_map.jsonl\")\n",
    "if not SPELL_MAP_PATH.exists():\n",
    "    print(\"Building spell/fuzzy map...\")\n",
    "    canon_phrases = [\" \".join(p) for p in normalizer.canon]\n",
    "    build_spell_map(canon_phrases, path, ner_col=NER_COL, out_path=SPELL_MAP_PATH)\n",
    "else:\n",
    "    print(\"Existing spell/fuzzy map found, skipping...\")\n",
    "\n",
    "# --- 3. Write cleaned + spellmapped parquet ---\n",
    "if not OUTPUT_PATH_SPELL.exists():\n",
    "    print(\"Transforming dataset using StatsNormalizer and spell map...\")\n",
    "    spell_map = {}\n",
    "    with open(SPELL_MAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            spell_map[obj[\"raw\"]] = obj[\"fixed\"]\n",
    "\n",
    "    normalizer.transform_csv_to_parquet(\n",
    "        csv_path=path,\n",
    "        out_path=OUTPUT_PATH_SPELL,\n",
    "        ner_col=NER_COL,\n",
    "        chunksize=CHUNK_SIZE\n",
    "    )\n",
    "    print(f\"Wrote cleaned (spell-mapped) file: {OUTPUT_PATH_SPELL}\")\n",
    "else:\n",
    "    print(\"Spellmapped Parquet already exists, skipping...\")\n",
    "\n",
    "# --- 4. Build vocab from cleaned Parquet ---\n",
    "print(\"Extracting vocabulary from cleaned Parquet...\")\n",
    "vocab = vocab_from_parquet(OUTPUT_PATH_SPELL)\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "# --- 5. Cosine Similarity Deduplication ---\n",
    "COS_MAP_PATH = Path(\"../data/cosine_dedupe_map.jsonl\")\n",
    "if not COS_MAP_PATH.exists():\n",
    "    print(\"Running cosine similarity deduplication...\")\n",
    "    cosine_map = cosine_dedupe(vocab, threshold=0.88, topk=20, out_path=COS_MAP_PATH)\n",
    "else:\n",
    "    print(\"Existing cosine map found, loading...\")\n",
    "    cosine_map = {}\n",
    "    with open(COS_MAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            cosine_map[obj[\"from\"]] = obj[\"to\"]\n",
    "\n",
    "# --- 6. Apply cosine dedup map ---\n",
    "FINAL_PATH = Path(\"../data/recipes_data_clean_spell_dedup.parquet\")\n",
    "if not FINAL_PATH.exists():\n",
    "    print(\"Applying cosine dedup map to Parquet...\")\n",
    "    apply_cosine_map_to_parquet(\n",
    "        in_path=OUTPUT_PATH_SPELL,\n",
    "        out_path=FINAL_PATH,\n",
    "        mapping=cosine_map,\n",
    "        list_col=\"NER_clean\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Deduped Parquet already exists, skipping...\")\n",
    "\n",
    "# --- 7. Summary ---\n",
    "summarize_pipeline(vocab, data_path=path)\n",
    "print(f\"Final deduped dataset: {FINAL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae060f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf95e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of final deduped dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NER</th>\n",
       "      <th>NER_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>509049</th>\n",
       "      <td>[\"oil\", \"sugar\", \"pepper\", \"tomato soup\", \"wor...</td>\n",
       "      <td>[oil, sugar, pepper, tomato soup, worcestershi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450745</th>\n",
       "      <td>[\"bacon\", \"sugar\", \"brown sugar\", \"onion\", \"be...</td>\n",
       "      <td>[bacon, sugar, brown sugar, onion, beans, ketc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543974</th>\n",
       "      <td>[\"lemon-lime\", \"sugar\", \"lemon juice\", \"water\"...</td>\n",
       "      <td>[lemon, lime, sugar, lemon juice, water, pinea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986308</th>\n",
       "      <td>[\"soy sauce\", \"oyster\", \"mirin\", \"red chile\", ...</td>\n",
       "      <td>[soy sauce, oyster, mirin, red chile, gingerro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894915</th>\n",
       "      <td>[\"egg\", \"pepper\", \"nutmeg\", \"brown sugar\", \"ra...</td>\n",
       "      <td>[egg, pepper, nutmeg, brown sugar, raisins, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746340</th>\n",
       "      <td>[\"sour cream\", \"pepper\", \"lemon juice\", \"green...</td>\n",
       "      <td>[sour cream, pepper, lemon juice, green onions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722102</th>\n",
       "      <td>[\"salt\", \"eggs\", \"leche\", \"flour\", \"milk\", \"co...</td>\n",
       "      <td>[salt, eggs, leche, flour, milk, confectioners...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349011</th>\n",
       "      <td>[\"sour cream\", \"chopped spinach\", \"onion soup\"]</td>\n",
       "      <td>[sour cream, chopped spinach, onion, soup]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133579</th>\n",
       "      <td>[\"frozen corn\", \"black beans\", \"tomatoes\", \"ch...</td>\n",
       "      <td>[frozen, corn, black beans, tomatoes, cheddar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458935</th>\n",
       "      <td>[\"sugar\", \"vanilla\", \"cream cheese\", \"milk\", \"...</td>\n",
       "      <td>[sugar, vanilla, cream, cheese, milk, pineapple]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       NER  \\\n",
       "509049   [\"oil\", \"sugar\", \"pepper\", \"tomato soup\", \"wor...   \n",
       "450745   [\"bacon\", \"sugar\", \"brown sugar\", \"onion\", \"be...   \n",
       "543974   [\"lemon-lime\", \"sugar\", \"lemon juice\", \"water\"...   \n",
       "1986308  [\"soy sauce\", \"oyster\", \"mirin\", \"red chile\", ...   \n",
       "894915   [\"egg\", \"pepper\", \"nutmeg\", \"brown sugar\", \"ra...   \n",
       "746340   [\"sour cream\", \"pepper\", \"lemon juice\", \"green...   \n",
       "1722102  [\"salt\", \"eggs\", \"leche\", \"flour\", \"milk\", \"co...   \n",
       "349011     [\"sour cream\", \"chopped spinach\", \"onion soup\"]   \n",
       "2133579  [\"frozen corn\", \"black beans\", \"tomatoes\", \"ch...   \n",
       "458935   [\"sugar\", \"vanilla\", \"cream cheese\", \"milk\", \"...   \n",
       "\n",
       "                                                 NER_clean  \n",
       "509049   [oil, sugar, pepper, tomato soup, worcestershi...  \n",
       "450745   [bacon, sugar, brown sugar, onion, beans, ketc...  \n",
       "543974   [lemon, lime, sugar, lemon juice, water, pinea...  \n",
       "1986308  [soy sauce, oyster, mirin, red chile, gingerro...  \n",
       "894915   [egg, pepper, nutmeg, brown sugar, raisins, ha...  \n",
       "746340   [sour cream, pepper, lemon juice, green onions...  \n",
       "1722102  [salt, eggs, leche, flour, milk, confectioners...  \n",
       "349011          [sour cream, chopped spinach, onion, soup]  \n",
       "2133579  [frozen, corn, black beans, tomatoes, cheddar ...  \n",
       "458935    [sugar, vanilla, cream, cheese, milk, pineapple]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35687 unmatched out of 0 canon terms\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load final deduped parquet and show sample\n",
    "df_final = pd.read_parquet(\"../data/normalized/recipes_data_clean_spell_dedup.parquet\")\n",
    "print(\"Sample of final deduped dataset:\")\n",
    "\n",
    "# #display the full NER/ NER_clean columns with no truncation\n",
    "# with pd.option_context('display.max_colwidth', None, 'display.max_rows', None):\n",
    "#     display(df_final[[NER_COL, \"NER_clean\"]].head(100))\n",
    "\n",
    "bad = df_final[df_final[\"NER_clean\"].apply(lambda lst: any(len(x.split())==1 for x in lst))].sample(10)\n",
    "display(bad[[\"NER\", \"NER_clean\"]])\n",
    "\n",
    "lens = [len(x) for x in df_final[\"NER_clean\"]]\n",
    "pd.Series(lens).hist(bins=50)\n",
    "normalizer = StatsNormalizer(\n",
    "    max_ngram=4,\n",
    "    min_unigram=MIN_UNIGRAM,\n",
    "    min_bigram=max(30, MIN_BIGRAM//1),      # a bit lower\n",
    "    min_trigram=max(20, MIN_TRIGRAM//1),\n",
    "    pmi_bigram=max(2.6, PMI_BIGRAM-0.4),\n",
    "    pmi_trigram=max(1.8, PMI_TRIGRAM-0.2),\n",
    "    min_child_share=0.06,                   # was 0.12\n",
    "    max_right_entropy=1.3,                  # was 1.0\n",
    "\n",
    "    # fallbacks\n",
    "    pmi_bigram_fallback=2.2,\n",
    "    min_bigram_fallback=15,\n",
    "    pmi_trigram_fallback=1.8,\n",
    "    min_trigram_fallback=10,\n",
    "    min_child_share_fallback=0.05,\n",
    "    max_right_entropy_fallback=1.5,\n",
    "\n",
    "    snap_score_cutoff=88,                   # was 92\n",
    "    snap_near_perfect=94                    # was 96\n",
    ")\n",
    "\n",
    "canon_vocab = {\" \".join(t) for t in normalizer.canon}\n",
    "unmatched = [tok for lst in df_final[\"NER_clean\"] for tok in lst if tok not in canon_vocab]\n",
    "print(f\"{len(set(unmatched))} unmatched out of {len(canon_vocab)} canon terms\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a0796c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canon coverage: 0.00%  (35687 unique unmatched / 35687 unique total)\n",
      "\n",
      "Top 50 unmatched tokens (by frequency):\n",
      "NER_clean\n",
      "salt               1007465\n",
      "sugar               658358\n",
      "onion               535339\n",
      "butter              521740\n",
      "flour               521089\n",
      "garlic              425517\n",
      "eggs                419762\n",
      "milk                404308\n",
      "pepper              390848\n",
      "water               343099\n",
      "vanilla             293044\n",
      "olive oil           217254\n",
      "cheese              182753\n",
      "brown sugar         182103\n",
      "red                 179897\n",
      "cream               179351\n",
      "egg                 178692\n",
      "tomatoes            176445\n",
      "baking powder       160918\n",
      "white               156705\n",
      "lemon juice         153945\n",
      "cinnamon            131401\n",
      "chicken             125445\n",
      "celery              125073\n",
      "parsley             125054\n",
      "sour cream          122449\n",
      "oil                 121408\n",
      "baking soda         120483\n",
      "onions              119754\n",
      "green               116072\n",
      "cheddar cheese      113802\n",
      "margarine           113549\n",
      "vegetable oil       101577\n",
      "fresh                99855\n",
      "potatoes             99613\n",
      "mayonnaise           97753\n",
      "rice                 93289\n",
      "vinegar              92295\n",
      "parmesan cheese      90855\n",
      "pecans               83037\n",
      "carrots              81278\n",
      "nuts                 80122\n",
      "mustard              79723\n",
      "pineapple            78496\n",
      "black pepper         76916\n",
      "soy sauce            76740\n",
      "bacon                75454\n",
      "oregano              75394\n",
      "thyme                74744\n",
      "mushrooms            74656\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "canon_vocab = {\" \".join(t) for t in normalizer.canon}\n",
    "\n",
    "emitted = (\n",
    "    pd.Series(df_final[\"NER_clean\"])\n",
    "      .explode()\n",
    "      .dropna().astype(str).str.strip()\n",
    ")\n",
    "\n",
    "unmatched = emitted[~emitted.isin(canon_vocab)]\n",
    "coverage = 1.0 - unmatched.size / emitted.size\n",
    "print(f\"Canon coverage: {coverage:.2%}  ({unmatched.nunique()} unique unmatched / {emitted.nunique()} unique total)\")\n",
    "\n",
    "print(\"\\nTop 50 unmatched tokens (by frequency):\")\n",
    "print(unmatched.value_counts().head(50))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
