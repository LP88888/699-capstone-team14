{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76418ad7",
   "metadata": {},
   "source": [
    "# Ingredient Normalization (Data-Driven, PMI-Based)\n",
    "\n",
    "## This notebook implements a **fully data-driven** normalization for ingredient phrases at scale (~2 M rows).\n",
    "\n",
    "-   1. Streams your dataset to count unigrams/bigrams/trigrams  \n",
    "-   2. Computes PMI-style association scores  \n",
    "-   3. Builds a canonical vocabulary (no hard-coded lists)  \n",
    "-   4. Segments each NER item by greedy longest-match using that vocabulary  \n",
    "-   5. Writes a cleaned column (`NER_clean`) back to disk\n",
    "\n",
    "### Designed for large CSVs: uses chunked ingestion and optional on-disk checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f440f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional installs (run once if missing)\n",
    "# !pip install pyarrow\n",
    "\n",
    "# # Update requirements.txt \n",
    "# !pip freeze > ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a58573",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "DATA_PATH = Path(\"../data/wilmerarltstrmberg_data.csv\")      # your raw CSV\n",
    "OUTPUT_PATH = Path(\"../data/recipes_data_clean.parquet\")  # output\n",
    "VOCAB_JSON = Path(\"../data/ingredient_vocab_stats.json\")\n",
    "NER_COL = \"NER\"\n",
    "CHUNK_SIZE = 200_000\n",
    "\n",
    "MIN_UNIGRAM = 50\n",
    "MIN_BIGRAM = 50\n",
    "MIN_TRIGRAM = 30\n",
    "PMI_BIGRAM = 3.5      # log odds threshold\n",
    "PMI_TRIGRAM = 2.0\n",
    "\n",
    "import ast, math, re, json, gc\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Iterable\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "_WORD_RE = re.compile(r\"[a-z']+\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a262b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ner_entry(entry) -> List[str]:\n",
    "    if entry is None or (isinstance(entry, float) and pd.isna(entry)):\n",
    "        return []\n",
    "    s = str(entry).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        parsed = ast.literal_eval(s)\n",
    "        if isinstance(parsed, list):\n",
    "            return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [x.strip() for x in s.split(',') if x.strip()]\n",
    "\n",
    "def tok(s: str) -> List[str]:\n",
    "    return _WORD_RE.findall(str(s).lower())\n",
    "\n",
    "def ngrams(tokens: List[str], n: int):\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        yield tuple(tokens[i:i+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "class SpellCorrector:\n",
    "    \"\"\"Word-level spell corrector + phrase-level fuzzy matcher on top of PMI canon.\"\"\"\n",
    "    def __init__(self, known_phrases, fuzzy_threshold=88):\n",
    "        # known_phrases = ['brown sugar', 'olive oil', ...]\n",
    "        self._canon = known_phrases\n",
    "        self._spell = SpellChecker(distance=2)\n",
    "        # prime spellchecker vocabulary\n",
    "        vocab_tokens = [t for p in known_phrases for t in p.split()]\n",
    "        self._spell.word_frequency.load_words(vocab_tokens)\n",
    "        self.fuzzy_threshold = fuzzy_threshold\n",
    "\n",
    "    def _spell_correct_phrase(self, text):\n",
    "        tokens = [self._spell.correction(t) or t for t in tok(text)]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def correct_and_match(self, raw_item):\n",
    "        \"\"\"Return best-matched canonical phrase and similarity score.\"\"\"\n",
    "        if not raw_item:\n",
    "            return \"\", None\n",
    "        corrected = self._spell_correct_phrase(raw_item)\n",
    "        match, score, _ = process.extractOne(\n",
    "            corrected, self._canon, scorer=fuzz.WRatio\n",
    "        )\n",
    "        if score >= self.fuzzy_threshold:\n",
    "            return match, int(score)\n",
    "        return corrected, int(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4026cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsNormalizer:\n",
    "    def __init__(self,\n",
    "                 max_ngram=3,\n",
    "                 min_unigram=50, min_bigram=50, min_trigram=30,\n",
    "                 pmi_bigram=3.5, pmi_trigram=2.0,\n",
    "                 min_child_share=0.12,          # NEW: keep trigram only if >= 12% of head\n",
    "                 max_right_entropy=1.0):        # NEW: keep trigram only if head's H_right <= 1.0\n",
    "        self.max_ngram = max_ngram\n",
    "        self.min_unigram = min_unigram\n",
    "        self.min_bigram = min_bigram\n",
    "        self.min_trigram = min_trigram\n",
    "        self.pmi_bigram = pmi_bigram\n",
    "        self.pmi_trigram = pmi_trigram\n",
    "        self.min_child_share = min_child_share\n",
    "        self.max_right_entropy = max_right_entropy\n",
    "         \n",
    "        from collections import defaultdict, Counter\n",
    "        self.c1, self.c2, self.c3 = Counter(), Counter(), Counter()\n",
    "        self.token_total = 0\n",
    "        self.canon, self._canon_ready = set(), False\n",
    "        self._followers = defaultdict(Counter)  \n",
    "\n",
    "    #  counting \n",
    "    def ingest_df(self, df, ner_col=\"NER\"):\n",
    "        for entry in df[ner_col]:\n",
    "            for item in parse_ner_entry(entry):\n",
    "                t = tok(item)\n",
    "                if not t: \n",
    "                    continue\n",
    "                self.c1.update(t)\n",
    "                self.token_total += len(t)\n",
    "                if self.max_ngram >= 2 and len(t) >= 2:\n",
    "                    self.c2.update(ngrams(t, 2))\n",
    "                if self.max_ngram >= 3 and len(t) >= 3:\n",
    "                    # count trigrams and follower distribution\n",
    "                    for i in range(len(t) - 2):\n",
    "                        a,b,c = t[i], t[i+1], t[i+2]\n",
    "                        self.c3[(a,b,c)] += 1\n",
    "                        self._followers[(a,b)][c] += 1\n",
    "\n",
    "    def _right_entropy(self, ab):\n",
    "        # ab is a tuple (a,b)\n",
    "        foll = self._followers.get(ab)\n",
    "        if not foll:\n",
    "            return 0.0\n",
    "        tot = sum(foll.values())\n",
    "        if tot == 0:\n",
    "            return 0.0\n",
    "        H = 0.0\n",
    "        for v in foll.values():\n",
    "            p = v / tot\n",
    "            H -= p * math.log(p + 1e-12)\n",
    "        return H\n",
    "\n",
    "    def _child_share(self, abc):\n",
    "        ab = abc[:2]\n",
    "        cabc = self.c3[abc]\n",
    "        cab = self.c2[ab]\n",
    "        if cab == 0:\n",
    "            return 0.0\n",
    "        return cabc / cab\n",
    "\n",
    "\n",
    "    def ingest_csv(self, csv_path, ner_col=\"NER\", chunksize=200_000):\n",
    "        for chunk in tqdm(pd.read_csv(csv_path, chunksize=chunksize, dtype=str), desc=\"Counting\"):\n",
    "            self.ingest_df(chunk, ner_col=ner_col)\n",
    "            del chunk; gc.collect()\n",
    "\n",
    "    #  PMI \n",
    "    def _pmi_bigram(self, ab):\n",
    "        a,b = ab\n",
    "        cab = self.c2[ab]\n",
    "        if cab==0 or self.token_total==0: return -1e9\n",
    "        pa,pb = self.c1[a]/self.token_total, self.c1[b]/self.token_total\n",
    "        pab = cab/self.token_total\n",
    "        return math.log((pab/(pa*pb))+1e-12)\n",
    "\n",
    "    def _pmi_trigram(self, abc):\n",
    "        a,b,c = abc\n",
    "        return (self._pmi_bigram((a,b))+self._pmi_bigram((b,c)))/2.0\n",
    "    \n",
    "    #  vocab \n",
    "    def build_vocab(self):\n",
    "        self.canon.clear()\n",
    "\n",
    "        # strong unigrams\n",
    "        for w,c in self.c1.items():\n",
    "            if c >= self.min_unigram:\n",
    "                self.canon.add((w,))\n",
    "\n",
    "        # strong bigrams\n",
    "        for ab,c in self.c2.items():\n",
    "            if c >= self.min_bigram and self._pmi_bigram(ab) >= self.pmi_bigram:\n",
    "                self.canon.add(ab)\n",
    "\n",
    "        # strong trigrams (apply PMI + child share + low branching entropy)\n",
    "        for abc,c in self.c3.items():\n",
    "            if c < self.min_trigram:\n",
    "                continue\n",
    "            if self._pmi_trigram(abc) < self.pmi_trigram:\n",
    "                continue\n",
    "            share = self._child_share(abc)\n",
    "            if share < self.min_child_share:\n",
    "                continue\n",
    "            H = self._right_entropy(abc[:2])\n",
    "            if H > self.max_right_entropy:\n",
    "                continue\n",
    "            self.canon.add(abc)\n",
    "\n",
    "        self._canon_ready = True\n",
    "\n",
    "    #  segmentation\n",
    "    def _longest_match(self,toks,i):\n",
    "        if not self._canon_ready:\n",
    "            raise RuntimeError(\"build_vocab() first\")\n",
    "        if i+2<len(toks) and tuple(toks[i:i+3]) in self.canon:\n",
    "            return tuple(toks[i:i+3]),3\n",
    "        if i+1<len(toks) and tuple(toks[i:i+2]) in self.canon:\n",
    "            return tuple(toks[i:i+2]),2\n",
    "        if (toks[i],) in self.canon:\n",
    "            return (toks[i],),1\n",
    "        return (toks[i],),1\n",
    "\n",
    "    def segment_item(self, text):\n",
    "        t = tok(text)\n",
    "        out, i = [], 0\n",
    "        while i < len(t):\n",
    "            phrase, k = self._longest_match(t, i)\n",
    "            out.append(\" \".join(phrase))\n",
    "            i += k\n",
    "\n",
    "        # dedupe\n",
    "        seen, clean = set(), []\n",
    "        for x in out:\n",
    "            if x not in seen:\n",
    "                clean.append(x); seen.add(x)\n",
    "\n",
    "        # drop immediate repetition of the tail of previous phrase\n",
    "        pruned = []\n",
    "        for j, x in enumerate(clean):\n",
    "            if pruned:\n",
    "                prev = pruned[-1].split()\n",
    "                if len(prev) >= 2 and x == prev[-1]:\n",
    "                    # e.g., [\"brown sugar\", \"sugar\"] -> drop \"sugar\"\n",
    "                    continue\n",
    "            pruned.append(x)\n",
    "        return pruned\n",
    "\n",
    "\n",
    "    #  transform\n",
    "    def transform_df(self, df, ner_col=\"NER\", out_col=\"NER_clean\"):\n",
    "        df[out_col] = [[seg for item in parse_ner_entry(v) for seg in self.segment_item(item)]\n",
    "                     for v in df[ner_col]]\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _sanitize_for_arrow(df: pd.DataFrame, list_col: str = \"NER_clean\") -> pd.DataFrame:\n",
    "        from pandas.api.types import (\n",
    "    is_datetime64_any_dtype, is_bool_dtype,\n",
    ")\n",
    "        import pandas as pd\n",
    "\n",
    "        \n",
    "        df = df.copy()\n",
    "\n",
    "        # ensure list[str]\n",
    "        if list_col in df.columns:\n",
    "            def _to_list_of_str(x):\n",
    "                if isinstance(x, (list, tuple)):\n",
    "                    return [str(y) for y in x]\n",
    "                if pd.isna(x) or x is None:\n",
    "                    return []\n",
    "                try:\n",
    "                    val = json.loads(x)\n",
    "                    if isinstance(val, list):\n",
    "                        return [str(y) for y in val]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                return [str(x)]\n",
    "            df[list_col] = df[list_col].apply(_to_list_of_str)\n",
    "\n",
    "        for col in df.columns:\n",
    "            if col == list_col:\n",
    "                continue\n",
    "            s = df[col]\n",
    "\n",
    "            try:\n",
    "                if isinstance(s.dtype, pd.PeriodDtype):\n",
    "                    df[col] = s.astype(\"string\")\n",
    "                    continue\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            if isinstance(s.dtype, pd.DatetimeTZDtype):\n",
    "\n",
    "                df[col] = pd.to_datetime(s, errors=\"coerce\").dt.tz_convert(\"UTC\").dt.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                continue\n",
    "\n",
    "            if isinstance(s.dtype, pd.PeriodDtype) and not isinstance(s.dtype, pd.DatetimeTZDtype):\n",
    "                df[col] = pd.to_datetime(s, errors=\"coerce\").dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "                continue\n",
    "\n",
    "            if isinstance(s.dtype, pd.BooleanDtype):\n",
    "                if s.isna().any():\n",
    "                    df[col] = s.fillna(False).astype(bool)\n",
    "                continue\n",
    "\n",
    "            if s.dtype == object:\n",
    "                def _to_scalar_str(v):\n",
    "                    if isinstance(v, (list, tuple, dict, set)):\n",
    "                        return json.dumps(v, ensure_ascii=False)\n",
    "                    return \"\" if v is None or (isinstance(v, float) and np.isnan(v)) else str(v)\n",
    "                df[col] = s.map(_to_scalar_str)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform_csv_to_parquet(self, csv_path, out_path, ner_col=\"NER\", chunksize=200_000,\n",
    "                                use_spellcheck=True, fuzzy_threshold=88):\n",
    "        import pyarrow as pa\n",
    "        import pyarrow.parquet as pq\n",
    "\n",
    "        # Build a spell-corrector from the current canon (if requested)\n",
    "        spell = None\n",
    "        if use_spellcheck:\n",
    "            canon_phrases = [\" \".join(p) for p in self.canon]\n",
    "            spell = SpellCorrector(canon_phrases, fuzzy_threshold=fuzzy_threshold)\n",
    "\n",
    "        writer = None\n",
    "\n",
    "        for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
    "            # 1) (optional) spell-check + fuzzy match each NER item to canon\n",
    "            if use_spellcheck and spell is not None:\n",
    "                spell_col = []\n",
    "                for entry in chunk[ner_col]:\n",
    "                    corrected = [spell.correct_and_match(item) for item in parse_ner_entry(entry)]\n",
    "                    spell_col.append(corrected)\n",
    "                chunk[\"NER_spellchecked\"] = spell_col\n",
    "                ner_for_seg = \"NER_spellchecked\"\n",
    "            else:\n",
    "                ner_for_seg = ner_col\n",
    "\n",
    "            # 2) segment using PMI canon\n",
    "            chunk = self.transform_df(chunk, ner_col=ner_for_seg, out_col=\"NER_clean\")\n",
    "\n",
    "            # 3) sanitize types for Arrow\n",
    "            chunk = self._sanitize_for_arrow(chunk, list_col=\"NER_clean\")\n",
    "\n",
    "            # 4) to Arrow, strip pandas metadata, force list<string> for NER_clean\n",
    "            table = pa.Table.from_pandas(chunk, preserve_index=False).replace_schema_metadata(None)\n",
    "            fields = []\n",
    "            for f in table.schema:\n",
    "                if f.name == \"NER_clean\" and not pa.types.is_list(f.type):\n",
    "                    fields.append(pa.field(\"NER_clean\", pa.list_(pa.string())))\n",
    "                else:\n",
    "                    fields.append(f)\n",
    "            target_schema = pa.schema(fields)\n",
    "            try:\n",
    "                table = table.cast(target_schema, safe=False)\n",
    "            except Exception:\n",
    "                arrays = [pa.array(arr, type=pa.list_(pa.string())) for arr in table.column(\"NER_clean\").to_pylist()]\n",
    "                table = table.set_column(table.schema.get_field_index(\"NER_clean\"), \"NER_clean\", pa.chunked_array(arrays))\n",
    "\n",
    "            # 5) write/append with a single ParquetWriter\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(out_path, target_schema, compression=\"zstd\")\n",
    "            writer.write_table(table)\n",
    "\n",
    "            del chunk, table\n",
    "            gc.collect()\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "\n",
    "    # save/load \n",
    "    def save_vocab(self,path):\n",
    "        data={\n",
    "            \"token_total\":self.token_total,\n",
    "            \"canon\":[\" \".join(p) for p in sorted(self.canon)]\n",
    "        }\n",
    "        path.parent.mkdir(parents=True,exist_ok=True)\n",
    "        with open(path,\"w\",encoding=\"utf-8\") as f: json.dump(data,f,indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load_vocab(cls,path):\n",
    "        data=json.load(open(path))\n",
    "        obj=cls()\n",
    "        obj.canon=set(tuple(p.split()) for p in data[\"canon\"])\n",
    "        obj._canon_ready=True\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fdb19a",
   "metadata": {},
   "source": [
    "## PASS 1: Count n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a220b77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting: 1it [00:05,  5.11s/it]\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m normalizer \u001b[38;5;241m=\u001b[39m StatsNormalizer(\n\u001b[0;32m      2\u001b[0m     max_ngram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      3\u001b[0m     min_unigram\u001b[38;5;241m=\u001b[39mMIN_UNIGRAM,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     max_right_entropy\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;66;03m# change to a larger value to keep more trigrams\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreaming counts from\u001b[39m\u001b[38;5;124m\"\u001b[39m, DATA_PATH)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mnormalizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mingest_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mner_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNER_COL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal tokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, normalizer\u001b[38;5;241m.\u001b[39mtoken_total)\n",
      "Cell \u001b[1;32mIn[6], line 65\u001b[0m, in \u001b[0;36mStatsNormalizer.ingest_csv\u001b[1;34m(self, csv_path, ner_col, chunksize)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mingest_csv\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_path, ner_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNER\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200_000\u001b[39m):\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m tqdm(pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path, chunksize\u001b[38;5;241m=\u001b[39mchunksize, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCounting\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mingest_df(chunk, ner_col\u001b[38;5;241m=\u001b[39mner_col)\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m chunk; gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m   1984\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[1;32m-> 1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "normalizer = StatsNormalizer(\n",
    "    max_ngram=3,\n",
    "    min_unigram=MIN_UNIGRAM,\n",
    "    min_bigram=MIN_BIGRAM,\n",
    "    min_trigram=MIN_TRIGRAM,\n",
    "    pmi_bigram=PMI_BIGRAM,\n",
    "    pmi_trigram=PMI_TRIGRAM,\n",
    "    min_child_share=0.01,   # change to a smaller value to keep more trigrams\n",
    "    max_right_entropy=1.0 # change to a larger value to keep more trigrams\n",
    ")\n",
    "print(\"Streaming counts from\", DATA_PATH)\n",
    "normalizer.ingest_csv(DATA_PATH, ner_col=NER_COL, chunksize=CHUNK_SIZE)\n",
    "print(\"Total tokens:\", normalizer.token_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe8ddc0",
   "metadata": {},
   "source": [
    "## PASS 2: Build vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768718de",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer.build_vocab()\n",
    "normalizer.save_vocab(VOCAB_JSON)\n",
    "print(\"Canonical phrases:\", len(normalizer.canon))\n",
    "print(\"Saved vocab:\", VOCAB_JSON)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a098a",
   "metadata": {},
   "source": [
    "## PASS 3: Segment & write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf4aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer.transform_csv_to_parquet(\n",
    "    csv_path=DATA_PATH,\n",
    "    out_path=OUTPUT_PATH,\n",
    "    ner_col=NER_COL,\n",
    "    chunksize=CHUNK_SIZE,\n",
    "    use_spellcheck=True,       \n",
    "    fuzzy_threshold=88\n",
    ")\n",
    "print(\"Wrote cleaned file:\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88547e2",
   "metadata": {},
   "source": [
    "## Example diagnostic: see top \"brown sugar ...\" trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a118a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load canon fast path\n",
    "loaded = StatsNormalizer.load_vocab(VOCAB_JSON)\n",
    "normalizer = StatsNormalizer(\n",
    "    max_ngram=3,\n",
    "    min_unigram=MIN_UNIGRAM,\n",
    "    min_bigram=MIN_BIGRAM,\n",
    "    min_trigram=MIN_TRIGRAM,\n",
    "    pmi_bigram=PMI_BIGRAM,\n",
    "    pmi_trigram=PMI_TRIGRAM,\n",
    "    min_child_share=0.01,\n",
    "    max_right_entropy=1.0\n",
    ")\n",
    "normalizer.canon = loaded.canon\n",
    "normalizer._canon_ready = True\n",
    "\n",
    "normalizer.transform_csv_to_parquet(\n",
    "    csv_path=DATA_PATH,\n",
    "    out_path=OUTPUT_PATH,\n",
    "    ner_col=NER_COL,\n",
    "    chunksize=CHUNK_SIZE,\n",
    "    use_spellcheck=True,\n",
    "    fuzzy_threshold=88\n",
    ")\n",
    "\n",
    "\n",
    "head = (\"brown\",\"sugar\")\n",
    "H = normalizer._right_entropy(head)\n",
    "tot = normalizer.c2[head]\n",
    "rows = []\n",
    "for (a,b,c), cnt in normalizer.c3.items():\n",
    "    if (a,b) == head:\n",
    "        share = cnt / max(1, tot)\n",
    "        rows.append((f\"{a} {b} {c}\", cnt, share))\n",
    "rows.sort(key=lambda x: -x[1])\n",
    "print(\"H_right(brown sugar) =\", H, \"total bigram count:\", tot)\n",
    "for s, cnt, share in rows[:20]:\n",
    "    print(f\"{s:<30} {cnt:>6}  share={share:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24276b91",
   "metadata": {},
   "source": [
    "## Quick check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfededf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check=pd.read_parquet(OUTPUT_PATH).head(10)\n",
    "df_check[[NER_COL,\"NER_clean\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
