{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e182d06",
   "metadata": {},
   "source": [
    "# Download file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afa51018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl\n",
    "# !pip freeze > ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9617dd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, json, time, shutil\n",
    "from typing import Iterable, List, Dict, Any, Optional\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Loads .env variables (e.g., SPOONACULAR_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80fe200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.abspath(\"../data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Choose the upstream API you want to use (\"spoonacular\" or \"themealdb\")\n",
    "API = os.getenv(\"RECIPE_API\", \"themealdb\").strip().lower()\n",
    "\n",
    "SPOONACULAR_API_KEY = os.getenv(\"SPOONACULAR_API_KEY\")  # put in .env\n",
    "CUISINES = [\n",
    "    \"African\",\"Asian\",\"American\",\"British\",\"Cajun\",\"Caribbean\",\"Chinese\",\n",
    "    \"Eastern European\",\"European\",\"French\",\"German\",\"Greek\",\"Indian\",\"Irish\",\n",
    "    \"Italian\",\"Japanese\",\"Jewish\",\"Korean\",\"Latin American\",\"Mediterranean\",\n",
    "    \"Mexican\",\"Middle Eastern\",\"Nordic\",\"Southern\",\"Spanish\",\"Thai\",\"Vietnamese\"\n",
    "]\n",
    "MEALDB_FIRST_LETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26751371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_kagglehub_dataset(dataset_name: str) -> None:\n",
    "    \"\"\"Remove cached kagglehub directory for a dataset (for a clean re-download).\"\"\"\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/kagglehub/datasets\")\n",
    "    dataset_dir = os.path.join(cache_dir, dataset_name.replace('/', os.sep))\n",
    "    if os.path.exists(dataset_dir):\n",
    "        shutil.rmtree(dataset_dir)\n",
    "        print(f\"Removed dataset cache: {dataset_dir}\")\n",
    "    else:\n",
    "        print(f\"No cache found for: {dataset_dir}\")\n",
    "\n",
    "def download_and_move_kaggle_file(\n",
    "    out_path: str,\n",
    "    kaggle_dataset: str,\n",
    "    kaggle_filename: str,\n",
    "    replace_cache: bool = False\n",
    ") -> bool:\n",
    "    \"\"\"Download a file from a Kaggle dataset (via kagglehub) and move to out_path.\"\"\"\n",
    "    import kagglehub\n",
    "\n",
    "    if replace_cache:\n",
    "        reset_kagglehub_dataset(kaggle_dataset)\n",
    "\n",
    "    kaggle_dir = kagglehub.dataset_download(kaggle_dataset)\n",
    "    src = os.path.join(kaggle_dir, kaggle_filename)\n",
    "\n",
    "    if not os.path.exists(src):\n",
    "        print(f\"[WARN] {kaggle_filename} not found in {kaggle_dir}\")\n",
    "        return False\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "        print(f\"Removed existing file: {out_path}\")\n",
    "\n",
    "    shutil.move(src, out_path)\n",
    "    print(f\"Saved {kaggle_filename} -> {out_path}\")\n",
    "    return True\n",
    "\n",
    "KAGGLE_PATHS = [\n",
    "    (\"alaknandaa/recipes-data-by-cuisine\", \"all_cuisines.xlsx\"),\n",
    "    (\"wilmerarltstrmberg/recipe-dataset-over-2m\", \"recipes_data.csv\"),\n",
    "    (\"snehallokesh31096/recipe\", \"recipes_82k.csv\"),\n",
    "    (\"mfarazf/cuisine-dataset\", \"New file.csv\"),\n",
    "    (\"sarthak71/food-recipes\", \"food_recipes.csv\"),\n",
    "    (\"ajitrajput/foodrecipes\", \"recipes.csv\"),\n",
    "]\n",
    "\n",
    "def safe_read_df(path: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        if path.lower().endswith(\".csv\"):\n",
    "            return pd.read_csv(path)\n",
    "        if path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "            return pd.read_excel(path)\n",
    "        print(f\"[WARN] Unsupported file type: {path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Reading {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_kaggle_sources(paths=KAGGLE_PATHS) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Download configured Kaggle files and return {nickname: DataFrame}.\"\"\"\n",
    "    dfs: Dict[str, pd.DataFrame] = {}\n",
    "    for dataset, filename in paths:\n",
    "        nick = dataset.replace(\"/\", \"__\")\n",
    "        out_path = os.path.join(DATA_DIR, f\"{nick}.{filename.split('.')[-1]}\")\n",
    "\n",
    "        try:\n",
    "            ok = download_and_move_kaggle_file(\n",
    "                out_path, kaggle_dataset=dataset, kaggle_filename=filename, replace_cache=True\n",
    "            )\n",
    "            if not ok:\n",
    "                continue\n",
    "            df = safe_read_df(out_path)\n",
    "            if df is not None and not df.empty:\n",
    "                dfs[nick] = df\n",
    "                display(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Processing {dataset}: {e}\")\n",
    "            continue\n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12941c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed dataset cache: C:\\Users\\georg.DESKTOP-2FS9VF1/.cache/kagglehub/datasets\\alaknandaa\\recipes-data-by-cuisine\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/alaknandaa/recipes-data-by-cuisine?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.28M/3.28M [00:01<00:00, 2.97MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all_cuisines.xlsx -> c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\data\\alaknandaa__recipes-data-by-cuisine.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>total_time</th>\n",
       "      <th>serving_size</th>\n",
       "      <th>ingr</th>\n",
       "      <th>instructions</th>\n",
       "      <th>nativeCuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jamaican Fried Dumplings</td>\n",
       "      <td>20</td>\n",
       "      <td>6 servings</td>\n",
       "      <td>4 cups all-purpose flour</td>\n",
       "      <td>In a large bowl, stir together the flour, baki...</td>\n",
       "      <td>carribean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jamaican Fried Dumplings</td>\n",
       "      <td>20</td>\n",
       "      <td>6 servings</td>\n",
       "      <td>2 teaspoons baking powder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>carribean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jamaican Fried Dumplings</td>\n",
       "      <td>20</td>\n",
       "      <td>6 servings</td>\n",
       "      <td>1 ½ teaspoons salt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>carribean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jamaican Fried Dumplings</td>\n",
       "      <td>20</td>\n",
       "      <td>6 servings</td>\n",
       "      <td>½ cup butter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>carribean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jamaican Fried Dumplings</td>\n",
       "      <td>20</td>\n",
       "      <td>6 servings</td>\n",
       "      <td>½ cup cold water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>carribean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title  total_time serving_size  \\\n",
       "0  Jamaican Fried Dumplings          20   6 servings   \n",
       "1  Jamaican Fried Dumplings          20   6 servings   \n",
       "2  Jamaican Fried Dumplings          20   6 servings   \n",
       "3  Jamaican Fried Dumplings          20   6 servings   \n",
       "4  Jamaican Fried Dumplings          20   6 servings   \n",
       "\n",
       "                        ingr  \\\n",
       "0   4 cups all-purpose flour   \n",
       "1  2 teaspoons baking powder   \n",
       "2         1 ½ teaspoons salt   \n",
       "3               ½ cup butter   \n",
       "4           ½ cup cold water   \n",
       "\n",
       "                                        instructions nativeCuisine  \n",
       "0  In a large bowl, stir together the flour, baki...     carribean  \n",
       "1                                                NaN     carribean  \n",
       "2                                                NaN     carribean  \n",
       "3                                                NaN     carribean  \n",
       "4                                                NaN     carribean  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed dataset cache: C:\\Users\\georg.DESKTOP-2FS9VF1/.cache/kagglehub/datasets\\wilmerarltstrmberg\\recipe-dataset-over-2m\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/wilmerarltstrmberg/recipe-dataset-over-2m?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 107M/635M [00:27<02:15, 4.10MB/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m kaggle_frames \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_kaggle_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 68\u001b[0m, in \u001b[0;36mdownload_kaggle_sources\u001b[1;34m(paths)\u001b[0m\n\u001b[0;32m     65\u001b[0m out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnick\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     ok \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_and_move_kaggle_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkaggle_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkaggle_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 23\u001b[0m, in \u001b[0;36mdownload_and_move_kaggle_file\u001b[1;34m(out_path, kaggle_dataset, kaggle_filename, replace_cache)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m replace_cache:\n\u001b[0;32m     21\u001b[0m     reset_kagglehub_dataset(kaggle_dataset)\n\u001b[1;32m---> 23\u001b[0m kaggle_dir \u001b[38;5;241m=\u001b[39m \u001b[43mkagglehub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkaggle_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m src \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(kaggle_dir, kaggle_filename)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(src):\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\kagglehub\\datasets.py:43\u001b[0m, in \u001b[0;36mdataset_download\u001b[1;34m(handle, path, force_download)\u001b[0m\n\u001b[0;32m     41\u001b[0m h \u001b[38;5;241m=\u001b[39m parse_dataset_handle(handle)\n\u001b[0;32m     42\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;241m.\u001b[39mto_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXTRA_CONSOLE_BLOCK})\n\u001b[1;32m---> 43\u001b[0m path, _ \u001b[38;5;241m=\u001b[39m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\kagglehub\\registry.py:28\u001b[0m, in \u001b[0;36mMultiImplRegistry.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m         fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\kagglehub\\resolver.py:29\u001b[0m, in \u001b[0;36mResolver.__call__\u001b[1;34m(self, handle, path, force_download)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m, handle: T, path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, force_download: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resolves a handle into a path with the requested file(s) and the resource's version number.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m        Some cases where version number might be missing: Competition datasource, API-based models.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     path, version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     register_datasource_access(handle, version)\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\kagglehub\\http_resolver.py:130\u001b[0m, in \u001b[0;36mDatasetHttpResolver._resolve\u001b[1;34m(self, h, path, force_download)\u001b[0m\n\u001b[0;32m    127\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(archive_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# First, we download the archive.\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m \u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m _extract_archive(archive_path, out_path)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Delete the archive\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\kagglehub\\clients.py:217\u001b[0m, in \u001b[0;36mKaggleApiV1Client.download_file\u001b[1;34m(self, path, out_file, resource_handle, cached_path, extract_auto_compressed_file)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 217\u001b[0m     \u001b[43m_download_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hash_object:\n\u001b[0;32m    220\u001b[0m     actual_md5_hash \u001b[38;5;241m=\u001b[39m to_b64_digest(hash_object)\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\kagglehub\\clients.py:276\u001b[0m, in \u001b[0;36m_download_file\u001b[1;34m(response, out_file, size_read, total_size, hash_object)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mtotal_size, initial\u001b[38;5;241m=\u001b[39msize_read, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, unit_divisor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_file, open_mode) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 276\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(CHUNK_SIZE):\n\u001b[0;32m    277\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[0;32m    278\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m hash_object:\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\urllib3\\response.py:1091\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1090\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1091\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1094\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\urllib3\\response.py:980\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 980\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\urllib3\\response.py:904\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    901\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    906\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\urllib3\\response.py:887\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kaggle_frames = download_kaggle_sources()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29280287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b095fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spoonacular_complex_search_ids(api_key: str, cuisine: str, number: int = 100) -> List[int]:\n",
    "    \"\"\"Collect recipe IDs from Spoonacular complexSearch for a given cuisine.\"\"\"\n",
    "    url = \"https://api.spoonacular.com/recipes/complexSearch\"\n",
    "    params = {\"cuisine\": cuisine.lower(), \"number\": number, \"apiKey\": api_key}\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=20)\n",
    "        if not r.ok:\n",
    "            print(f\"[Spoonacular] {r.status_code} for cuisine={cuisine}: {(r.text or '')[:180]}\")\n",
    "            return []\n",
    "        data = r.json()\n",
    "        return [it[\"id\"] for it in data.get(\"results\", []) if \"id\" in it]\n",
    "    except Exception as e:\n",
    "        print(f\"[Spoonacular] Error for cuisine={cuisine}: {e}\")\n",
    "        return []\n",
    "\n",
    "def spoonacular_information_bulk(\n",
    "    api_key: str, ids: Iterable[int], chunk_size: int = 100, pause: float = 0.25, max_retries: int = 3\n",
    ") -> List[dict]:\n",
    "    \"\"\"Fetch recipe details in chunks from Spoonacular /recipes/informationBulk.\"\"\"\n",
    "    url = \"https://api.spoonacular.com/recipes/informationBulk\"\n",
    "    ids = list(map(int, ids))\n",
    "    out: List[dict] = []\n",
    "\n",
    "    for i in range(0, len(ids), chunk_size):\n",
    "        batch = ids[i:i + chunk_size]\n",
    "        params = {\"ids\": \",\".join(map(str, batch)), \"apiKey\": api_key}\n",
    "\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                resp = requests.get(url, params=params, timeout=30)\n",
    "                if resp.status_code == 429:\n",
    "                    # Rate limit: linear backoff\n",
    "                    time.sleep(pause * attempt + 0.5)\n",
    "                    continue\n",
    "                if not resp.ok:\n",
    "                    preview = (resp.text or \"\")[:200].replace(\"\\n\", \" \")\n",
    "                    print(f\"[Bulk {i//chunk_size}] HTTP {resp.status_code} {preview}\")\n",
    "                    break  # don't retry non-429 errors by default\n",
    "                try:\n",
    "                    data = resp.json()\n",
    "                except ValueError:\n",
    "                    print(f\"[Bulk {i//chunk_size}] Non-JSON response\")\n",
    "                    break\n",
    "\n",
    "                if isinstance(data, list):\n",
    "                    out.extend(data)\n",
    "                elif isinstance(data, dict) and \"recipes\" in data and isinstance(data[\"recipes\"], list):\n",
    "                    out.extend(data[\"recipes\"])\n",
    "                else:\n",
    "                    print(f\"[Bulk {i//chunk_size}] Unexpected JSON type={type(data)}\")\n",
    "                break  # success -> exit retry loop\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries:\n",
    "                    print(f\"[Bulk {i//chunk_size}] Failed after {attempt} attempts: {e}\")\n",
    "                else:\n",
    "                    time.sleep(pause * attempt)\n",
    "        time.sleep(pause)\n",
    "    return out\n",
    "\n",
    "def themealdb_search_by_first_letter(letter: str) -> List[dict]:\n",
    "    \"\"\"Pull meals whose names start with `letter` from TheMealDB.\"\"\"\n",
    "    url = f\"https://www.themealdb.com/api/json/v1/1/search.php\"\n",
    "    try:\n",
    "        resp = requests.get(url, params={\"f\": letter}, timeout=20)\n",
    "        if not resp.ok:\n",
    "            print(f\"[TheMealDB] {resp.status_code} for f={letter}\")\n",
    "            return []\n",
    "        data = resp.json()\n",
    "        return data.get(\"meals\") or []\n",
    "    except Exception as e:\n",
    "        print(f\"[TheMealDB] Error for f={letter}: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_upstream_recipes() -> List[dict]:\n",
    "    \"\"\"Main orchestrator for the selected API.\"\"\"\n",
    "    if API == \"spoonacular\":\n",
    "        if not SPOONACULAR_API_KEY:\n",
    "            print(\"[WARN] SPOONACULAR_API_KEY missing; skipping Spoonacular pull.\")\n",
    "            return []\n",
    "        # 1) collect IDs per cuisine\n",
    "        all_ids: List[int] = []\n",
    "        for c in CUISINES:\n",
    "            all_ids.extend(spoonacular_complex_search_ids(SPOONACULAR_API_KEY, c))\n",
    "        all_ids = sorted(set(all_ids))\n",
    "        print(f\"Total unique Spoonacular IDs: {len(all_ids)}\")\n",
    "\n",
    "        # 2) bulk fetch details\n",
    "        return spoonacular_information_bulk(SPOONACULAR_API_KEY, all_ids)\n",
    "\n",
    "    elif API == \"themealdb\":\n",
    "        payload: List[dict] = []\n",
    "        for ch in MEALDB_FIRST_LETTERS:\n",
    "            payload.extend(themealdb_search_by_first_letter(ch))\n",
    "        print(f\"Total TheMealDB recipes: {len(payload)}\")\n",
    "        return payload\n",
    "\n",
    "    else:\n",
    "        print(f\"[WARN] Unsupported API: {API}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c2f1f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total TheMealDB recipes: 319\n",
      "Saved API JSON -> c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\data\\recipe_api_data.json\n",
      "Saved API CSV  -> c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\data\\recipe_api_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Run: API pull \n",
    "api_recipes = fetch_upstream_recipes()\n",
    "\n",
    "#  Save API results \n",
    "api_json_path = os.path.join(DATA_DIR, \"recipe_api_data.json\")\n",
    "api_csv_path = os.path.join(DATA_DIR, \"recipe_api_data.csv\")\n",
    "\n",
    "with open(api_json_path, \"w\") as f:\n",
    "    json.dump(api_recipes, f)\n",
    "\n",
    "pd.DataFrame(api_recipes).to_csv(api_csv_path, index=False)\n",
    "print(f\"Saved API JSON -> {api_json_path}\")\n",
    "print(f\"Saved API CSV  -> {api_csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c243b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Quick peek \n",
    "if api_recipes:\n",
    "    display(pd.DataFrame(api_recipes).head())\n",
    "\n",
    "for nick, df in kaggle_frames.items():\n",
    "    print(f\"{nick}: {len(df):,} rows, {len(df.columns)} cols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a6b7a6",
   "metadata": {},
   "source": [
    "## Composition\n",
    "\n",
    "We've got different datasets that we have access to - the usual approach would be to consolidate each table into a single large dataframe. Is this the best solution? It depends on our scope. In theory we can create a map, stick it in a DB and call it for info on each table.\n",
    "\n",
    "If we are only interested in matching ingredients (not time spent, etc.) we can simply do something similar to this:\n",
    "\n",
    "\n",
    "\n",
    "| Dataset ID    | Index | Encodings                   |\n",
    "|---------------|-------|-----------------------------|\n",
    "| 1             | 3     | [123,421,2,28479]           |\n",
    "| 2             | 7     | [98, 204, 17, 3902]         |\n",
    "| 3             | 12    | [56, 789, 34, 1201]         |\n",
    "| 4             | 5     | [301, 22, 88, 4500]         |\n",
    "| 5             | 9     | [77, 333, 19, 8765]         |\n",
    "| 6             | 2     | [210, 654, 31, 9999]        |\n",
    "| 7             | 8     | [44, 555, 23, 1234]         |\n",
    "\n",
    "We can then just grab the entire row and get the full information.\n",
    "\n",
    "We can go even further and pretty much automate the process: By running a model that identifies which column is most likely the ingredients tab, we can in theory add any dataset with minimal work on our side.\n",
    "\n",
    "This process may also be replicable for any of the columns, adding additional features etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
