{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610cd950",
   "metadata": {},
   "source": [
    "# Multi-dataset normalizer/encoder\n",
    "\n",
    "\n",
    "## 1. Load normalizer + maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e86834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Iterable, Dict, List, Optional\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "UNK_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022f287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_list_of_str(x) -> List[str]:\n",
    "    \"\"\"Coerce various stored formats (list/tuple/np.ndarray/strified list) to list[str].\"\"\"\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple, np.ndarray)):\n",
    "        return [str(t).strip() for t in x if str(t).strip()]\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # leave strings as-is (already-normalized phrases) â€“ do not re-segment here\n",
    "    # if you have comma-joined strings in some datasets, you can split by comma here\n",
    "    return [s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b306a0a",
   "metadata": {},
   "source": [
    "## IngredientEncoder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b750dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IngredientEncoder:\n",
    "    def __init__(self, token_to_id: Optional[Dict[str, int]] = None):\n",
    "        if token_to_id is None:\n",
    "            token_to_id = {UNK_TOKEN: UNK_ID}\n",
    "        self.token_to_id: Dict[str, int] = dict(token_to_id)\n",
    "        self.id_to_token: Dict[int, str] = {v: k for k, v in self.token_to_id.items()}\n",
    "\n",
    "    # ---- fitting / vocab building ----\n",
    "    def fit_from_series(self, listcol: Iterable, min_freq: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Build/extend the vocab from a Series of lists (e.g., df['NER_clean']).\n",
    "        Tokens below min_freq are ignored (remain OOV -> 0).\n",
    "        \"\"\"\n",
    "        freq: Dict[str, int] = {}\n",
    "        for entry in listcol:\n",
    "            for tok in _ensure_list_of_str(entry):\n",
    "                freq[tok] = freq.get(tok, 0) + 1\n",
    "\n",
    "        # deterministic order: by (-freq, token) so mappings are reproducible\n",
    "        items = sorted([(t, c) for t, c in freq.items() if c >= min_freq],\n",
    "                       key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "        # assign ids starting after existing\n",
    "        next_id = max(self.id_to_token.keys(), default=UNK_ID) + 1\n",
    "        for tok, _ in items:\n",
    "            if tok not in self.token_to_id:\n",
    "                self.token_to_id[tok] = next_id\n",
    "                self.id_to_token[next_id] = tok\n",
    "                next_id += 1\n",
    "\n",
    "    def fit_from_parquet(self, path: Path, col: str = \"NER_clean\", min_freq: int = 1) -> None:\n",
    "        pf = pq.ParquetFile(path)\n",
    "        # stream row-groups\n",
    "        for rg in range(pf.num_row_groups):\n",
    "            df = pf.read_row_group(rg, columns=[col]).to_pandas()\n",
    "            self.fit_from_series(df[col], min_freq=min_freq)\n",
    "\n",
    "    # ---- transform / encode ----\n",
    "    def encode_list(self, tokens: Iterable[str]) -> List[int]:\n",
    "        out: List[int] = []\n",
    "        for tok in _ensure_list_of_str(tokens):\n",
    "            out.append(self.token_to_id.get(tok, UNK_ID))\n",
    "        return out or [UNK_ID]\n",
    "\n",
    "    def transform_series_to_idlists(self, listcol: Iterable) -> List[List[int]]:\n",
    "        return [self.encode_list(x) for x in listcol]\n",
    "\n",
    "    def transform_df(self, df: pd.DataFrame, ingredients_col: str = \"NER_clean\",\n",
    "                     dataset_id: int = 1) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns a DataFrame with: Dataset ID | Index | Ingredients (list[int])\n",
    "        \"\"\"\n",
    "        enc = self.transform_series_to_idlists(df[ingredients_col])\n",
    "        out = pd.DataFrame({\n",
    "            \"Dataset ID\": dataset_id,\n",
    "            \"Index\": np.arange(len(df), dtype=np.int64),\n",
    "            \"Ingredients\": enc\n",
    "        })\n",
    "        return out\n",
    "\n",
    "    # ---- persistence ----\n",
    "    def save_maps(self, id_to_token_path: Path, token_to_id_path: Path) -> None:\n",
    "        id_to_token_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(id_to_token_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            # write as {id: token}\n",
    "            json.dump({int(i): t for i, t in self.id_to_token.items()},\n",
    "                      f, ensure_ascii=False, indent=2)\n",
    "        with open(token_to_id_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            # write as {token: id}\n",
    "            json.dump({t: int(i) for t, i in self.token_to_id.items()},\n",
    "                      f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load_maps(cls, id_to_token_path: Path = None, token_to_id_path: Path = None):\n",
    "        if id_to_token_path and id_to_token_path.exists():\n",
    "            id_to_token = json.load(open(id_to_token_path, \"r\", encoding=\"utf-8\"))\n",
    "            # keys may come back as strings; coerce\n",
    "            id_to_token = {int(k): v for k, v in id_to_token.items()}\n",
    "            token_to_id = {v: k for k, v in id_to_token.items()}\n",
    "            return cls(token_to_id=token_to_id)\n",
    "        elif token_to_id_path and token_to_id_path.exists():\n",
    "            token_to_id = json.load(open(token_to_id_path, \"r\", encoding=\"utf-8\"))\n",
    "            # values may come back as strings; coerce\n",
    "            token_to_id = {k: int(v) for k, v in token_to_id.items()}\n",
    "            return cls(token_to_id=token_to_id)\n",
    "        else:\n",
    "            return cls()  # empty (only <UNK>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73e750a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] Failed to open local file '../data/recipes_data_clean_spell.parquet'. Detail: [Windows error 2] The system cannot find the file specified.\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 1) Fit encoder on the sample parquet\u001b[39;00m\n\u001b[0;32m     12\u001b[0m enc \u001b[38;5;241m=\u001b[39m IngredientEncoder()\n\u001b[1;32m---> 13\u001b[0m \u001b[43menc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_from_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPARQUET_FOR_VOCAB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mING_COL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 2) Transform the same parquet to ID lists\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#    (You can also stream row-groups if the file is large; here we do a single read for the sample)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m df_sample \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(PARQUET_FOR_VOCAB, columns\u001b[38;5;241m=\u001b[39m[ING_COL])\n",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m, in \u001b[0;36mIngredientEncoder.fit_from_parquet\u001b[1;34m(self, path, col, min_freq)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_from_parquet\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: Path, col: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNER_clean\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_freq: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     pf \u001b[38;5;241m=\u001b[39m \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParquetFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# stream row-groups\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pf\u001b[38;5;241m.\u001b[39mnum_row_groups):\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pyarrow\\parquet\\core.py:324\u001b[0m, in \u001b[0;36mParquetFile.__init__\u001b[1;34m(self, source, metadata, common_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, filesystem, page_checksum_verification, arrow_extensions_enabled)\u001b[0m\n\u001b[0;32m    321\u001b[0m filesystem, source \u001b[38;5;241m=\u001b[39m _resolve_filesystem_and_path(\n\u001b[0;32m    322\u001b[0m     source, filesystem, memory_map\u001b[38;5;241m=\u001b[39mmemory_map)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filesystem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_input_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# We opened it here, ensure we close it.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader \u001b[38;5;241m=\u001b[39m ParquetReader()\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pyarrow\\_fs.pyx:815\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.open_input_file\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Failed to open local file '../data/recipes_data_clean_spell.parquet'. Detail: [Windows error 2] The system cannot find the file specified.\r\n"
     ]
    }
   ],
   "source": [
    "PARQUET_FOR_VOCAB = Path(\"../data/recipes_data_clean_spell.parquet\")  # or the baseline parquet you prefer\n",
    "ING_COL = \"NER_clean\"\n",
    "\n",
    "OUT_DIR = Path(\"../data/multiset\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "UNIFIED_PARQUET = OUT_DIR / \"datasets_unified.parquet\"\n",
    "UNIFIED_CSV     = OUT_DIR / \"datasets_unified.csv\"\n",
    "REF_ING_JSON    = OUT_DIR / \"ingredient_id_to_token.json\"   # id -> token\n",
    "REF_ING_REV_JSON= OUT_DIR / \"ingredient_token_to_id.json\"   # token -> id\n",
    "\n",
    "# 1) Fit encoder on the sample parquet\n",
    "enc = IngredientEncoder()\n",
    "enc.fit_from_parquet(PARQUET_FOR_VOCAB, col=ING_COL, min_freq=1)\n",
    "\n",
    "# 2) Transform the same parquet to ID lists\n",
    "#    (You can also stream row-groups if the file is large; here we do a single read for the sample)\n",
    "df_sample = pd.read_parquet(PARQUET_FOR_VOCAB, columns=[ING_COL])\n",
    "encoded_df = enc.transform_df(df_sample, ingredients_col=ING_COL, dataset_id=1)\n",
    "\n",
    "# 3) Save outputs\n",
    "# Parquet with list<int64>\n",
    "ingredients_array = pa.array(\n",
    "    [(lst if isinstance(lst, (list, tuple, np.ndarray)) else []) for lst in encoded_df[\"Ingredients\"]],\n",
    "    type=pa.list_(pa.int64())\n",
    ")\n",
    "table = pa.Table.from_arrays(\n",
    "    [\n",
    "        pa.array(encoded_df[\"Dataset ID\"].to_numpy(dtype=np.int32)),\n",
    "        pa.array(encoded_df[\"Index\"].to_numpy(dtype=np.int64)),\n",
    "        ingredients_array\n",
    "    ],\n",
    "    names=[\"Dataset ID\", \"Index\", \"Ingredients\"],\n",
    ")\n",
    "pq.write_table(table, UNIFIED_PARQUET, compression=\"zstd\")\n",
    "\n",
    "# CSV fallback (lists stringified)\n",
    "encoded_df.to_csv(UNIFIED_CSV, index=False)\n",
    "\n",
    "# Reference maps (true inverses)\n",
    "enc.save_maps(REF_ING_JSON, REF_ING_REV_JSON)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(f\"Encoded rows: {len(encoded_df)}\")\n",
    "print(f\"Vocab size (incl <UNK>): {len(enc.token_to_id)}\")\n",
    "print(f\"Written:\\n  {UNIFIED_PARQUET}\\n  {UNIFIED_CSV}\\n  {REF_ING_JSON}\\n  {REF_ING_REV_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac693be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset ID</th>\n",
       "      <th>Index</th>\n",
       "      <th>Ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[39, 57, 238, 154, 14, 738, 86, 1900, 2545, 164]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[353, 352, 12, 563, 165, 6, 9, 126, 1499, 46, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[3, 14, 32, 314]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[15, 16, 2, 244, 50, 9, 4, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[2966, 22]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dataset ID  Index                                        Ingredients\n",
       "0           1      0   [39, 57, 238, 154, 14, 738, 86, 1900, 2545, 164]\n",
       "1           1      1  [353, 352, 12, 563, 165, 6, 9, 126, 1499, 46, ...\n",
       "2           1      2                                   [3, 14, 32, 314]\n",
       "3           1      3                      [15, 16, 2, 244, 50, 9, 4, 1]\n",
       "4           1      4                                         [2966, 22]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check head of UNIFIED_PARQUET\n",
    "pq.read_table(UNIFIED_PARQUET).to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76320ab",
   "metadata": {},
   "source": [
    "## Streaming & Inference helpers for IngredientEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "523a46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def fit_encoder_from_parquet_streaming(\n",
    "    encoder,\n",
    "    parquet_path: Path,\n",
    "    col: str = \"NER_clean\",\n",
    "    min_freq: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream row-groups from a Parquet file to build/extend the encoder vocab.\n",
    "    \"\"\"\n",
    "    pf = pq.ParquetFile(parquet_path)\n",
    "    for rg in range(pf.num_row_groups):\n",
    "        df_rg = pf.read_row_group(rg, columns=[col]).to_pandas()\n",
    "        encoder.fit_from_series(df_rg[col], min_freq=min_freq)\n",
    "        del df_rg\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def encode_parquet_streaming(\n",
    "    encoder,\n",
    "    parquet_path: Path,\n",
    "    out_parquet_path: Path,\n",
    "    dataset_id: int = 1,\n",
    "    col: str = \"NER_clean\",\n",
    "    compression: str = \"zstd\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream-encode a large Parquet file row-group by row-group into a unified Parquet\n",
    "    with schema: [\"Dataset ID\" (int32), \"Index\" (int64), \"Ingredients\" (list<int64>)].\n",
    "    \"\"\"\n",
    "    pf = pq.ParquetFile(parquet_path)\n",
    "\n",
    "    # Prepare Arrow schema up-front\n",
    "    target_schema = pa.schema([\n",
    "        pa.field(\"Dataset ID\", pa.int32()),\n",
    "        pa.field(\"Index\", pa.int64()),\n",
    "        pa.field(\"Ingredients\", pa.list_(pa.int64())),\n",
    "    ])\n",
    "\n",
    "    # Create the writer once\n",
    "    out_parquet_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    writer = pq.ParquetWriter(out_parquet_path, target_schema, compression=compression)\n",
    "\n",
    "    global_index_start = 0\n",
    "    for rg in range(pf.num_row_groups):\n",
    "        # Pull only required column\n",
    "        df_rg = pf.read_row_group(rg, columns=[col]).to_pandas()\n",
    "\n",
    "        # Encode this chunk\n",
    "        id_lists = encoder.transform_series_to_idlists(df_rg[col])\n",
    "\n",
    "        # Build Arrow arrays\n",
    "        ds_ids = pa.array(np.full(len(id_lists), dataset_id, dtype=np.int32))\n",
    "        idxs   = pa.array(np.arange(global_index_start, global_index_start + len(id_lists), dtype=np.int64))\n",
    "        ingr   = pa.array(\n",
    "            [(lst if isinstance(lst, (list, tuple, np.ndarray)) else []) for lst in id_lists],\n",
    "            type=pa.list_(pa.int64())\n",
    "        )\n",
    "        tbl = pa.Table.from_arrays([ds_ids, idxs, ingr],\n",
    "                                   names=[\"Dataset ID\", \"Index\", \"Ingredients\"])\n",
    "\n",
    "        writer.write_table(tbl)\n",
    "        global_index_start += len(id_lists)\n",
    "\n",
    "        del df_rg, id_lists, ds_ids, idxs, ingr, tbl\n",
    "\n",
    "    writer.close()\n",
    "    return out_parquet_path\n",
    "\n",
    "\n",
    "def encode_dataframe_inference(\n",
    "    encoder,\n",
    "    df: pd.DataFrame,\n",
    "    ingredients_col: str = \"NER_clean\",\n",
    "    dataset_id: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode a small in-memory DataFrame (inference batch).\n",
    "    Returns DataFrame: [\"Dataset ID\", \"Index\", \"Ingredients\"].\n",
    "    \"\"\"\n",
    "    return encoder.transform_df(df, ingredients_col=ingredients_col, dataset_id=dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec5363",
   "metadata": {},
   "source": [
    "## For large datasets, use streaming functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b408e0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] Failed to open local file '../data/recipes_data_clean_spell.parquet'. Detail: [Windows error 2] The system cannot find the file specified.\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m enc \u001b[38;5;241m=\u001b[39m IngredientEncoder()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 1) Build vocab by streaming (or load existing maps if you want stable IDs)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mfit_encoder_from_parquet_streaming\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/recipes_data_clean_spell.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNER_clean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Optional: persist maps for later reuse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m enc\u001b[38;5;241m.\u001b[39msave_maps(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/multiset/ingredient_id_to_token.json\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      8\u001b[0m               Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/multiset/ingredient_token_to_id.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m, in \u001b[0;36mfit_encoder_from_parquet_streaming\u001b[1;34m(encoder, parquet_path, col, min_freq)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_encoder_from_parquet_streaming\u001b[39m(\n\u001b[0;32m      8\u001b[0m     encoder,\n\u001b[0;32m      9\u001b[0m     parquet_path: Path,\n\u001b[0;32m     10\u001b[0m     col: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNER_clean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     min_freq: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     12\u001b[0m ):\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    Stream row-groups from a Parquet file to build/extend the encoder vocab.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     pf \u001b[38;5;241m=\u001b[39m \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParquetFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pf\u001b[38;5;241m.\u001b[39mnum_row_groups):\n\u001b[0;32m     18\u001b[0m         df_rg \u001b[38;5;241m=\u001b[39m pf\u001b[38;5;241m.\u001b[39mread_row_group(rg, columns\u001b[38;5;241m=\u001b[39m[col])\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pyarrow\\parquet\\core.py:324\u001b[0m, in \u001b[0;36mParquetFile.__init__\u001b[1;34m(self, source, metadata, common_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, filesystem, page_checksum_verification, arrow_extensions_enabled)\u001b[0m\n\u001b[0;32m    321\u001b[0m filesystem, source \u001b[38;5;241m=\u001b[39m _resolve_filesystem_and_path(\n\u001b[0;32m    322\u001b[0m     source, filesystem, memory_map\u001b[38;5;241m=\u001b[39mmemory_map)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filesystem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_input_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# We opened it here, ensure we close it.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader \u001b[38;5;241m=\u001b[39m ParquetReader()\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pyarrow\\_fs.pyx:815\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.open_input_file\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Failed to open local file '../data/recipes_data_clean_spell.parquet'. Detail: [Windows error 2] The system cannot find the file specified.\r\n"
     ]
    }
   ],
   "source": [
    "enc = IngredientEncoder()\n",
    "\n",
    "# 1) Build vocab by streaming (or load existing maps if you want stable IDs)\n",
    "fit_encoder_from_parquet_streaming(enc, Path(\"../data/recipes_data_clean_spell.parquet\"), col=\"NER_clean\", min_freq=1)\n",
    "\n",
    "# Optional: persist maps for later reuse\n",
    "enc.save_maps(Path(\"../data/multiset/ingredient_id_to_token.json\"),\n",
    "              Path(\"../data/multiset/ingredient_token_to_id.json\"))\n",
    "\n",
    "# 2) Encode to unified Parquet, streaming row-groups\n",
    "encode_parquet_streaming(\n",
    "    enc,\n",
    "    parquet_path=Path(\"../data/recipes_data_clean_spell.parquet\"),\n",
    "    out_parquet_path=Path(\"../data/multiset/datasets_unified.parquet\"),\n",
    "    dataset_id=1,\n",
    "    col=\"NER_clean\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e59c1",
   "metadata": {},
   "source": [
    "## For smaller datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaba7f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset ID</th>\n",
       "      <th>Index</th>\n",
       "      <th>Ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[7, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[10, 11, 12, 13, 6, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[15, 7, 16, 17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[18, 3, 19, 20, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dataset ID  Index              Ingredients\n",
       "0           1      0       [1, 2, 3, 4, 5, 6]\n",
       "1           1      1                [7, 8, 9]\n",
       "2           1      2  [10, 11, 12, 13, 6, 14]\n",
       "3           1      3          [15, 7, 16, 17]\n",
       "4           1      4       [18, 3, 19, 20, 6]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open up datasets_unified.parquet to check\n",
    "pq.read_table(\"../data/encoded/datasets_unified.parquet\").to_pandas().head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
