# Ingredient NER Inference Configuration
# This config is used by apply_ingredient_ner.py for running inference on new datasets

# Model path (trained NER model)
model:
  model_dir: "./models/ingredient_ner_trf/model-best"  # Path to trained spaCy model

# Input data settings
data:
  # Default input path (can be overridden via --in-path CLI argument)
  input_path: null  # Set to a path if you want a default, or leave null to require --in-path
  data_is_parquet: true  # Auto-detected from file extension if not set

# Output settings
output:
  # Base path for output files (can be overridden via --out-base CLI argument)
  # Will write <base>_wide.parquet and <base>_tall.parquet
  out_base: "./data/inference_output"

# Paths to artifacts created by ingrnorm pipeline
# These are used for normalization and encoding
artifacts:
  # Dedupe map: maps variant phrases → canonical forms
  cosine_map_path: "./data/normalized/cosine_dedupe_map.jsonl"
  # Encoder maps: maps canonical tokens ↔ integer IDs
  ingredient_id_to_token: "./data/encoded/ingredient_id_to_token.json"
  ingredient_token_to_id: "./data/encoded/ingredient_token_to_id.json"

# Inference settings
inference:
  # Default text column name (can be overridden via --text-col CLI argument)
  text_col: null  # Must be specified via CLI or set here
  
  # Performance settings
  batch_size: 256  # Batch size for spaCy processing
  n_process: 1     # Number of processes (keep 1 for GPU/transformers, >1 may not work on Windows)
  use_gpu: true   # Attempt to use GPU (default: CPU for reliability)
  
  # Sampling defaults (can be overridden via CLI)
  sample_n: null      # Randomly sample N rows
  sample_frac: null   # Randomly sample fraction (0.0-1.0)
  head_n: null        # Take first N rows

logging:
  level: INFO
  console: true
  file: preprocess_pipeline/logs/ingredient_ner_inference.log
  rotate:
    max_bytes: 10485760
    backup_count: 5
  fmt: "[%(asctime)s] %(levelname)s %(name)s: %(message)s"
  datefmt: "%Y-%m-%d %H:%M:%S"

