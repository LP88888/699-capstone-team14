# pipeline/config.yaml

# 1. GLOBAL SETTINGS
global:
  base_dir: "./data"
  logging_level: "INFO"

# 2. INGESTION (Combines CSVs -> One Parquet)
# This replaces "combine_raw_datasets.py"
ingestion:
  enabled: true
  input_dir: "./data/raw"
  output_file: "./data/intermediate/combined_raw.parquet"
  # Rename your messy CSV columns to standard names here
  column_mapping:
    "NER": "ingredients_raw"       # The list-like column
    "ingredients": "ingredients_text" # The string column (if needed)
    "cuisine": "cuisine_raw"

# 3. PROCESSING TASKS (Run sequentially)
tasks:
  # --- TASK A: PROCESS INGREDIENTS ---
  - name: "ingredients_pipeline"
    enabled: true
    input_path: "./data/intermediate/combined_raw.parquet"
    output_path: "./data/processed/ingredients_encoded.parquet"
    
    target_column: "ingredients_raw"   # Read this
    output_column: "ingredients_clean" # Create/Write this
    
    steps:
      - name: "normalization"
        type: "spacy"
        params:
          model: "en_core_web_sm"
          batch_size: 1024
          
      - name: "deduplication"
        type: "sbert"
        params:
          model: "all-MiniLM-L6-v2"
          threshold: 0.88            # 0.88 is good for ingredients
          require_token_overlap: true
          
      - name: "encoding"
        type: "encoder"
        params:
          vocab_file: "./data/encoded/ingredient_token_to_id.json"
          min_freq: 2

  # --- TASK B: PROCESS CUISINE ---
  - name: "cuisine_pipeline"
    enabled: true
    input_path: "./data/intermediate/combined_raw.parquet"
    output_path: "./data/processed/cuisine_encoded.parquet"
    
    target_column: "cuisine_raw"
    output_column: "cuisine_clean"
    
    steps:
      - name: "split_lists"
        type: "list_splitter"        # Handles "[Italian, American]"
        
      - name: "normalization"
        type: "spacy"
        params:
          model: "en_core_web_sm"
          
      - name: "deduplication"
        type: "sbert"
        params:
          model: "all-MiniLM-L6-v2"
          threshold: 0.92            # Higher threshold for cuisine (less variation)
          
      - name: "encoding"
        type: "encoder"
        params:
          vocab_file: "./data/encoded/cuisine_token_to_id.json"
          min_freq: 1

# 4. TRAINING TASKS (A list of models to train)
training_tasks:

  # TASK 1: INGREDIENT NER (Token Classification) 
  - name: "ingredient_ner_model"
    enabled: true
    # This trains the NER model using the raw ingredient text.
    task_type: "token_classification" 
    
    # Input/Output paths
    input_path: "./data/intermediate/combined_raw.parquet"
    target_column: "ingredients_text"     # Input text column
    label_column: "ingredients_raw"       # Label/NER list column
    model_dir: "./models/ingredient_ner_trf/model-best"
    
    params:
      base_model: "distilbert-base-uncased"
      epochs: 20
      batch_size: 256
      valid_fraction: 0.2
      # ... (rest of the NER params) ...

  # TASK 2: CUISINE CLASSIFICATION 
  - name: "cuisine_classification_model"
    enabled: true
    # This trains a model to classify the cuisine based on some feature text.
    task_type: "text_classification"
    
    # Input/Output paths
    # Use the encoded dataset which contains the clean labels
    input_path: "./data/processed/cuisine_encoded.parquet" 
    text_column: "ingredients_text"       # Feature text (e.g., Recipe Text)
    label_column: "cuisine_clean"         # Target label column from Task B
    model_dir: "./models/cuisine_cls_trf/model-best"
    
    params:
      base_model: "roberta-base"          # Use a strong model for classification
      epochs: 10
      batch_size: 64                      # Smaller batch size often better for classification
      valid_fraction: 0.2
      learning_rate: 5e-5