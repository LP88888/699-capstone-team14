{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unified Preprocessing Pipeline\n",
        "\n",
        "This notebook demonstrates the task-based pipeline that processes multiple tasks sequentially.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The task-based pipeline (`run_pipeline.py`) integrates:\n",
        "1. **Data Ingestion**: Combines CSVs/Parquets from a directory into a unified dataset\n",
        "2. **Processing Tasks**: Each task processes a column (e.g., ingredients, cuisine) through normalization, deduplication, and encoding\n",
        "3. **Training Tasks**: Trains models (e.g., NER, classification) from processed data\n",
        "\n",
        "## Task-Based Architecture\n",
        "\n",
        "### Processing Tasks\n",
        "Each task defines:\n",
        "- `input_path`: Source data file\n",
        "- `output_path`: Final processed output\n",
        "- `target_column`: Input column to process\n",
        "- `output_column`: Output column name\n",
        "- `steps`: List of processing steps (spacy, sbert, encoder, etc.)\n",
        "\n",
        "### Training Tasks\n",
        "Each training task defines:\n",
        "- `task_type`: Type of model (token_classification, text_classification)\n",
        "- `input_path`: Training data file\n",
        "- `model_dir`: Where to save the trained model\n",
        "- `params`: Training hyperparameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Add pipeline to path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "# set sys.path to the parent directory\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded config from: ..\\pipeline\\config\\config.yaml\n",
            "\n",
            "Global Settings:\n",
            "  Base dir: ./data\n",
            "  Logging level: INFO\n",
            "\n",
            "Ingestion:\n",
            "  Enabled: True\n",
            "  Input dir: ./data/raw\n",
            "  Output file: ./data/intermediate/combined_raw.parquet\n",
            "\n",
            "Processing Tasks (2):\n",
            "  - ingredients_pipeline (enabled)\n",
            "    Input: ./data/intermediate/combined_raw.parquet\n",
            "    Output: ./data/processed/ingredients_encoded.parquet\n",
            "    Column: ingredients_raw -> ingredients_clean\n",
            "    Steps: 3\n",
            "  - cuisine_pipeline (enabled)\n",
            "    Input: ./data/intermediate/combined_raw.parquet\n",
            "    Output: ./data/processed/cuisine_encoded.parquet\n",
            "    Column: cuisine_raw -> cuisine_clean\n",
            "    Steps: 4\n",
            "\n",
            "Training Tasks (2):\n",
            "  - ingredient_ner_model (enabled)\n",
            "    Type: token_classification\n",
            "    Input: ./data/intermediate/combined_raw.parquet\n",
            "    Model dir: ./models/ingredient_ner_trf/model-best\n",
            "  - cuisine_classification_model (enabled)\n",
            "    Type: text_classification\n",
            "    Input: ./data/processed/cuisine_encoded.parquet\n",
            "    Model dir: ./models/cuisine_cls_trf/model-best\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pipeline.config import PipelineConfig\n",
        "from pathlib import Path\n",
        "\n",
        "# Load configuration\n",
        "config_path = Path(\"../pipeline/config/config.yaml\")\n",
        "if not config_path.exists():\n",
        "    print(f\"Config not found at {config_path}\")\n",
        "    print(\"Please ensure the config file exists.\")\n",
        "else:\n",
        "    config = PipelineConfig.from_yaml(config_path)\n",
        "    print(f\"Loaded config from: {config_path}\")\n",
        "    print(f\"\\nGlobal Settings:\")\n",
        "    if config.global_settings:\n",
        "        print(f\"  Base dir: {config.global_settings.base_dir}\")\n",
        "        print(f\"  Logging level: {config.global_settings.logging_level}\")\n",
        "    \n",
        "    print(f\"\\nIngestion:\")\n",
        "    if config.ingestion:\n",
        "        print(f\"  Enabled: {config.ingestion.enabled}\")\n",
        "        print(f\"  Input dir: {config.ingestion.input_dir}\")\n",
        "        print(f\"  Output file: {config.ingestion.output_file}\")\n",
        "    \n",
        "    print(f\"\\nProcessing Tasks ({len(config.tasks)}):\")\n",
        "    for task in config.tasks:\n",
        "        print(f\"  - {task.name} ({'enabled' if task.enabled else 'disabled'})\")\n",
        "        print(f\"    Input: {task.input_path}\")\n",
        "        print(f\"    Output: {task.output_path}\")\n",
        "        print(f\"    Column: {task.target_column} -> {task.output_column}\")\n",
        "        print(f\"    Steps: {len(task.steps)}\")\n",
        "    \n",
        "    print(f\"\\nTraining Tasks ({len(config.training_tasks)}):\")\n",
        "    for task in config.training_tasks:\n",
        "        print(f\"  - {task.name} ({'enabled' if task.enabled else 'disabled'})\")\n",
        "        print(f\"    Type: {task.task_type}\")\n",
        "        print(f\"    Input: {task.input_path}\")\n",
        "        print(f\"    Model dir: {task.model_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Run Unified Pipeline\n",
        "\n",
        "The unified pipeline can run in different modes. Configure the mode and execute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
            "  warnings.warn(\n",
            "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
            "  warnings.warn(\n",
            "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
            "  warnings.warn(\n",
            "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
            "  warnings.warn(\n",
            "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task-Based Pipeline Configuration:\n",
            "  Ingestion: Enabled\n",
            "  Processing tasks: 2 enabled\n",
            "  Training tasks: 2 enabled\n",
            "\n",
            "Ingestion:\n",
            "  Input: ./data/raw\n",
            "  Output: ./data/intermediate/combined_raw.parquet\n",
            "\n",
            "Processing Tasks:\n",
            "  - ingredients_pipeline: ./data/intermediate/combined_raw.parquet -> ./data/processed/ingredients_encoded.parquet\n",
            "  - cuisine_pipeline: ./data/intermediate/combined_raw.parquet -> ./data/processed/cuisine_encoded.parquet\n",
            "\n",
            "Training Tasks:\n",
            "  - ingredient_ner_model: ./data/intermediate/combined_raw.parquet -> ./models/ingredient_ner_trf/model-best\n",
            "  - cuisine_classification_model: ./data/processed/cuisine_encoded.parquet -> ./models/cuisine_cls_trf/model-best\n"
          ]
        }
      ],
      "source": [
        "from pipeline.task_orchestrator import TaskBasedOrchestrator\n",
        "from pipeline.common.logging_setup import setup_logging\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "setup_logging(config.to_dict())\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Display configuration summary\n",
        "print(\"Task-Based Pipeline Configuration:\")\n",
        "print(f\"  Ingestion: {'Enabled' if config.ingestion and config.ingestion.enabled else 'Disabled'}\")\n",
        "print(f\"  Processing tasks: {len([t for t in config.tasks if t.enabled])} enabled\")\n",
        "print(f\"  Training tasks: {len([t for t in config.training_tasks if t.enabled])} enabled\")\n",
        "\n",
        "if config.ingestion:\n",
        "    print(f\"\\nIngestion:\")\n",
        "    print(f\"  Input: {config.ingestion.input_dir}\")\n",
        "    print(f\"  Output: {config.ingestion.output_file}\")\n",
        "\n",
        "if config.tasks:\n",
        "    print(f\"\\nProcessing Tasks:\")\n",
        "    for task in config.tasks:\n",
        "        if task.enabled:\n",
        "            print(f\"  - {task.name}: {task.input_path} -> {task.output_path}\")\n",
        "\n",
        "if config.training_tasks:\n",
        "    print(f\"\\nTraining Tasks:\")\n",
        "    for task in config.training_tasks:\n",
        "        if task.enabled:\n",
        "            print(f\"  - {task.name}: {task.input_path} -> {task.model_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-11-25 16:17:32] INFO pipeline.task_orchestrator.TaskOrchestrator: Setting up task-based pipeline steps...\n",
            "[2025-11-25 16:17:32] INFO pipeline.task_orchestrator.TaskOrchestrator: Setting up ingestion: ./data/raw -> ./data/intermediate/combined_raw.parquet\n",
            "[2025-11-25 16:17:32] INFO pipeline.base.RawDataCombiner: Initialized with data_dir=data\\raw\n",
            "[2025-11-25 16:17:32] INFO pipeline.task_orchestrator.TaskOrchestrator: Setting up task: ingredients_pipeline\n",
            "[2025-11-25 16:17:32] INFO pipeline.task_orchestrator.TaskOrchestrator:   Input: ./data/intermediate/combined_raw.parquet\n",
            "[2025-11-25 16:17:32] INFO pipeline.task_orchestrator.TaskOrchestrator:   Output: ./data/processed/ingredients_encoded.parquet\n",
            "[2025-11-25 16:17:32] INFO pipeline.task_orchestrator.TaskOrchestrator:   Target column: ingredients_raw -> ingredients_clean\n",
            "[2025-11-25 16:17:32] INFO pipeline.task_orchestrator.TaskOrchestrator: Creating step 'normalization' (type: spacy)\n",
            "[2025-11-25 16:17:33] INFO pipeline.base.IngredientNormalizer: Initialized with model=en_core_web_sm, batch_size=1024, n_process=1\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator: Creating step 'deduplication' (type: sbert)\n",
            "[2025-11-25 16:17:33] INFO pipeline.base.DeduplicationStep_SBERT: Initialized with method=sbert\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator: Creating step 'encoding' (type: encoder)\n",
            "[2025-11-25 16:17:33] INFO pipeline.base.EncodingStep: Initialized with min_freq=2, dataset_id=1, col=ingredients_clean\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator: Setting up task: cuisine_pipeline\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator:   Input: ./data/intermediate/combined_raw.parquet\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator:   Output: ./data/processed/cuisine_encoded.parquet\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator:   Target column: cuisine_raw -> cuisine_clean\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator: Creating step 'split_lists' (type: list_splitter)\n",
            "[2025-11-25 16:17:33] INFO pipeline.base.CuisinePreprocessing: Initialized with cuisine_col=cuisine_raw\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator: Creating step 'normalization' (type: spacy)\n",
            "[2025-11-25 16:17:33] INFO pipeline.base.IngredientNormalizer: Initialized with model=en_core_web_sm, batch_size=512, n_process=1\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator: Creating step 'deduplication' (type: sbert)\n",
            "[2025-11-25 16:17:33] INFO pipeline.base.DeduplicationStep_SBERT: Initialized with method=sbert\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator: Creating step 'encoding' (type: encoder)\n",
            "[2025-11-25 16:17:33] INFO pipeline.base.EncodingStep: Initialized with min_freq=1, dataset_id=1, col=cuisine_clean\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator: Setting up training task: ingredient_ner_model\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator:   Task type: token_classification\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator:   Input: ./data/intermediate/combined_raw.parquet\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator:   Model output: ./models/ingredient_ner_trf/model-best\n",
            "[2025-11-25 16:17:33] INFO pipeline.base.NERModelTrainer: Initialized with target_column=ingredients_raw, base_model=distilbert-base-uncased, epochs=20\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator: Setting up training task: cuisine_classification_model\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator:   Task type: text_classification\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator:   Input: ./data/processed/cuisine_encoded.parquet\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator:   Model output: ./models/cuisine_cls_trf/model-best\n",
            "[2025-11-25 16:17:33] WARNING pipeline.task_orchestrator.TaskOrchestrator: Training task 'cuisine_classification_model' uses text_classification which is not yet supported by NERModelTrainerStep, skipping\n",
            "[2025-11-25 16:17:33] INFO pipeline.task_orchestrator.TaskOrchestrator: Setup complete: 9 step(s) configured\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configured 9 step(s):\n",
            "  1. RawDataCombiner\n",
            "      Input: data\\raw\n",
            "      Output: data\\intermediate\\combined_raw.parquet\n",
            "  2. IngredientNormalizer\n",
            "      Input: data\\intermediate\\combined_raw.parquet\n",
            "      Output: data\\processed\\ingredients_pipeline_step_0_normalization.parquet\n",
            "  3. DeduplicationStep_SBERT\n",
            "      Input: data\\processed\\ingredients_pipeline_step_0_normalization.parquet\n",
            "      Output: data\\processed\\ingredients_pipeline_step_1_deduplication.parquet\n",
            "  4. EncodingStep\n",
            "      Input: data\\processed\\ingredients_pipeline_step_1_deduplication.parquet\n",
            "      Output: data\\processed\\ingredients_encoded.parquet\n",
            "  5. CuisinePreprocessing\n",
            "      Input: data\\intermediate\\combined_raw.parquet\n",
            "      Output: data\\processed\\cuisine_pipeline_step_0_split_lists.parquet\n",
            "  6. IngredientNormalizer\n",
            "      Input: data\\processed\\cuisine_pipeline_step_0_split_lists.parquet\n",
            "      Output: data\\processed\\cuisine_pipeline_step_1_normalization.parquet\n",
            "  7. DeduplicationStep_SBERT\n",
            "      Input: data\\processed\\cuisine_pipeline_step_1_normalization.parquet\n",
            "      Output: data\\processed\\cuisine_pipeline_step_2_deduplication.parquet\n",
            "  8. EncodingStep\n",
            "      Input: data\\processed\\cuisine_pipeline_step_2_deduplication.parquet\n",
            "      Output: data\\processed\\cuisine_encoded.parquet\n",
            "  9. NERModelTrainer\n",
            "      Input: data\\intermediate\\combined_raw.parquet\n",
            "      Output: models\\ingredient_ner_trf\\model-best\n"
          ]
        }
      ],
      "source": [
        "# Create orchestrator and setup steps\n",
        "orchestrator = TaskBasedOrchestrator(config)\n",
        "orchestrator.setup_steps()\n",
        "\n",
        "print(f\"Configured {len(orchestrator.steps)} step(s):\")\n",
        "for i, step in enumerate(orchestrator.steps, 1):\n",
        "    print(f\"  {i}. {step.name}\")\n",
        "    if hasattr(step, 'input_path') and step.input_path:\n",
        "        print(f\"      Input: {step.input_path}\")\n",
        "    if hasattr(step, 'output_path') and step.output_path:\n",
        "        print(f\"      Output: {step.output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline configured. Uncomment orchestrator.run() above to execute.\n"
          ]
        }
      ],
      "source": [
        "# Execute pipeline (uncomment to run)\n",
        "# orchestrator.run(force=False)\n",
        "\n",
        "# Or run with force to rebuild all artifacts:\n",
        "# orchestrator.run(force=True)\n",
        "\n",
        "print(\"Pipeline configured. Uncomment orchestrator.run() above to execute.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking output files from processing tasks:\n",
            "\n",
            "✗ ingredients_pipeline: data\\processed\\ingredients_encoded.parquet (not found)\n",
            "\n",
            "✗ cuisine_pipeline: data\\processed\\cuisine_encoded.parquet (not found)\n",
            "\n",
            "\n",
            "Checking training model outputs:\n",
            "\n",
            "✗ ingredient_ner_model: models\\ingredient_ner_trf\\model-best (not found)\n",
            "✗ cuisine_classification_model: models\\cuisine_cls_trf\\model-best (not found)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check output files from processing tasks\n",
        "print(\"Checking output files from processing tasks:\\n\")\n",
        "for task in config.tasks:\n",
        "    if task.enabled:\n",
        "        output_path = Path(task.output_path)\n",
        "        if output_path.exists():\n",
        "            df = pd.read_parquet(output_path)\n",
        "            print(f\"✓ {task.name}: {output_path}\")\n",
        "            print(f\"  Rows: {len(df):,}\")\n",
        "            print(f\"  Columns: {list(df.columns)}\")\n",
        "            print(f\"  Sample data:\")\n",
        "            print(f\"    {task.output_column}: {df[task.output_column].iloc[0] if task.output_column in df.columns and len(df) > 0 else 'N/A'}\")\n",
        "            print()\n",
        "        else:\n",
        "            print(f\"✗ {task.name}: {output_path} (not found)\")\n",
        "            print()\n",
        "\n",
        "# Check training model outputs\n",
        "if config.training_tasks:\n",
        "    print(\"\\nChecking training model outputs:\\n\")\n",
        "    for task in config.training_tasks:\n",
        "        if task.enabled:\n",
        "            model_dir = Path(task.model_dir)\n",
        "            if model_dir.exists() and any(model_dir.iterdir()):\n",
        "                print(f\"✓ {task.name}: {model_dir} (exists)\")\n",
        "            else:\n",
        "                print(f\"✗ {task.name}: {model_dir} (not found)\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
