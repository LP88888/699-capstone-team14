{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unified Preprocessing Pipeline\n",
        "\n",
        "This notebook demonstrates the unified pipeline that supports multiple modes:\n",
        "- **train**: Train NER model from raw data\n",
        "- **inference**: Run inference and normalization pipeline\n",
        "- **full**: Train model then run inference pipeline\n",
        "\n",
        "## Overview\n",
        "\n",
        "The unified pipeline (`run_pipeline.py`) integrates:\n",
        "1. **Raw Data Combination**: Ingests CSVs/Parquets from a directory\n",
        "2. **Model Training**: Trains transformer-based NER model (optional)\n",
        "3. **Inference & Normalization**: Processes columns through normalization, deduplication, and encoding\n",
        "\n",
        "## Pipeline Modes\n",
        "\n",
        "### Train Mode\n",
        "- Combines raw datasets\n",
        "- Trains NER model from labeled data\n",
        "- Saves model to `config.pipeline.model_dir`\n",
        "\n",
        "### Inference Mode\n",
        "- Loads existing model (or uses pre-trained)\n",
        "- Runs inference on input data\n",
        "- Applies normalization, deduplication, encoding\n",
        "\n",
        "### Full Mode\n",
        "- Runs train mode, then inference mode\n",
        "- Complete end-to-end pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Add pipeline to path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "# set sys.path to the parent directory\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded config from: ..\\pipeline\\config\\config.yaml\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'PipelineConfig' object has no attribute 'pipeline'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m config \u001b[38;5;241m=\u001b[39m PipelineConfig\u001b[38;5;241m.\u001b[39mfrom_yaml(config_path)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded config from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot set\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns to process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mconfig\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\pydantic\\main.py:1026\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'PipelineConfig' object has no attribute 'pipeline'"
          ]
        }
      ],
      "source": [
        "\n",
        "from pipeline.config import PipelineConfig\n",
        "from pathlib import Path\n",
        "# Load configuration\n",
        "# preprocess_pipeline\\pipeline\\config\\ingrnorm_unified_example.yaml\n",
        "\n",
        "config_path = Path(\"../pipeline/config/config.yaml\")\n",
        "if not config_path.exists():\n",
        "    print(f\"Config not found at {config_path}\")\n",
        "    print(\"Please ensure the config file exists.\")\n",
        "else:\n",
        "    config = PipelineConfig.from_yaml(config_path)\n",
        "    print(f\"Loaded config from: {config_path}\")\n",
        "    print(f\"Pipeline mode: {config.pipeline.mode if config.pipeline else 'not set'}\")\n",
        "    print(f\"Columns to process: {config.pipeline.columns if config.pipeline else 'default'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Run Unified Pipeline\n",
        "\n",
        "The unified pipeline can run in different modes. Configure the mode and execute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
            "  warnings.warn(\n",
            "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
            "  warnings.warn(\n",
            "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
            "  warnings.warn(\n",
            "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
            "  warnings.warn(\n",
            "c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline mode: full\n",
            "Input path: data/raw/\n",
            "Model dir: models/ingredient_ner/\n"
          ]
        }
      ],
      "source": [
        "from pipeline.run_pipeline import UnifiedPipelineOrchestrator\n",
        "from pipeline.common.logging_setup import setup_logging\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "setup_logging(config.to_dict())\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set pipeline mode (train, inference, or full)\n",
        "if config.pipeline is None:\n",
        "    from pipeline.config import PipelineModeConfig\n",
        "    config.pipeline = PipelineModeConfig()\n",
        "\n",
        "# Uncomment the mode you want to use:\n",
        "# config.pipeline.mode = \"train\"      # Train model only\n",
        "# config.pipeline.mode = \"inference\"  # Run inference only\n",
        "config.pipeline.mode = \"full\"         # Train + inference\n",
        "\n",
        "print(f\"Pipeline mode: {config.pipeline.mode}\")\n",
        "print(f\"Input path: {config.pipeline.input_path if config.pipeline else config.data.input_path}\")\n",
        "print(f\"Model dir: {config.pipeline.model_dir if config.pipeline else 'not set'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-11-25 15:13:59] INFO pipeline.run_pipeline.Orchestrator: Setting up pipeline steps for mode: full\n",
            "[2025-11-25 15:13:59] INFO pipeline.run_pipeline.Orchestrator: Combining data for column 'ingredients' from data\\raw\n",
            "[2025-11-25 15:13:59] INFO pipeline.base.RawDataCombiner: Initialized with data_dir=data\\raw\n",
            "[2025-11-25 15:13:59] INFO pipeline.run_pipeline.Orchestrator: Combining data for column 'cuisine' from data\\raw\n",
            "[2025-11-25 15:13:59] INFO pipeline.base.RawDataCombiner: Initialized with data_dir=data\\raw\n",
            "[2025-11-25 15:13:59] INFO pipeline.base.NERModelTrainer: Initialized with target_column=NER, base_model=roberta-base, epochs=10\n",
            "[2025-11-25 15:13:59] INFO pipeline.run_pipeline.Orchestrator: Processing column: ingredients\n",
            "[2025-11-25 15:14:00] INFO pipeline.base.IngredientNormalizer: Initialized with model=en_core_web_sm, batch_size=1024, n_process=4\n",
            "[2025-11-25 15:14:00] INFO pipeline.base.DeduplicationStep_SBERT: Initialized with method=sbert\n",
            "[2025-11-25 15:14:00] INFO pipeline.base.EncodingStep: Initialized with min_freq=1, dataset_id=1, col=NER_clean\n",
            "[2025-11-25 15:14:00] INFO pipeline.run_pipeline.Orchestrator: Processing column: cuisine\n",
            "[2025-11-25 15:14:00] INFO pipeline.base.IngredientNormalizer: Initialized with model=en_core_web_sm, batch_size=1024, n_process=4\n",
            "[2025-11-25 15:14:00] INFO pipeline.base.DeduplicationStep_SBERT: Initialized with method=sbert\n",
            "[2025-11-25 15:14:00] INFO pipeline.base.EncodingStep: Initialized with min_freq=1, dataset_id=1, col=cuisine_clean\n",
            "[2025-11-25 15:14:00] INFO pipeline.run_pipeline.Orchestrator: Setup complete: 9 step(s) configured\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configured 9 step(s):\n",
            "  1. RawDataCombiner\n",
            "      Input: data\\raw\n",
            "      Output: data\\normalized\\combined_ingredients_data.parquet\n",
            "  2. RawDataCombiner\n",
            "      Input: data\\raw\n",
            "      Output: data\\normalized\\combined_cuisine_data.parquet\n",
            "  3. NERModelTrainer\n",
            "      Input: data\\normalized\\combined_ingredients_data.parquet\n",
            "      Output: models\\ingredient_ner\n",
            "  4. IngredientNormalizer\n",
            "      Input: data\\normalized\\combined_ingredients_data.parquet\n",
            "      Output: data\\normalized\\recipes_data_clean.parquet\n",
            "  5. DeduplicationStep_SBERT\n",
            "      Input: data\\normalized\\recipes_data_clean.parquet\n",
            "      Output: data\\normalized\\recipes_data_clean_spell_dedup.parquet\n",
            "  6. EncodingStep\n",
            "      Input: data\\normalized\\recipes_data_clean_spell_dedup.parquet\n",
            "      Output: data\\encoded\\datasets_unified.parquet\n",
            "  7. IngredientNormalizer\n",
            "      Input: data\\normalized\\combined_cuisine_data.parquet\n",
            "      Output: data\\normalized\\cuisine_baseline.parquet\n",
            "  8. DeduplicationStep_SBERT\n",
            "      Input: data\\normalized\\cuisine_baseline.parquet\n",
            "      Output: data\\normalized\\cuisine_deduped.parquet\n",
            "  9. EncodingStep\n",
            "      Input: data\\normalized\\cuisine_deduped.parquet\n",
            "      Output: data\\encoded\\cuisine_unified.parquet\n"
          ]
        }
      ],
      "source": [
        "# Create orchestrator and setup steps\n",
        "orchestrator = UnifiedPipelineOrchestrator(config)\n",
        "orchestrator.setup_steps()\n",
        "\n",
        "print(f\"Configured {len(orchestrator.steps)} step(s):\")\n",
        "for i, step in enumerate(orchestrator.steps, 1):\n",
        "    print(f\"  {i}. {step.name}\")\n",
        "    if hasattr(step, 'input_path') and step.input_path:\n",
        "        print(f\"      Input: {step.input_path}\")\n",
        "    if hasattr(step, 'output_path') and step.output_path:\n",
        "        print(f\"      Output: {step.output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline configured. Uncomment orchestrator.run() above to execute.\n"
          ]
        }
      ],
      "source": [
        "# Execute pipeline (uncomment to run)\n",
        "# orchestrator.run(force=False)\n",
        "\n",
        "# Or run with force to rebuild all artifacts:\n",
        "# orchestrator.run(force=True)\n",
        "\n",
        "print(\"Pipeline configured. Uncomment orchestrator.run() above to execute.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output file not found: data\\encoded\\datasets_unified.parquet\n",
            "Run the pipeline first to generate output.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check output files\n",
        "output_path = Path(config.output.unified_parquet)\n",
        "if output_path.exists():\n",
        "    df = pd.read_parquet(output_path)\n",
        "    print(f\"Output file: {output_path}\")\n",
        "    print(f\"Rows: {len(df):,}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(f\"Output file not found: {output_path}\")\n",
        "    print(\"Run the pipeline first to generate output.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
