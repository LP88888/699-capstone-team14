{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cuisine Normalization Pipeline\n",
        "\n",
        "This notebook runs the cuisine normalization pipeline, which applies the same normalization workflow used for ingredients to cuisine labels.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The `run_cuisine_norm.py` script performs normalization for cuisine labels:\n",
        "\n",
        "1. **Step 1: Prepare Cuisine Lists** - Splits multi-cuisine entries (e.g., \"[American, Italian]\") into separate list items\n",
        "2. **Step 2: spaCy Normalization** - Cleans and normalizes cuisine text\n",
        "3. **Step 3: SBERT Deduplication** - Identifies duplicate cuisine names (e.g., \"American\" vs \"american\")\n",
        "4. **Step 4: Apply Dedupe Map** - Creates canonical cuisine forms\n",
        "5. **Step 5: Encoding** - Encodes cuisines to integer IDs\n",
        "6. **Step 6: Apply to Combined Dataset** - Updates the main combined dataset with normalized and encoded cuisines\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- Handles multi-cuisine entries (splits on commas, \"&\", \"and\")\n",
        "- Removes \"recipes\" suffix from cuisine names\n",
        "- Deduplicates similar cuisine names using semantic similarity\n",
        "- Creates encoder maps for downstream use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Add pipeline to path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add pipeline directory to path\n",
        "pipeline_root = Path.cwd().parent.parent / \"pipeline\"\n",
        "if str(pipeline_root) not in sys.path:\n",
        "    sys.path.insert(0, str(pipeline_root))\n",
        "\n",
        "print(f\"Pipeline root: {pipeline_root}\")\n",
        "print(f\"Python path includes: {pipeline_root.exists()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Configure Cuisine Normalization\n",
        "\n",
        "Load the configuration file that specifies input/output paths and normalization parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration file\n",
        "config_path = Path(\"./pipeline/config/cuisnorm.yaml\")\n",
        "\n",
        "# Load and display config\n",
        "if config_path.exists():\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    print(\"Configuration loaded:\")\n",
        "    data_cfg = config.get('data', {})\n",
        "    output_cfg = config.get('output', {})\n",
        "    \n",
        "    print(f\"  Input path: {data_cfg.get('input_path', 'N/A')}\")\n",
        "    print(f\"  Cuisine column: {data_cfg.get('cuisine_col', 'cuisine')}\")\n",
        "    print(f\"\\n  Output paths:\")\n",
        "    print(f\"    - Baseline: {output_cfg.get('baseline_parquet', 'N/A')}\")\n",
        "    print(f\"    - Dedupe map: {output_cfg.get('cosine_map_path', 'N/A')}\")\n",
        "    print(f\"    - Unified: {output_cfg.get('unified_parquet', 'N/A')}\")\n",
        "    print(f\"    - Token→ID: {output_cfg.get('cuisine_token_to_id', 'N/A')}\")\n",
        "else:\n",
        "    print(f\"Config file not found: {config_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Run Cuisine Normalization Pipeline\n",
        "\n",
        "Execute the cuisine normalization pipeline. This will process all cuisine labels through the normalization stages and update the combined dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Run the cuisine normalization script\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    str(Path(\"./pipeline/scripts/run_cuisine_norm.py\")),\n",
        "    \"--config\", str(config_path),\n",
        "    # Uncomment to force rebuild all artifacts:\n",
        "    # \"--force\",\n",
        "]\n",
        "\n",
        "print(\"Running cuisine normalization pipeline...\")\n",
        "print(f\"Command: {' '.join(cmd)}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "# Print output\n",
        "print(result.stdout)\n",
        "if result.stderr:\n",
        "    print(\"STDERR:\", result.stderr)\n",
        "print(f\"\\nReturn code: {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Inspect Cuisine Normalization Results\n",
        "\n",
        "Examine the outputs to verify cuisine normalization worked correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Load config to get output paths\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "output_cfg = config.get('output', {})\n",
        "\n",
        "# Check dedupe map\n",
        "dedupe_map_path = Path(output_cfg.get('cosine_map_path', './data/cuisine_normalized/cuisine_dedupe_map.jsonl'))\n",
        "if dedupe_map_path.exists():\n",
        "    print(f\"✓ Cuisine dedupe map exists: {dedupe_map_path}\")\n",
        "    with open(dedupe_map_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    print(f\"  Number of mappings: {len(lines)}\")\n",
        "    if lines:\n",
        "        print(f\"  Sample mapping: {json.loads(lines[0])}\")\n",
        "else:\n",
        "    print(f\"✗ Cuisine dedupe map not found: {dedupe_map_path}\")\n",
        "\n",
        "# Check encoder maps\n",
        "token_to_id_path = Path(output_cfg.get('cuisine_token_to_id', './data/cuisine_encoded/cuisine_token_to_id.json'))\n",
        "id_to_token_path = Path(output_cfg.get('cuisine_id_to_token', './data/cuisine_encoded/cuisine_id_to_token.json'))\n",
        "\n",
        "if token_to_id_path.exists():\n",
        "    print(f\"\\n✓ Cuisine token-to-ID map exists: {token_to_id_path}\")\n",
        "    with open(token_to_id_path, 'r') as f:\n",
        "        tok2id = json.load(f)\n",
        "    print(f\"  Vocabulary size: {len(tok2id):,} unique cuisines\")\n",
        "    # Show sample cuisines\n",
        "    sample_cuisines = list(tok2id.keys())[:20]\n",
        "    print(f\"  Sample cuisines: {sample_cuisines}\")\n",
        "else:\n",
        "    print(f\"\\n✗ Cuisine token-to-ID map not found: {token_to_id_path}\")\n",
        "\n",
        "# Check if combined dataset was updated\n",
        "data_cfg = config.get('data', {})\n",
        "combined_path = Path(data_cfg.get('input_path', './data/combined_raw_datasets.parquet'))\n",
        "updated_path = combined_path.parent / f\"{combined_path.stem}_with_cuisine_encoded.parquet\"\n",
        "\n",
        "if updated_path.exists():\n",
        "    print(f\"\\n✓ Updated combined dataset exists: {updated_path}\")\n",
        "    df = pd.read_parquet(updated_path, nrows=10)\n",
        "    print(f\"  Shape: {df.shape}\")\n",
        "    print(f\"  Columns: {list(df.columns)}\")\n",
        "    \n",
        "    # Show cuisine transformations\n",
        "    if 'cuisine' in df.columns and 'cuisine_encoded' in df.columns:\n",
        "        print(f\"\\n  Sample cuisine transformations:\")\n",
        "        for idx in range(min(5, len(df))):\n",
        "            orig = df['cuisine'].iloc[idx]\n",
        "            encoded = df['cuisine_encoded'].iloc[idx]\n",
        "            print(f\"    Row {idx}: {orig} → {encoded}\")\n",
        "else:\n",
        "    print(f\"\\n✗ Updated combined dataset not found: {updated_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
