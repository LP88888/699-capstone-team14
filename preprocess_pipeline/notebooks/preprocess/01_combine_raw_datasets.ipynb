{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combine Raw Datasets\n",
        "\n",
        "This notebook combines multiple raw CSV datasets into a unified format and runs ingredient NER inference to extract ingredients from raw text.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The `combine_raw_datasets.py` script:\n",
        "1. Reads multiple CSV files from `data/raw/`\n",
        "2. Extracts ingredients and cuisine columns (with intelligent column detection)\n",
        "3. Combines all datasets into a single DataFrame with standardized columns\n",
        "4. Runs ingredient NER inference to extract normalized ingredients from raw text\n",
        "5. Outputs a combined parquet file with `inferred_ingredients` and `encoded_ingredients` columns\n",
        "\n",
        "## Workflow Steps\n",
        "\n",
        "1. **Dataset Discovery**: Finds all CSV files in the raw data directory\n",
        "2. **Column Detection**: Automatically detects ingredients and cuisine columns (case-insensitive)\n",
        "3. **Cuisine Extraction**: Extracts cuisine labels from various formats (lists, strings, text columns)\n",
        "4. **Data Combination**: Merges all datasets with Dataset_ID tracking\n",
        "5. **NER Inference**: Applies trained ingredient NER model to extract ingredients from raw text\n",
        "6. **Output**: Saves combined dataset as parquet file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline root: c:\\Users\\georg.DESKTOP-2FS9VF1\\source\\repos\\699-capstone-team14\\preprocess_pipeline\\pipeline\n",
            "Python path includes: True\n"
          ]
        }
      ],
      "source": [
        "# Setup: Add pipeline to path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add pipeline directory to path\n",
        "pipeline_root = Path.cwd().parent.parent / \"pipeline\"\n",
        "if str(pipeline_root) not in sys.path:\n",
        "    sys.path.insert(0, str(pipeline_root))\n",
        "\n",
        "print(f\"Pipeline root: {pipeline_root}\")\n",
        "print(f\"Python path includes: {pipeline_root.exists()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Required Modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Import from pipeline scripts\n",
        "from common.logging_setup import setup_logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"[%(asctime)s] %(levelname)s %(name)s: %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure Paths\n",
        "\n",
        "Set the input directory (raw CSV files) and output path for the combined dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data directory: data\\raw (exists: False)\n",
            "Output path: data\\combined_raw_datasets.parquet\n",
            "Inference config: pipeline\\config\\ingredient_ner_inference.yaml (exists: False)\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "data_dir = Path(\"./data/raw\")\n",
        "output_path = Path(\"./data/combined_raw_datasets.parquet\")\n",
        "inference_config = Path(\"./pipeline/config/ingredient_ner_inference.yaml\")\n",
        "\n",
        "# Check if directories exist\n",
        "print(f\"Data directory: {data_dir} (exists: {data_dir.exists()})\")\n",
        "print(f\"Output path: {output_path}\")\n",
        "print(f\"Inference config: {inference_config} (exists: {inference_config.exists()})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Run Dataset Combination Script\n",
        "\n",
        "This cell runs the actual combination script. The script will:\n",
        "- Find all CSV files in the data directory\n",
        "- Process each file to extract ingredients and cuisine columns\n",
        "- Combine all datasets\n",
        "- Run NER inference to extract ingredients from raw text\n",
        "- Save the combined dataset\n",
        "\n",
        "**Note**: This can take a while depending on the size of your datasets and whether NER inference is enabled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the main function from combine_raw_datasets\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Run the script using subprocess to capture output\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    str(Path(\"./pipeline/scripts/combine_raw_datasets.py\")),\n",
        "    \"--data-dir\", str(data_dir),\n",
        "    \"--output\", str(output_path),\n",
        "    \"--inference-config\", str(inference_config),\n",
        "    # Uncomment to skip inference if you just want to combine datasets:\n",
        "    # \"--skip-inference\",\n",
        "]\n",
        "\n",
        "print(\"Running dataset combination script...\")\n",
        "print(f\"Command: {' '.join(cmd)}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Run the script\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "# Print output\n",
        "print(result.stdout)\n",
        "if result.stderr:\n",
        "    print(\"STDERR:\", result.stderr)\n",
        "print(f\"\\nReturn code: {result.returncode}\")\n",
        "\n",
        "# Alternative: Import and run directly (uncomment to use)\n",
        "# from pipeline.scripts.combine_raw_datasets import main\n",
        "# import sys\n",
        "# sys.argv = ['combine_raw_datasets.py', \n",
        "#              '--data-dir', str(data_dir),\n",
        "#              '--output', str(output_path),\n",
        "#              '--inference-config', str(inference_config)]\n",
        "# main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Inspect Combined Dataset\n",
        "\n",
        "Load and examine the combined dataset to verify the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the combined dataset\n",
        "if output_path.exists():\n",
        "    df = pd.read_parquet(output_path)\n",
        "    \n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"\\nColumns: {list(df.columns)}\")\n",
        "    print(f\"\\nDataset_ID distribution:\")\n",
        "    print(df['Dataset_ID'].value_counts().sort_index())\n",
        "    \n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    print(df.head())\n",
        "    \n",
        "    # Check inferred ingredients\n",
        "    if 'inferred_ingredients' in df.columns:\n",
        "        print(f\"\\nSample inferred ingredients:\")\n",
        "        sample_idx = df['inferred_ingredients'].notna().idxmax() if df['inferred_ingredients'].notna().any() else 0\n",
        "        print(f\"Row {sample_idx}: {df.loc[sample_idx, 'inferred_ingredients']}\")\n",
        "    \n",
        "    # Check cuisine distribution\n",
        "    if 'cuisine' in df.columns:\n",
        "        print(f\"\\nCuisine distribution (top 10):\")\n",
        "        print(df['cuisine'].value_counts().head(10))\n",
        "else:\n",
        "    print(f\"Output file not found: {output_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
