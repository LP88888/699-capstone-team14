{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ingredient Normalization Pipeline\n",
        "\n",
        "This notebook runs the ingredient normalization pipeline, which processes raw ingredient lists through multiple stages to create a clean, deduplicated, and encoded dataset.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The `run_ingrnorm.py` script performs a 4-stage normalization pipeline:\n",
        "\n",
        "1. **Stage 1: spaCy Normalization** - Cleans and normalizes ingredient text using spaCy NLP\n",
        "2. **Stage 2: Deduplication** - Identifies and maps duplicate ingredients using SBERT (sentence embeddings) or Word2Vec\n",
        "3. **Stage 3: Apply Dedupe Map** - Applies the deduplication mapping to create canonical ingredient forms\n",
        "4. **Stage 4: Encoding** - Encodes normalized ingredients to integer IDs for efficient storage and processing\n",
        "\n",
        "## Input/Output\n",
        "\n",
        "- **Input**: Parquet file with ingredient lists (e.g., from NER inference or raw data)\n",
        "- **Output**: \n",
        "  - Normalized parquet files at each stage\n",
        "  - Dedupe mapping (JSONL)\n",
        "  - Encoder maps (JSON): `ingredient_token_to_id.json`, `ingredient_id_to_token.json`\n",
        "  - Final encoded dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Add pipeline to path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add pipeline directory to path\n",
        "pipeline_root = Path.cwd().parent.parent / \"pipeline\"\n",
        "if str(pipeline_root) not in sys.path:\n",
        "    sys.path.insert(0, str(pipeline_root))\n",
        "\n",
        "print(f\"Pipeline root: {pipeline_root}\")\n",
        "print(f\"Python path includes: {pipeline_root.exists()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Configure Normalization Pipeline\n",
        "\n",
        "Set the configuration file path. The config file specifies:\n",
        "- Input data path\n",
        "- Output paths for each stage\n",
        "- Normalization parameters (SBERT model, thresholds, etc.)\n",
        "- Which stages to run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration file\n",
        "config_path = Path(\"./pipeline/config/ingrnorm.yaml\")\n",
        "\n",
        "# Load and display config\n",
        "if config_path.exists():\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    print(\"Configuration loaded:\")\n",
        "    print(f\"  Input path: {config.get('data', {}).get('input_path', 'N/A')}\")\n",
        "    print(f\"  Baseline parquet: {config.get('output', {}).get('baseline_parquet', 'N/A')}\")\n",
        "    print(f\"  Dedupe map: {config.get('output', {}).get('cosine_map_path', 'N/A')}\")\n",
        "    print(f\"  Unified parquet: {config.get('output', {}).get('unified_parquet', 'N/A')}\")\n",
        "    print(f\"\\n  Stages enabled:\")\n",
        "    stages = config.get('stages', {})\n",
        "    print(f\"    - Write parquet: {stages.get('write_parquet', False)}\")\n",
        "    print(f\"    - SBERT dedupe: {stages.get('sbert_dedupe', False)}\")\n",
        "    print(f\"    - Apply map: {stages.get('apply_cosine_map', False)}\")\n",
        "    print(f\"    - Encode IDs: {stages.get('encode_ids', False)}\")\n",
        "else:\n",
        "    print(f\"Config file not found: {config_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Run Normalization Pipeline\n",
        "\n",
        "Execute the full normalization pipeline. This will:\n",
        "1. Normalize ingredients using spaCy\n",
        "2. Build deduplication map using SBERT embeddings\n",
        "3. Apply the dedupe map to create canonical forms\n",
        "4. Encode ingredients to integer IDs\n",
        "\n",
        "**Note**: This process can take significant time depending on dataset size. The pipeline is designed to skip stages if output files already exist (unless `--force` is used).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Run the normalization script\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    str(Path(\"./pipeline/scripts/run_ingrnorm.py\")),\n",
        "    \"--config\", str(config_path),\n",
        "    # Uncomment to force rebuild all artifacts:\n",
        "    # \"--force\",\n",
        "]\n",
        "\n",
        "print(\"Running ingredient normalization pipeline...\")\n",
        "print(f\"Command: {' '.join(cmd)}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "# Print output\n",
        "print(result.stdout)\n",
        "if result.stderr:\n",
        "    print(\"STDERR:\", result.stderr)\n",
        "print(f\"\\nReturn code: {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Inspect Normalization Results\n",
        "\n",
        "Examine the outputs from each stage to verify the normalization process worked correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Load config to get output paths\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "output_cfg = config.get('output', {})\n",
        "\n",
        "# Check baseline (normalized) parquet\n",
        "baseline_path = Path(output_cfg.get('baseline_parquet', './data/normalized/recipes_data_clean.parquet'))\n",
        "if baseline_path.exists():\n",
        "    print(f\"✓ Baseline parquet exists: {baseline_path}\")\n",
        "    df_baseline = pd.read_parquet(baseline_path, nrows=5)\n",
        "    print(f\"  Shape: {df_baseline.shape}\")\n",
        "    print(f\"  Columns: {list(df_baseline.columns)}\")\n",
        "    if 'NER_clean' in df_baseline.columns:\n",
        "        print(f\"  Sample NER_clean: {df_baseline['NER_clean'].iloc[0]}\")\n",
        "else:\n",
        "    print(f\"✗ Baseline parquet not found: {baseline_path}\")\n",
        "\n",
        "# Check dedupe map\n",
        "dedupe_map_path = Path(output_cfg.get('cosine_map_path', './data/normalized/cosine_dedupe_map.jsonl'))\n",
        "if dedupe_map_path.exists():\n",
        "    print(f\"\\n✓ Dedupe map exists: {dedupe_map_path}\")\n",
        "    # Count lines in JSONL file\n",
        "    with open(dedupe_map_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    print(f\"  Number of mappings: {len(lines)}\")\n",
        "    # Show first few mappings\n",
        "    if lines:\n",
        "        import json\n",
        "        print(f\"  Sample mapping: {json.loads(lines[0])}\")\n",
        "else:\n",
        "    print(f\"\\n✗ Dedupe map not found: {dedupe_map_path}\")\n",
        "\n",
        "# Check encoder maps\n",
        "token_to_id_path = Path(output_cfg.get('ingredient_token_to_id', './data/encoded/ingredient_token_to_id.json'))\n",
        "id_to_token_path = Path(output_cfg.get('ingredient_id_to_token', './data/encoded/ingredient_id_to_token.json'))\n",
        "\n",
        "if token_to_id_path.exists():\n",
        "    print(f\"\\n✓ Token-to-ID map exists: {token_to_id_path}\")\n",
        "    with open(token_to_id_path, 'r') as f:\n",
        "        tok2id = json.load(f)\n",
        "    print(f\"  Vocabulary size: {len(tok2id):,} tokens\")\n",
        "    # Show sample tokens\n",
        "    sample_tokens = list(tok2id.keys())[:10]\n",
        "    print(f\"  Sample tokens: {sample_tokens}\")\n",
        "else:\n",
        "    print(f\"\\n✗ Token-to-ID map not found: {token_to_id_path}\")\n",
        "\n",
        "# Check final encoded dataset\n",
        "unified_path = Path(output_cfg.get('unified_parquet', './data/encoded/datasets_unified.parquet'))\n",
        "if unified_path.exists():\n",
        "    print(f\"\\n✓ Unified encoded parquet exists: {unified_path}\")\n",
        "    df_unified = pd.read_parquet(unified_path, nrows=5)\n",
        "    print(f\"  Shape: {df_unified.shape}\")\n",
        "    print(f\"  Columns: {list(df_unified.columns)}\")\n",
        "    if 'Ingredients' in df_unified.columns:\n",
        "        print(f\"  Sample encoded ingredients: {df_unified['Ingredients'].iloc[0]}\")\n",
        "else:\n",
        "    print(f\"\\n✗ Unified parquet not found: {unified_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
