{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ingredient NER Model Training\n",
        "\n",
        "This notebook trains a spaCy transformer-based Named Entity Recognition (NER) model to extract ingredients from raw text.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The `run_ingredient_ner.py` script trains a custom NER model using:\n",
        "- **Transformer backbone**: DistilBERT (configurable)\n",
        "- **Training data**: Normalized ingredient lists from the preprocessing pipeline\n",
        "- **Output**: Trained spaCy model saved to `models/ingredient_ner_trf/model-best/`\n",
        "\n",
        "## Training Pipeline\n",
        "\n",
        "1. **Data Preparation**: Converts ingredient lists into spaCy DocBin format for training\n",
        "2. **Train/Validation Split**: Splits data into training and validation sets\n",
        "3. **Model Training**: Trains transformer-based NER model with early stopping\n",
        "4. **Model Evaluation**: Validates on held-out validation set\n",
        "5. **Model Export**: Saves the best model checkpoint\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- Uses pre-normalized ingredients from the normalization pipeline\n",
        "- Applies deduplication map to ensure consistent ingredient forms\n",
        "- Supports transformer models (DistilBERT, BERT, etc.)\n",
        "- Includes early stopping to prevent overfitting\n",
        "- Cleans up intermediate training artifacts after completion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Add pipeline to path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add pipeline directory to path\n",
        "pipeline_root = Path.cwd().parent.parent / \"pipeline\"\n",
        "if str(pipeline_root) not in sys.path:\n",
        "    sys.path.insert(0, str(pipeline_root))\n",
        "\n",
        "print(f\"Pipeline root: {pipeline_root}\")\n",
        "print(f\"Python path includes: {pipeline_root.exists()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Configure Training\n",
        "\n",
        "Load the training configuration file. This specifies:\n",
        "- Training data path\n",
        "- Model architecture (transformer model, window size, etc.)\n",
        "- Training hyperparameters (learning rate, epochs, batch size)\n",
        "- Output directory for the trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration file\n",
        "config_path = Path(\"./pipeline/config/ingredient_ner.yaml\")\n",
        "\n",
        "# Load and display config\n",
        "if config_path.exists():\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    print(\"Training configuration loaded:\")\n",
        "    ner_cfg = config.get('ner', {})\n",
        "    \n",
        "    print(f\"\\n  Data:\")\n",
        "    print(f\"    - Train path: {ner_cfg.get('train_path', 'N/A')}\")\n",
        "    print(f\"    - NER list column: {ner_cfg.get('ner_list_col', 'N/A')}\")\n",
        "    print(f\"    - Max rows: {ner_cfg.get('max_rows', 'all')}\")\n",
        "    \n",
        "    print(f\"\\n  Model:\")\n",
        "    print(f\"    - Transformer: {ner_cfg.get('transformer_model', 'N/A')}\")\n",
        "    print(f\"    - Window size: {ner_cfg.get('window', 'N/A')}\")\n",
        "    print(f\"    - Stride: {ner_cfg.get('stride', 'N/A')}\")\n",
        "    \n",
        "    print(f\"\\n  Training:\")\n",
        "    print(f\"    - Epochs: {ner_cfg.get('n_epochs', 'N/A')}\")\n",
        "    print(f\"    - Learning rate: {ner_cfg.get('lr', 'N/A')}\")\n",
        "    print(f\"    - Batch size: {ner_cfg.get('batch_size', 'N/A')}\")\n",
        "    print(f\"    - Validation fraction: {ner_cfg.get('valid_fraction', 'N/A')}\")\n",
        "    print(f\"    - Early stopping patience: {ner_cfg.get('early_stopping_patience', 'N/A')}\")\n",
        "    \n",
        "    print(f\"\\n  Output:\")\n",
        "    print(f\"    - Model directory: {ner_cfg.get('model_dir', 'N/A')}\")\n",
        "else:\n",
        "    print(f\"Config file not found: {config_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Check Training Data\n",
        "\n",
        "Verify that the training data exists and is in the expected format before starting training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Check training data\n",
        "train_path = Path(ner_cfg.get('train_path', './data/normalized/recipes_data_clean.parquet'))\n",
        "ner_col = ner_cfg.get('ner_list_col', 'NER_clean')\n",
        "\n",
        "if train_path.exists():\n",
        "    print(f\"✓ Training data found: {train_path}\")\n",
        "    \n",
        "    # Load a sample to inspect\n",
        "    df_sample = pd.read_parquet(train_path, nrows=10)\n",
        "    print(f\"  Shape: {df_sample.shape}\")\n",
        "    print(f\"  Columns: {list(df_sample.columns)}\")\n",
        "    \n",
        "    if ner_col in df_sample.columns:\n",
        "        print(f\"\\n  Sample {ner_col} data:\")\n",
        "        for idx in range(min(5, len(df_sample))):\n",
        "            ingredients = df_sample[ner_col].iloc[idx]\n",
        "            print(f\"    Row {idx}: {ingredients}\")\n",
        "    else:\n",
        "        print(f\"\\n  ✗ Column '{ner_col}' not found in dataset\")\n",
        "        print(f\"    Available columns: {list(df_sample.columns)}\")\n",
        "else:\n",
        "    print(f\"✗ Training data not found: {train_path}\")\n",
        "    print(\"  Make sure you've run the normalization pipeline first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Run Training\n",
        "\n",
        "Execute the training script. This will:\n",
        "1. Prepare training data in spaCy DocBin format\n",
        "2. Split into train/validation sets\n",
        "3. Train the transformer-based NER model\n",
        "4. Save the best model checkpoint\n",
        "\n",
        "**Note**: Training can take a significant amount of time depending on:\n",
        "- Dataset size\n",
        "- Number of epochs\n",
        "- Transformer model size\n",
        "- Hardware (CPU vs GPU)\n",
        "\n",
        "The script will show progress and validation metrics during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Run the training script\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    str(Path(\"./pipeline/scripts/run_ingredient_ner.py\")),\n",
        "    \"--config\", str(config_path),\n",
        "]\n",
        "\n",
        "print(\"Starting NER model training...\")\n",
        "print(f\"Command: {' '.join(cmd)}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"This may take a while. Training progress will be shown below.\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "# Print output\n",
        "print(result.stdout)\n",
        "if result.stderr:\n",
        "    print(\"STDERR:\", result.stderr)\n",
        "print(f\"\\nReturn code: {result.returncode}\")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n✓ Training completed successfully!\")\n",
        "else:\n",
        "    print(\"\\n✗ Training failed. Check the error messages above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Verify Trained Model\n",
        "\n",
        "Check that the model was saved correctly and inspect its location.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Check model directory\n",
        "model_dir = Path(ner_cfg.get('model_dir', './models/ingredient_ner_trf/model-best'))\n",
        "\n",
        "if model_dir.exists():\n",
        "    print(f\"✓ Trained model found: {model_dir}\")\n",
        "    \n",
        "    # List model files\n",
        "    model_files = list(model_dir.rglob(\"*\"))\n",
        "    print(f\"\\n  Model files ({len(model_files)} total):\")\n",
        "    for f in sorted(model_files)[:20]:  # Show first 20 files\n",
        "        if f.is_file():\n",
        "            size = f.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "            print(f\"    - {f.relative_to(model_dir)} ({size:.2f} MB)\")\n",
        "    \n",
        "    # Check for key model components\n",
        "    key_files = ['config.cfg', 'meta.json']\n",
        "    for key_file in key_files:\n",
        "        key_path = model_dir / key_file\n",
        "        if key_path.exists():\n",
        "            print(f\"\\n  ✓ {key_file} exists\")\n",
        "        else:\n",
        "            print(f\"\\n  ✗ {key_file} not found\")\n",
        "    \n",
        "    # Check for transformer model\n",
        "    transformer_dir = model_dir / \"transformer\"\n",
        "    if transformer_dir.exists():\n",
        "        print(f\"\\n  ✓ Transformer model directory exists\")\n",
        "    else:\n",
        "        print(f\"\\n  ✗ Transformer model directory not found\")\n",
        "        \n",
        "else:\n",
        "    print(f\"✗ Model directory not found: {model_dir}\")\n",
        "    print(\"  Training may have failed or model was saved to a different location.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Test the Trained Model (Optional)\n",
        "\n",
        "Load and test the trained model on a sample text to verify it works correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Test the trained model\n",
        "if model_dir.exists():\n",
        "    try:\n",
        "        import spacy\n",
        "        \n",
        "        print(\"Loading trained model...\")\n",
        "        nlp = spacy.load(str(model_dir))\n",
        "        print(\"✓ Model loaded successfully\")\n",
        "        \n",
        "        # Test on sample text\n",
        "        test_texts = [\n",
        "            \"2 cups all-purpose flour, 1 teaspoon salt, 3 eggs\",\n",
        "            \"chicken breast, olive oil, garlic, lemon juice\",\n",
        "            \"tomatoes, basil, mozzarella cheese, balsamic vinegar\"\n",
        "        ]\n",
        "        \n",
        "        print(\"\\nTesting model on sample texts:\")\n",
        "        for text in test_texts:\n",
        "            doc = nlp(text)\n",
        "            ingredients = [ent.text for ent in doc.ents if ent.label_ == \"INGREDIENT\"]\n",
        "            print(f\"\\n  Text: {text}\")\n",
        "            print(f\"  Extracted ingredients: {ingredients}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading/testing model: {e}\")\n",
        "        print(\"This is okay - the model may still be valid for inference scripts\")\n",
        "else:\n",
        "    print(\"Model not found - skipping test\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
